{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad2316e2-c727-4da5-9148-8354b9aa5579",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import yaml\n",
    "import optuna\n",
    "import joblib\n",
    "import copy\n",
    "import subprocess\n",
    "import shutil\n",
    "from tdc.single_pred import ADME\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from optuna.storages import JournalFileStorage, JournalStorage\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from ase import Atoms\n",
    "from ase.io import read, write\n",
    "from ase.calculators.singlepoint import SinglePointCalculator\n",
    "from project_resources.import_utils import NotebookFinder\n",
    "sys.meta_path.append(NotebookFinder())\n",
    "from project_resources.cytochrome_P450 import smiles_to_xyz_file, get_unique_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e003390-4867-4fb4-b0d0-8ab8bdf9af84",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdc_benchmarks = [\"obach\", \"microsome\", \"hepatocyte\"]\n",
    "# sampler - a method used to generate new sets of hyperparameters in each iteration of the optimization process\n",
    "samplers = {\n",
    "    'RandomSampler': optuna.samplers.RandomSampler,          # Sampler that selects hyperparameters randomly from the search space.\n",
    "    'GridSampler': optuna.samplers.GridSampler,              # Sampler that performs a grid search over the hyperparameter space.\n",
    "    'TPESampler': optuna.samplers.TPESampler,                # Sampler that uses a tree-structured Parzen estimator to model the objective function and sample new points from the search space.\n",
    "    'CmaEsSampler': optuna.samplers.CmaEsSampler,            # Sampler that uses the Covariance Matrix Adaptation Evolution Strategy algorithm to efficiently search the hyperparameter space.\n",
    "    'NSGAIISampler': optuna.samplers.NSGAIISampler,          # Multi-objective evolutionary algorithm that generates new samples using non-dominated sorting and crowding distance selection.\n",
    "    'QMCSampler': optuna.samplers.QMCSampler,                # Quasi-Monte Carlo sampler that uses low-discrepancy sequences to sample the search space in a more efficient and evenly distributed way than random sampling.\n",
    "    'BoTorchSampler': optuna.integration.BoTorchSampler,     # Sampler that leverages the BoTorch library for Bayesian optimization and can handle both continuous and categorical hyperparameters.\n",
    "    'BruteForceSampler': optuna.samplers.BruteForceSampler,  # Sampler that exhaustively evaluates all possible combinations of hyperparameters in the search space.\n",
    "}\n",
    "# pruner - a technique used to eliminate unpromising trials during the course of hyperparameter optimization.\n",
    "pruners = {\n",
    "    'BasePruner': optuna.pruners.BasePruner,                            # This is the base class for all pruning strategies in Optuna. It provides a skeleton for implementing custom pruning strategies.\n",
    "    'MedianPruner': optuna.pruners.MedianPruner,                        # A pruner that prunes unpromising trials that have median objective values, as determined in previous steps.\n",
    "    'SuccessiveHalvingPruner': optuna.pruners.SuccessiveHalvingPruner,  # This pruner repeatedly splits trials into halves, discarding the lower performing half at each iteration.\n",
    "    'HyperbandPruner': optuna.pruners.HyperbandPruner,                  # This pruner implements the Hyperband algorithm, which selects promising trials and runs them with different resource allocation schemes to determine the best one.\n",
    "    'PercentilePruner': optuna.pruners.PercentilePruner,                # A pruner that prunes unpromising trials based on their percentile rank relative to all completed trials.\n",
    "    'NopPruner': optuna.pruners.NopPruner,                              # A pruner that does nothing and does not prune any trials.\n",
    "    'ThresholdPruner': optuna.pruners.ThresholdPruner,                  # This pruner prunes trials that have not reached a certain level of performance (i.e., objective value).\n",
    "    'PatientPruner': optuna.pruners.PatientPruner,                      # This pruner prunes trials that do not show improvement over a certain number of steps (or epochs).\n",
    "}\n",
    "tdc_datasets = {}\n",
    "unique_symbols = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a246d306-a59b-412e-a34a-bab7500ed5e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n",
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n",
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "obach = ADME(name='Half_Life_Obach')\n",
    "obach_split = obach.get_split()\n",
    "tdc_datasets[\"obach\"] = obach_split\n",
    "microsome = ADME(name='Clearance_Microsome_AZ')\n",
    "microsome_split = microsome.get_split()\n",
    "tdc_datasets[\"microsome\"] = microsome_split\n",
    "hepatocyte = ADME(name='Clearance_Hepatocyte_AZ')\n",
    "hepatocyte_split = hepatocyte.get_split()\n",
    "tdc_datasets[\"hepatocyte\"] = hepatocyte_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0c3125a-dbff-4fb6-8555-7eca09e47d0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obach_train.extxyz already exists\n",
      "obach_test.extxyz already exists\n",
      "microsome_train.extxyz already exists\n",
      "microsome_test.extxyz already exists\n",
      "hepatocyte_train.extxyz already exists\n",
      "hepatocyte_test.extxyz already exists\n"
     ]
    }
   ],
   "source": [
    "for benchmark in tdc_benchmarks:\n",
    "    benchmark_train_smiles = tdc_datasets[benchmark][\"train\"][\"Drug\"]\n",
    "    benchmark_test_smiles = tdc_datasets[benchmark][\"test\"][\"Drug\"]\n",
    "    \n",
    "    benchmark_train_halflives = tdc_datasets[benchmark][\"train\"][\"Y\"]\n",
    "    reshaped_train_halflife = np.array(benchmark_train_halflives).reshape(-1, 1)\n",
    "    scaler = MinMaxScaler().fit(reshaped_train_halflife)\n",
    "    train_halflife_scaled = scaler.transform(reshaped_train_halflife)\n",
    "    train_halflives_scaled = np.array([val[0] for val in train_halflife_scaled])\n",
    "\n",
    "    benchmark_test_halflives = tdc_datasets[benchmark][\"test\"][\"Y\"]\n",
    "    reshaped_test_halflife = np.array(benchmark_test_halflives).reshape(-1, 1)\n",
    "    scaler = MinMaxScaler().fit(reshaped_test_halflife)\n",
    "    test_halflife_scaled = scaler.transform(reshaped_test_halflife)\n",
    "    test_halflives_scaled = np.array([val[0] for val in test_halflife_scaled])\n",
    "    \n",
    "    file_location = \"project_resources/nequip/positions\"\n",
    "\n",
    "    smiles_to_xyz_file(benchmark_train_smiles, train_halflives_scaled, f\"{file_location}/{benchmark}_train.extxyz\")\n",
    "    smiles_to_xyz_file(benchmark_test_smiles, test_halflives_scaled, f\"{file_location}/{benchmark}_test.extxyz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4a6b3411-9177-45fb-bb82-700159072bf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def foo(list_smiles):\n",
    "    # gets unique symbols from every mol in the list e.g. [\"C1=CC=C(C=C1)O\", \"C1=CSC=C1\"] -> [\"C\", \"O\", \"S\"]\n",
    "    formulae = \"\"\n",
    "    for smiles in list_smiles:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        chemical_formula = Chem.rdMolDescriptors.CalcMolFormula(mol)\n",
    "        formulae += chemical_formula\n",
    "    unique_elements = [part for part in formulae if part.isalpha()]\n",
    "    for idx, element in enumerate(unique_elements):\n",
    "        two_letter_element = \"\"\n",
    "        if element.islower():\n",
    "            two_letter_element += unique_elements[idx - 1]\n",
    "            two_letter_element += unique_elements[idx]\n",
    "            unique_elements.remove(unique_elements[idx])\n",
    "            unique_elements.remove(unique_elements[idx - 1])\n",
    "            unique_elements.append(two_letter_element)\n",
    "    return set(unique_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "50cb965d-cb0c-466e-be4a-cd25a02caabe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I', 'Li', 'O', 'H', 'P', 'N', 'C', 'B', 'F', 'S', 'Na', 'Br', 'Cl'}\n"
     ]
    }
   ],
   "source": [
    "benchmark = \"obach\"\n",
    "benchmark_train_smiles = tdc_datasets[benchmark][\"train\"][\"Drug\"]\n",
    "benchmark_test_smiles = tdc_datasets[benchmark][\"test\"][\"Drug\"]\n",
    "benchmark_all_smiles = list(benchmark_train_smiles) + list(benchmark_test_smiles)\n",
    "\n",
    "print(foo(benchmark_all_smiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8970ff85-5f0c-4042-9dd8-3eb6881ad554",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chemical elements present in obach: ['Li', 'S', 'L', 'F', 'N', 'B', 'Cl', 'Br', 'O', 'P', 'H', 'I', 'Na', 'C']\n",
      "chemical elements present in microsome: ['S', 'N', 'F', 'B', 'Cl', 'Br', 'O', 'P', 'H', 'I', 'C']\n",
      "chemical elements present in hepatocyte: ['S', 'N', 'F', 'B', 'Cl', 'Br', 'O', 'P', 'H', 'I', 'C']\n"
     ]
    }
   ],
   "source": [
    "for benchmark in tdc_benchmarks:\n",
    "    benchmark_train_smiles = tdc_datasets[benchmark][\"train\"][\"Drug\"]\n",
    "    benchmark_test_smiles = tdc_datasets[benchmark][\"test\"][\"Drug\"]\n",
    "    benchmark_all_smiles = list(benchmark_train_smiles) + list(benchmark_test_smiles)\n",
    "    \n",
    "    unique_symbs = get_unique_elements(benchmark_all_smiles)\n",
    "    unique_symbols[benchmark] = list(unique_symbs)\n",
    "    print(f\"chemical elements present in {benchmark}: {unique_symbols[benchmark]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea19ddf4-57db-40e7-a3b0-37e868a4a912",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_nequip_eval_yaml(yaml_path, root, trial_number, eval_dataset_file_name, unique_symbols):\n",
    "    with open(yaml_path, \"w\") as out_f:\n",
    "        data = {}\n",
    "        \n",
    "        # root is the same as nequip-train, except at the end is also included the number of the run\n",
    "        data[\"root\"] = root\n",
    "        data[\"chemical_symbols\"] = unique_symbols\n",
    "        data[\"dataset_file_name\"] = eval_dataset_file_name\n",
    "        data[\"dataset\"] = \"ase\"\n",
    "        \n",
    "        yaml.dump(data, out_f)\n",
    "        print(f\"\"\"The evaluation yaml file for the {trial_number}. iteration of {splitter} {isozyme}\n",
    "        with {root} as root and was successfully created \"\"\")\n",
    "        out_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "761c3f13-7be7-444a-841b-9df42bd185d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_nequip_train_yaml(yaml_path, batch_size, learning_rate, num_layers, root, trial_number,\n",
    "                       train_dataset_file_name, unique_symbols, num_mols):\n",
    "    with open(yaml_path, \"w\") as out_f:\n",
    "        data = {}\n",
    "\n",
    "        data[\"batch_size\"] = batch_size\n",
    "        data[\"learning_rate\"] = learning_rate\n",
    "        data[\"num_layers\"] = num_layers\n",
    "\n",
    "        data[\"root\"] = root  # project_resources/nequip/{dataset}\n",
    "        data[\"run_name\"] = str(trial_number)  # number of the trial for a given combination of splitter and isozyme\n",
    "        data[\"dataset_file_name\"] = train_dataset_file_name\n",
    "        data[\"seed\"] = 123\n",
    "        data[\"dataset_seed\"] = 456\n",
    "        data[\"append\"] = True\n",
    "        data[\"model_builders\"] = [\"SimpleIrrepsConfig\", \"EnergyModel\", \"PerSpeciesRescale\", \"RescaleEnergyEtc\"]\n",
    "\n",
    "        data[\"max_epochs\"] = 3\n",
    "        data[\"r_max\"] = 0.4  # cutoff radius in Angstroms; changing the value has no effect on validation_e_mae\n",
    "        data[\"l_max\"] = 2\n",
    "        data[\"parity\"] = False  # slighly worse performance than True, but training takes way less time\n",
    "        data[\"num_features\"] = 32\n",
    "        data[\"num_basis\"] = 8\n",
    "\n",
    "        data[\"dataset\"] = \"ase\"\n",
    "        key_mapping = {\"z\": \"atomic_numbers\", \"E\": \"total_energy\", \"R\": \"pos\"}\n",
    "        data[\"key_mapping\"] = key_mapping\n",
    "        data[\"npz_fixed_field_keys\"] = \"atomic_numbers\"\n",
    "        data[\"chemical_symbols\"] = unique_symbols\n",
    "        data[\"wandb\"] = False  # impossible to use wandb inside a notebook (cannot choose an option)\n",
    "\n",
    "        num_train_val = num_mols  # number of molecules in the dataset\n",
    "        num_train = int(np.floor(0.8 * num_train_val))\n",
    "        num_validation = int(num_train_val - num_train)\n",
    "        data[\"n_train\"] = num_train\n",
    "        data[\"n_val\"] = num_validation\n",
    "        data[\"train_val_split\"] = \"sequential\"\n",
    "        data[\"validation_batch_size\"] = num_validation\n",
    "        data[\"loss_coeffs\"] = \"total_energy\"\n",
    "        data[\"optimizer_name\"] = \"Adam\"\n",
    "\n",
    "        splitter = root.split(\"/\")[-2]\n",
    "        isozyme = root.split(\"/\")[-1]\n",
    "        \n",
    "        yaml.dump(data, out_f)\n",
    "        print(f\"\"\"The train yaml file for the {trial_number}. iteration of {splitter} {isozyme}\n",
    "        with {root} as root and was successfully created \"\"\")\n",
    "        out_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42967962-bdf4-457d-827c-53e23f58c510",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_nequip_xyz_out(xyz):\n",
    "    # get the predicted half-lives from xyz file created after using nequip-evaluate --output\n",
    "    with open(xyz) as f:\n",
    "        energies = [float(re.search(r'energy=(-?\\d+\\.\\d+)', line).group(1)) for line in f if \"energy\" in line]\n",
    "    return energies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dbc66bfd-14db-49d5-b308-97040b85633c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_nequip_log(log):\n",
    "    # get the rmse value from the log file created after using nequip-evaluate --log\n",
    "    with open(log) as l:\n",
    "        file_content = l.read()\n",
    "    # use regular expressions to find the rmse value\n",
    "    rmse = re.search(r'\\s*e_rmse\\s*=\\s*(\\d+\\.\\d+)', file_content)\n",
    "    return float(rmse.group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f2e6c1ad-90fb-4b18-8a05-6e326f120c91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NequIPTuner():\n",
    "    def __init__(self, run_name, root, unique_symbols, num_mols, train_dataset_file_name, eval_dataset_file_name):\n",
    "        self.run_name = run_name  # {splitter}_{isozyme}\n",
    "        self.root = root\n",
    "        self.unique_symbols = unique_symbols\n",
    "        self.num_mols = num_mols  # number of mols present in the train .extxyz file\n",
    "        self.train_dataset_file_name = train_dataset_file_name\n",
    "        self.eval_dataset_file_name = eval_dataset_file_name\n",
    "\n",
    "    def sample_params(self, trial: optuna.Trial):\n",
    "        batch_size = trial.suggest_int(\"batch_size\", 2, 50)\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 0.001, 0.1)\n",
    "        num_layers = trial.suggest_int(\"num_layers\", 3, 5)\n",
    "        return {\n",
    "            \"batch_size\": batch_size,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"num_layers\": num_layers\n",
    "        }\n",
    "\n",
    "    def train_test_return(self, train_yaml_name, eval_yaml_name, train_dir, return_predictions=False):\n",
    "        xyz_out = f\"{train_dir}/output.xyz\"\n",
    "        log_file = f\"{train_dir}/evaluation_log.txt\"\n",
    "        \n",
    "        print(f\"running command: nequip-train {train_yaml_name}\")\n",
    "        !nequip-train {train_yaml_name}\n",
    "        # {train_dir} ... project_resources/optuna/nequip/random/3A4/1\n",
    "        # {eval_yaml_name} ... eval_dataset.yaml\n",
    "        # {xyz_out} ... project_resources/optuna/nequip/random/3A4/1/output.xyz\n",
    "        # ^ Parse this to get the predicted half-lives ^\n",
    "        # {log} ... project_resources/optuna/nequip/random/3A4/1/eval_log.txt\n",
    "        \n",
    "        print(f\"\"\"running command: nequip-evaluate --train-dir {train_dir} \n",
    "              --dataset-config {eval_yaml_name} \n",
    "              --output {xyz_out} \n",
    "              --log {log_file}\"\"\")\n",
    "        !nequip-evaluate --train-dir {train_dir} --dataset-config {eval_yaml_name} --output {xyz_out} --log {log_file}\n",
    "\n",
    "        rmse = parse_nequip_log(log_file)\n",
    "        \n",
    "        # delete the dirs and files that were created, as they are no longer needed\n",
    "        shutil.rmtree(train_dir)\n",
    "        os.remove(train_yaml_name)\n",
    "        os.remove(eval_yaml_name)\n",
    "        \n",
    "        if return_predictions:\n",
    "            predictions = parse_nequip_xyz_out(xyz_out)\n",
    "            return rmse, predictions\n",
    "        else:\n",
    "            return rmse\n",
    "\n",
    "    def objective(self, trial=None):\n",
    "        parameters = self.sample_params(trial)\n",
    "    \n",
    "        run_splitter = self.run_name.split(\"_\")[0]\n",
    "        run_isozyme = self.run_name.split(\"_\")[1]\n",
    "        if run_splitter == \"rand\":\n",
    "            run_splitter_name = \"random\"\n",
    "        elif run_splitter == \"scaff\":\n",
    "            run_splitter_name = \"scaffold_splitter\"\n",
    "        else:\n",
    "            run_splitter_name = \"time_split\"\n",
    "\n",
    "        working_dir = f\"project_resources/optuna/nequip/{run_splitter_name}/{run_isozyme}\"\n",
    "        train_yaml_name = f\"{working_dir}/{self.run_name}_nequip_train_{trial.number}.yaml\"\n",
    "        eval_yaml_name = f\"{working_dir}/{self.run_name}_nequip_eval_{trial.number}.yaml\"\n",
    "        train_dir = self.root + f\"/{trial.number}\"\n",
    "        \n",
    "        # create train yaml file for this trial\n",
    "        create_nequip_train_yaml(train_yaml_name, parameters[\"batch_size\"], parameters[\"learning_rate\"],\n",
    "                           parameters[\"num_layers\"], self.root, trial.number, self.train_dataset_file_name,\n",
    "                           self.unique_symbols, self.num_mols)\n",
    "\n",
    "        # create evaluation yaml file for this trial\n",
    "        create_nequip_eval_yaml(eval_yaml_name, self.root, trial.number,\n",
    "                                self.eval_dataset_file_name, self.unique_symbols)\n",
    "\n",
    "        return self.train_test_return(train_yaml_name, eval_yaml_name, train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "0a1558bd-e1fd-4b75-bca9-5b8b224ce04e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-22 22:10:00,107] Using an existing study with name 'nequip_rand_3A4' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train yaml file for the 1. iteration of random 3A4\n",
      "        with project_resources/optuna/nequip/random/3A4 as root and was successfully created \n",
      "The evaluation yaml file for the 1. iteration of rand 3A4\n",
      "        with project_resources/optuna/nequip/random/3A4 as root and was successfully created \n",
      "running command: nequip-train project_resources/optuna/nequip/random/3A4/rand_3A4_nequip_train_1.yaml\n",
      "Torch device: cpu\n",
      "Successfully loaded the data set of type ASEDataset(56)...\n",
      "Replace string dataset_per_atom_total_energy_std to 0.04218994081020355\n",
      "Replace string dataset_per_atom_total_energy_mean to -0.08114522695541382\n",
      "Atomic outputs are scaled by: [H, C, N, O, F, P, S, Cl: 0.042190], shifted by [H, C, N, O, F, P, S, Cl: -0.081145].\n",
      "Replace string dataset_total_energy_std to tensor([1.4029])\n",
      "Initially outputs are globally scaled by: tensor([1.4029]), total_energy are globally shifted by None.\n",
      "Successfully built the network...\n",
      "Number of weights: 240664\n",
      "Number of trainable weights: 240664\n",
      "! Starting training ...\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_e        e_mae       e_rmse\n",
      "      0     1         1.15         1.15         1.32         1.51\n",
      "\n",
      "\n",
      "  Initialization     #    Epoch      wal       LR       loss_e         loss        e_mae       e_rmse\n",
      "! Initial Validation          0    0.780    0.564         1.15         1.15         1.32         1.51\n",
      "Wall time: 0.7797690739971586\n",
      "! Best model        0    1.153\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_e        e_mae       e_rmse\n",
      "      1     1         1.09         1.09         1.28         1.47\n",
      "      1     2            4            4         2.53         2.81\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_e        e_mae       e_rmse\n",
      "      1     1         1.89         1.89         1.41         1.93\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_e         loss        e_mae       e_rmse\n",
      "! Train               1    6.905    0.564         2.55         2.55         1.82         2.15\n",
      "! Validation          1    6.905    0.564         1.89         1.89         1.41         1.93\n",
      "Wall time: 6.9050437479963875\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_e        e_mae       e_rmse\n",
      "      2     1         1.44         1.44         1.39         1.68\n",
      "      2     2         2.77         2.77         2.05         2.33\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_e        e_mae       e_rmse\n",
      "      2     1         2.74         2.74         1.72         2.32\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_e         loss        e_mae       e_rmse\n",
      "! Train               2   10.732    0.564          2.1          2.1         1.67         1.99\n",
      "! Validation          2   10.732    0.564         2.74         2.74         1.72         2.32\n",
      "Wall time: 10.732151192001766\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_e        e_mae       e_rmse\n",
      "      3     1         2.33         2.33         1.77         2.14\n",
      "      3     2         2.46         2.46         1.89          2.2\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_e        e_mae       e_rmse\n",
      "      3     1         1.15         1.15         1.26          1.5\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_e         loss        e_mae       e_rmse\n",
      "! Train               3   14.331    0.564         2.39         2.39         1.82         2.17\n",
      "! Validation          3   14.331    0.564         1.15         1.15         1.26          1.5\n",
      "Wall time: 14.33137340599933\n",
      "! Best model        3    1.150\n",
      "! Stop training: max epochs\n",
      "Wall time: 14.359901279000042\n",
      "Cumulative wall time: 14.359901279000042\n",
      "running command: nequip-evaluate --train-dir project_resources/optuna/nequip/random/3A4/1 \n",
      "              --dataset-config project_resources/optuna/nequip/random/3A4/rand_3A4_nequip_eval_1.yaml \n",
      "              --output project_resources/optuna/nequip/random/3A4/1/output.xyz \n",
      "              --log project_resources/optuna/nequip/random/3A4/1/evaluation_log.txt\n",
      "Using device: cpu\n",
      "Loading model... \n",
      "loaded model from training session\n",
      "Loading dataset...\n",
      "Loaded dataset specified in rand_3A4_nequip_eval_1.yaml.\n",
      "Using all frames from the specified test dataset, yielding a test set size of 14 frames.\n",
      "Starting...\n",
      "  0%|                                                    | 0/14 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "100%|███████████████████████████████████████████| 14/14 [00:00<00:00, 16.02it/s]\n",
      "e_mae = 1.2147 | e_rmse = 1.6266\n",
      "\n",
      "--- Final result: ---\n",
      "               e_mae =  1.214671           \n",
      "              e_rmse =  1.626565           \n",
      "               e_mae =  1.214671           \n",
      "              e_rmse =  1.626565           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-22 22:10:25,662] Trial 1 finished with value: 1.626565 and parameters: {'batch_size': 25, 'learning_rate': 0.5642759064409827, 'num_layers': 4}. Best is trial 0 with value: 1.588775.\n",
      "[I 2023-11-22 22:10:25,668] Using an existing study with name 'nequip_rand_RLM' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train yaml file for the 1. iteration of random RLM\n",
      "        with project_resources/optuna/nequip/random/RLM as root and was successfully created \n",
      "The evaluation yaml file for the 1. iteration of rand RLM\n",
      "        with project_resources/optuna/nequip/random/RLM as root and was successfully created \n",
      "running command: nequip-train project_resources/optuna/nequip/random/RLM/rand_RLM_nequip_train_1.yaml\n",
      "Torch device: cpu\n",
      "Processing dataset...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23533/428338992.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtuner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNequIPTuner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique_symbols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misozyme\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_mols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_dataset_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{root}/nequip.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/soc/lib/python3.7/site-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgc_after_trial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m             \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m         )\n\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/soc/lib/python3.7/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    115\u001b[0m                             \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                             \u001b[0mtime_start\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                             \u001b[0mprogress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m                         )\n\u001b[1;32m    119\u001b[0m                     )\n",
      "\u001b[0;32m~/anaconda3/envs/soc/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/soc/lib/python3.7/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_threads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                 \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/soc/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/soc/lib/python3.7/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1058\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sampler = samplers[\"TPESampler\"]\n",
    "pruner = pruners[\"BasePruner\"]\n",
    "n_trials = 1\n",
    "for benchmark in tdc_benchmarks:\n",
    "    root = f\"project_resources/optuna/nequip/{benchmark}\"\n",
    "    lock_obj = optuna.storages.JournalFileOpenLock(root + \"/nequip_journal.log\")\n",
    "    storage = JournalStorage(\n",
    "        JournalFileStorage(root + \"/nequip_journal.log\", lock_obj=lock_obj)\n",
    "    )\n",
    "\n",
    "    study = optuna.create_study(study_name=f\"nequip_{benchmark}\", directions=[\"minimize\"],\n",
    "                                pruner=pruner, storage=storage, load_if_exists=True)\n",
    "\n",
    "    run_name = benchmark\n",
    "    \n",
    "    num_mols = len(tdc_datasets[benchmark][\"train\"][\"Drug\"])\n",
    "    train_dataset_file_name = f\"project_resources/nequip/positions/{benchmark}_train.extxyz\"\n",
    "    eval_dataset_file_name = f\"project_resources/nequip/positions/{benchmark}_test.extxyz\"\n",
    "    tuner = NequIPTuner(run_name, root, unique_symbols[benchmark], num_mols, train_dataset_file_name, eval_dataset_file_name)\n",
    "\n",
    "    study.optimize(tuner.objective, n_trials=n_trials, n_jobs=-1)\n",
    "    joblib.dump(study, f\"{root}/nequip.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda5c822-db51-48e7-b7ce-acae2cbdfdfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
