{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad2316e2-c727-4da5-9148-8354b9aa5579",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from /home/lukas/Documents/datacytochromy/project_resources/cytochrome_P450.ipynb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import yaml\n",
    "import optuna\n",
    "import joblib\n",
    "import copy\n",
    "import subprocess\n",
    "import shutil\n",
    "from tdc.single_pred import ADME\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from optuna.storages import JournalFileStorage, JournalStorage\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from ase import Atoms\n",
    "from ase.io import read, write\n",
    "from ase.calculators.singlepoint import SinglePointCalculator\n",
    "from project_resources.import_utils import NotebookFinder\n",
    "sys.meta_path.append(NotebookFinder())\n",
    "from project_resources.cytochrome_P450 import smiles_to_xyz_file, get_unique_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e003390-4867-4fb4-b0d0-8ab8bdf9af84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tdc_benchmarks = [\"obach\", \"microsome\", \"hepatocyte\"]\n",
    "# sampler - a method used to generate new sets of hyperparameters in each iteration of the optimization process\n",
    "samplers = {\n",
    "    'RandomSampler': optuna.samplers.RandomSampler,          # Sampler that selects hyperparameters randomly from the search space.\n",
    "    'GridSampler': optuna.samplers.GridSampler,              # Sampler that performs a grid search over the hyperparameter space.\n",
    "    'TPESampler': optuna.samplers.TPESampler,                # Sampler that uses a tree-structured Parzen estimator to model the objective function and sample new points from the search space.\n",
    "    'CmaEsSampler': optuna.samplers.CmaEsSampler,            # Sampler that uses the Covariance Matrix Adaptation Evolution Strategy algorithm to efficiently search the hyperparameter space.\n",
    "    'NSGAIISampler': optuna.samplers.NSGAIISampler,          # Multi-objective evolutionary algorithm that generates new samples using non-dominated sorting and crowding distance selection.\n",
    "    'QMCSampler': optuna.samplers.QMCSampler,                # Quasi-Monte Carlo sampler that uses low-discrepancy sequences to sample the search space in a more efficient and evenly distributed way than random sampling.\n",
    "    'BoTorchSampler': optuna.integration.BoTorchSampler,     # Sampler that leverages the BoTorch library for Bayesian optimization and can handle both continuous and categorical hyperparameters.\n",
    "    'BruteForceSampler': optuna.samplers.BruteForceSampler,  # Sampler that exhaustively evaluates all possible combinations of hyperparameters in the search space.\n",
    "}\n",
    "# pruner - a technique used to eliminate unpromising trials during the course of hyperparameter optimization.\n",
    "pruners = {\n",
    "    'BasePruner': optuna.pruners.BasePruner,                            # This is the base class for all pruning strategies in Optuna. It provides a skeleton for implementing custom pruning strategies.\n",
    "    'MedianPruner': optuna.pruners.MedianPruner,                        # A pruner that prunes unpromising trials that have median objective values, as determined in previous steps.\n",
    "    'SuccessiveHalvingPruner': optuna.pruners.SuccessiveHalvingPruner,  # This pruner repeatedly splits trials into halves, discarding the lower performing half at each iteration.\n",
    "    'HyperbandPruner': optuna.pruners.HyperbandPruner,                  # This pruner implements the Hyperband algorithm, which selects promising trials and runs them with different resource allocation schemes to determine the best one.\n",
    "    'PercentilePruner': optuna.pruners.PercentilePruner,                # A pruner that prunes unpromising trials based on their percentile rank relative to all completed trials.\n",
    "    'NopPruner': optuna.pruners.NopPruner,                              # A pruner that does nothing and does not prune any trials.\n",
    "    'ThresholdPruner': optuna.pruners.ThresholdPruner,                  # This pruner prunes trials that have not reached a certain level of performance (i.e., objective value).\n",
    "    'PatientPruner': optuna.pruners.PatientPruner,                      # This pruner prunes trials that do not show improvement over a certain number of steps (or epochs).\n",
    "}\n",
    "tdc_datasets = {}\n",
    "unique_symbols = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a246d306-a59b-412e-a34a-bab7500ed5e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n",
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n",
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "obach = ADME(name='Half_Life_Obach')\n",
    "obach_split = obach.get_split()\n",
    "tdc_datasets[\"obach\"] = obach_split\n",
    "microsome = ADME(name='Clearance_Microsome_AZ')\n",
    "microsome_split = microsome.get_split()\n",
    "tdc_datasets[\"microsome\"] = microsome_split\n",
    "hepatocyte = ADME(name='Clearance_Hepatocyte_AZ')\n",
    "hepatocyte_split = hepatocyte.get_split()\n",
    "tdc_datasets[\"hepatocyte\"] = hepatocyte_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0c3125a-dbff-4fb6-8555-7eca09e47d0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obach_train.extxyz already exists\n",
      "obach_test.extxyz already exists\n",
      "microsome_train.extxyz already exists\n",
      "microsome_test.extxyz already exists\n",
      "hepatocyte_train.extxyz already exists\n",
      "hepatocyte_test.extxyz already exists\n"
     ]
    }
   ],
   "source": [
    "for benchmark in tdc_benchmarks:\n",
    "    benchmark_train_smiles = tdc_datasets[benchmark][\"train\"][\"Drug\"]\n",
    "    benchmark_test_smiles = tdc_datasets[benchmark][\"test\"][\"Drug\"]\n",
    "    \n",
    "    benchmark_train_halflives = tdc_datasets[benchmark][\"train\"][\"Y\"]\n",
    "    reshaped_train_halflife = np.array(benchmark_train_halflives).reshape(-1, 1)\n",
    "    scaler = MinMaxScaler().fit(reshaped_train_halflife)\n",
    "    train_halflife_scaled = scaler.transform(reshaped_train_halflife)\n",
    "    train_halflives_scaled = np.array([val[0] for val in train_halflife_scaled])\n",
    "\n",
    "    benchmark_test_halflives = tdc_datasets[benchmark][\"test\"][\"Y\"]\n",
    "    reshaped_test_halflife = np.array(benchmark_test_halflives).reshape(-1, 1)\n",
    "    scaler = MinMaxScaler().fit(reshaped_test_halflife)\n",
    "    test_halflife_scaled = scaler.transform(reshaped_test_halflife)\n",
    "    test_halflives_scaled = np.array([val[0] for val in test_halflife_scaled])\n",
    "    \n",
    "    file_location = \"project_resources/nequip/positions\"\n",
    "\n",
    "    smiles_to_xyz_file(benchmark_train_smiles, train_halflives_scaled, f\"{file_location}/{benchmark}_train.extxyz\")\n",
    "    smiles_to_xyz_file(benchmark_test_smiles, test_halflives_scaled, f\"{file_location}/{benchmark}_test.extxyz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8970ff85-5f0c-4042-9dd8-3eb6881ad554",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chemical elements present in obach: ['N', 'H', 'I', 'Li', 'O', 'P', 'C', 'Na', 'Br', 'Cl', 'F', 'S', 'B']\n",
      "chemical elements present in microsome: ['N', 'H', 'I', 'O', 'P', 'C', 'Br', 'Cl', 'F', 'S', 'B']\n",
      "chemical elements present in hepatocyte: ['N', 'H', 'I', 'O', 'P', 'C', 'Br', 'Cl', 'F', 'S', 'B']\n"
     ]
    }
   ],
   "source": [
    "for benchmark in tdc_benchmarks:\n",
    "    benchmark_train_smiles = tdc_datasets[benchmark][\"train\"][\"Drug\"]\n",
    "    benchmark_test_smiles = tdc_datasets[benchmark][\"test\"][\"Drug\"]\n",
    "    benchmark_all_smiles = list(benchmark_train_smiles) + list(benchmark_test_smiles)\n",
    "    \n",
    "    unique_symbs = get_unique_elements(benchmark_all_smiles)\n",
    "    unique_symbols[benchmark] = list(unique_symbs)\n",
    "    print(f\"chemical elements present in {benchmark}: {unique_symbols[benchmark]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea19ddf4-57db-40e7-a3b0-37e868a4a912",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_nequip_eval_yaml(yaml_path, root, trial_number, eval_dataset_file_name, unique_symbols):\n",
    "    with open(yaml_path, \"w\") as out_f:\n",
    "        data = {}\n",
    "        \n",
    "        # root is the same as nequip-train, except at the end is also included the number of the run\n",
    "        data[\"root\"] = root\n",
    "        data[\"chemical_symbols\"] = unique_symbols\n",
    "        data[\"dataset_file_name\"] = eval_dataset_file_name\n",
    "        data[\"dataset\"] = \"ase\"\n",
    "        \n",
    "        yaml.dump(data, out_f)\n",
    "        benchmark = root.split(\"/\")[-1]\n",
    "        print(f\"\"\"The evaluation yaml file for the {trial_number}. iteration of {benchmark}\n",
    "        with {root} as root and was successfully created \"\"\")\n",
    "        out_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "761c3f13-7be7-444a-841b-9df42bd185d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_nequip_train_yaml(yaml_path, batch_size, learning_rate, num_layers, root, trial_number,\n",
    "                       train_dataset_file_name, unique_symbols, num_mols):\n",
    "    with open(yaml_path, \"w\") as out_f:\n",
    "        data = {}\n",
    "\n",
    "        data[\"batch_size\"] = batch_size\n",
    "        data[\"learning_rate\"] = learning_rate\n",
    "        data[\"num_layers\"] = num_layers\n",
    "\n",
    "        data[\"root\"] = root  # project_resources/nequip/{dataset}\n",
    "        data[\"run_name\"] = str(trial_number)  # number of the trial for a given combination of splitter and isozyme\n",
    "        data[\"dataset_file_name\"] = train_dataset_file_name\n",
    "        data[\"seed\"] = 123\n",
    "        data[\"dataset_seed\"] = 456\n",
    "        data[\"append\"] = True\n",
    "        data[\"model_builders\"] = [\"SimpleIrrepsConfig\", \"EnergyModel\", \"PerSpeciesRescale\", \"RescaleEnergyEtc\"]\n",
    "\n",
    "        data[\"max_epochs\"] = 3\n",
    "        data[\"r_max\"] = 0.4  # cutoff radius in Angstroms; changing the value has no effect on validation_e_mae\n",
    "        data[\"l_max\"] = 2\n",
    "        data[\"parity\"] = False  # slighly worse performance than True, but training takes way less time\n",
    "        data[\"num_features\"] = 32\n",
    "        data[\"num_basis\"] = 8\n",
    "\n",
    "        data[\"dataset\"] = \"ase\"\n",
    "        key_mapping = {\"z\": \"atomic_numbers\", \"E\": \"total_energy\", \"R\": \"pos\"}\n",
    "        data[\"key_mapping\"] = key_mapping\n",
    "        data[\"npz_fixed_field_keys\"] = \"atomic_numbers\"\n",
    "        data[\"chemical_symbols\"] = unique_symbols\n",
    "        data[\"wandb\"] = False  # impossible to use wandb inside a notebook (cannot choose an option)\n",
    "\n",
    "        num_train_val = num_mols  # number of molecules in the dataset\n",
    "        num_train = int(np.floor(0.8 * num_train_val))\n",
    "        num_validation = int(num_train_val - num_train)\n",
    "        data[\"n_train\"] = num_train\n",
    "        data[\"n_val\"] = num_validation\n",
    "        data[\"train_val_split\"] = \"sequential\"\n",
    "        data[\"validation_batch_size\"] = num_validation\n",
    "        data[\"loss_coeffs\"] = \"total_energy\"\n",
    "        data[\"optimizer_name\"] = \"Adam\"\n",
    "\n",
    "        benchmark = root.split(\"/\")[-1]\n",
    "        \n",
    "        yaml.dump(data, out_f)\n",
    "        print(f\"\"\"The train yaml file for the {trial_number}. iteration of {benchmark}\n",
    "        with {root} as root and was successfully created \"\"\")\n",
    "        out_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42967962-bdf4-457d-827c-53e23f58c510",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_nequip_xyz_out(xyz):\n",
    "    # get the predicted half-lives from xyz file created after using nequip-evaluate --output\n",
    "    with open(xyz) as f:\n",
    "        energies = [float(re.search(r'energy=(-?\\d+\\.\\d+)', line).group(1)) for line in f if \"energy\" in line]\n",
    "    return energies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbc66bfd-14db-49d5-b308-97040b85633c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_nequip_log(log):\n",
    "    # get the rmse value from the log file created after using nequip-evaluate --log\n",
    "    with open(log) as l:\n",
    "        file_content = l.read()\n",
    "    # use regular expressions to find the rmse value\n",
    "    rmse = re.search(r'\\s*e_rmse\\s*=\\s*(\\d+\\.\\d+)', file_content)\n",
    "    return float(rmse.group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2e6c1ad-90fb-4b18-8a05-6e326f120c91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NequIPTuner():\n",
    "    def __init__(self, run_name, unique_symbols, num_mols, train_dataset_file_name, eval_dataset_file_name):\n",
    "        self.run_name = run_name\n",
    "        self.root = root\n",
    "        self.unique_symbols = unique_symbols\n",
    "        self.num_mols = num_mols  # number of mols present in the train .extxyz file\n",
    "        self.train_dataset_file_name = train_dataset_file_name\n",
    "        self.eval_dataset_file_name = eval_dataset_file_name\n",
    "\n",
    "    def sample_params(self, trial: optuna.Trial):\n",
    "        batch_size = trial.suggest_int(\"batch_size\", 2, 50)\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 0.001, 0.1)\n",
    "        num_layers = trial.suggest_int(\"num_layers\", 3, 5)\n",
    "        return {\n",
    "            \"batch_size\": batch_size,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"num_layers\": num_layers\n",
    "        }\n",
    "\n",
    "    def train_test_return(self, train_yaml_name, eval_yaml_name, train_dir, return_predictions=False):\n",
    "        xyz_out = f\"{train_dir}/output.xyz\"\n",
    "        log_file = f\"{train_dir}/evaluation_log.txt\"\n",
    "        \n",
    "        print(f\"running command: nequip-train {train_yaml_name}\")\n",
    "        !nequip-train {train_yaml_name}\n",
    "        # {train_dir} ... project_resources/optuna/nequip/random/3A4/1\n",
    "        # {eval_yaml_name} ... eval_dataset.yaml\n",
    "        # {xyz_out} ... project_resources/optuna/nequip/random/3A4/1/output.xyz\n",
    "        # ^ Parse this to get the predicted half-lives ^\n",
    "        # {log} ... project_resources/optuna/nequip/random/3A4/1/eval_log.txt\n",
    "        \n",
    "        print(f\"\"\"running command: nequip-evaluate --train-dir {train_dir} \n",
    "              --dataset-config {eval_yaml_name} \n",
    "              --output {xyz_out} \n",
    "              --log {log_file}\"\"\")\n",
    "        !nequip-evaluate --train-dir {train_dir} --dataset-config {eval_yaml_name} --output {xyz_out} --log {log_file}\n",
    "\n",
    "        rmse = parse_nequip_log(log_file)\n",
    "        \n",
    "        # delete the dirs and files that were created, as they are no longer needed\n",
    "        shutil.rmtree(train_dir)\n",
    "        os.remove(train_yaml_name)\n",
    "        os.remove(eval_yaml_name)\n",
    "        \n",
    "        if return_predictions:\n",
    "            predictions = parse_nequip_xyz_out(xyz_out)\n",
    "            return rmse, predictions\n",
    "        else:\n",
    "            return rmse\n",
    "\n",
    "    def objective(self, trial=None):\n",
    "        parameters = self.sample_params(trial)\n",
    "        root = f\"project_resources/optuna/nequip/{self.run_name}\"\n",
    "        train_yaml_name = f\"{root}/nequip_train_{trial.number}.yaml\"\n",
    "        eval_yaml_name = f\"{root}/nequip_eval_{trial.number}.yaml\"\n",
    "        train_dir = root + f\"/{trial.number}\"\n",
    "        \n",
    "        # create train yaml file for this trial\n",
    "        create_nequip_train_yaml(train_yaml_name, parameters[\"batch_size\"], parameters[\"learning_rate\"],\n",
    "                           parameters[\"num_layers\"], root, trial.number, self.train_dataset_file_name,\n",
    "                           self.unique_symbols, self.num_mols)\n",
    "\n",
    "        # create evaluation yaml file for this trial\n",
    "        create_nequip_eval_yaml(eval_yaml_name, root, trial.number,\n",
    "                                self.eval_dataset_file_name, self.unique_symbols)\n",
    "\n",
    "        return self.train_test_return(train_yaml_name, eval_yaml_name, train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a1558bd-e1fd-4b75-bca9-5b8b224ce04e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 19:18:21,425] A new study created in Journal with name: nequip_obach\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obach\n",
      "The train yaml file for the 0. iteration of obach\n",
      "        with project_resources/optuna/nequip/obach as root and was successfully created \n",
      "The evaluation yaml file for the 0. iteration of obach\n",
      "        with project_resources/optuna/nequip/obach as root and was successfully created \n",
      "running command: nequip-train project_resources/optuna/nequip/obach/nequip_train_0.yaml\n",
      "Torch device: cpu\n",
      "Processing dataset...\n",
      "multiprocessing.pool.RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/data/dataset.py\", line 790, in _ase_dataset_reader\n",
      "    if global_index in include_frames\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/data/AtomicData.py\", line 449, in from_ase\n",
      "    **add_fields,\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/data/AtomicData.py\", line 318, in from_points\n",
      "    pbc=pbc,\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/data/AtomicData.py\", line 777, in neighbor_list_and_relative_vec\n",
      "    f\"Every single atom has no neighbors within the cutoff r_max={r_max} (after eliminating self edges, no edges remain in this system)\"\n",
      "ValueError: Every single atom has no neighbors within the cutoff r_max=0.4 (after eliminating self edges, no edges remain in this system)\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/utils/auto_init.py\", line 232, in instantiate\n",
      "    instance = builder(**positional_args, **final_optional_args)\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/data/dataset.py\", line 887, in __init__\n",
      "    type_mapper=type_mapper,\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/data/dataset.py\", line 166, in __init__\n",
      "    super().__init__(root=root, type_mapper=type_mapper)\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/data/dataset.py\", line 50, in __init__\n",
      "    super().__init__(root=root, transform=type_mapper)\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/utils/torch_geometric/dataset.py\", line 91, in __init__\n",
      "    self._process()\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/utils/torch_geometric/dataset.py\", line 176, in _process\n",
      "    self.process()\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/data/dataset.py\", line 218, in process\n",
      "    data = self.get_data()\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/data/dataset.py\", line 964, in get_data\n",
      "    datas = p.map(reader, list(range(n_proc)))\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/multiprocessing/pool.py\", line 268, in map\n",
      "    return self._map_async(func, iterable, mapstar, chunksize).get()\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/multiprocessing/pool.py\", line 657, in get\n",
      "    raise self._value\n",
      "ValueError: Every single atom has no neighbors within the cutoff r_max=0.4 (after eliminating self edges, no edges remain in this system)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lukas/anaconda3/envs/soc/bin/nequip-train\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/scripts/train.py\", line 72, in main\n",
      "    trainer = fresh_start(config)\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/scripts/train.py\", line 148, in fresh_start\n",
      "    dataset = dataset_from_config(config, prefix=\"dataset\")\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/data/_build.py\", line 82, in dataset_from_config\n",
      "    optional_args=config,\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/utils/auto_init.py\", line 236, in instantiate\n",
      "    ) from e\n",
      "RuntimeError: Failed to build object with prefix `dataset` using builder `ASEDataset`\n",
      "running command: nequip-evaluate --train-dir project_resources/optuna/nequip/obach/0 \n",
      "              --dataset-config project_resources/optuna/nequip/obach/nequip_eval_0.yaml \n",
      "              --output project_resources/optuna/nequip/obach/0/output.xyz \n",
      "              --log project_resources/optuna/nequip/obach/0/evaluation_log.txt\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lukas/anaconda3/envs/soc/bin/nequip-evaluate\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/scripts/evaluate.py\", line 142, in main\n",
      "    str(args.train_dir / \"trainer.pth\"), map_location=\"cpu\"\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/torch/serialization.py\", line 699, in load\n",
      "    with _open_file_like(f, 'rb') as opened_file:\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/torch/serialization.py\", line 231, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/torch/serialization.py\", line 212, in __init__\n",
      "    super(_open_file, self).__init__(open(name, mode))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'project_resources/optuna/nequip/obach/0/trainer.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-12-04 19:19:08,993] Trial 0 failed with parameters: {'batch_size': 28, 'learning_rate': 0.0462399708745702, 'num_layers': 3} because of the following error: FileNotFoundError(2, 'No such file or directory').\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_27390/992946843.py\", line 67, in objective\n",
      "    return self.train_test_return(train_yaml_name, eval_yaml_name, train_dir)\n",
      "  File \"/tmp/ipykernel_27390/992946843.py\", line 38, in train_test_return\n",
      "    rmse = parse_nequip_log(log_file)\n",
      "  File \"/tmp/ipykernel_27390/3300226570.py\", line 3, in parse_nequip_log\n",
      "    with open(log) as l:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'project_resources/optuna/nequip/obach/0/evaluation_log.txt'\n",
      "[W 2023-12-04 19:19:08,994] Trial 0 failed with value None.\n",
      "[I 2023-12-04 19:19:09,001] A new study created in Journal with name: nequip_microsome\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "microsome\n",
      "The train yaml file for the 0. iteration of microsome\n",
      "        with project_resources/optuna/nequip/microsome as root and was successfully created \n",
      "The evaluation yaml file for the 0. iteration of microsome\n",
      "        with project_resources/optuna/nequip/microsome as root and was successfully created \n",
      "running command: nequip-train project_resources/optuna/nequip/microsome/nequip_train_0.yaml\n",
      "Torch device: cpu\n",
      "Processing dataset...\n",
      "Loaded data: Batch(atomic_numbers=[22572, 1], batch=[22572], cell=[772, 3, 3], edge_cell_shift=[182674, 3], edge_index=[2, 182674], pbc=[772, 3], pos=[22572, 3], ptr=[773], total_energy=[772, 1])\n",
      "    processed data size: ~5.52 MB\n",
      "Cached processed data to disk\n",
      "Done!\n",
      "Successfully loaded the data set of type ASEDataset(772)...\n",
      "Replace string dataset_per_atom_total_energy_std to 0.0097426138818264\n",
      "Replace string dataset_per_atom_total_energy_mean to 0.0061898925341665745\n",
      "Atomic outputs are scaled by: [H, B, C, N, O, F, P, S, Cl, Br, I: 0.009743], shifted by [H, B, C, N, O, F, P, S, Cl, Br, I: 0.006190].\n",
      "Replace string dataset_total_energy_std to tensor([0.2710])\n",
      "Initially outputs are globally scaled by: tensor([0.2710]), total_energy are globally shifted by None.\n",
      "Successfully built the network...\n",
      "Number of weights: 380856\n",
      "Number of trainable weights: 380856\n",
      "! Starting training ...\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_e        e_mae       e_rmse\n",
      "      0     1         2.49         2.49        0.313        0.427\n",
      "\n",
      "\n",
      "  Initialization     #    Epoch      wal       LR       loss_e         loss        e_mae       e_rmse\n",
      "! Initial Validation          0    4.038   0.0982         2.49         2.49        0.313        0.427\n",
      "Wall time: 4.037865592999879\n",
      "! Best model        0    2.487\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_e        e_mae       e_rmse\n",
      "      1     1         1.07         1.07          0.2         0.28\n",
      "      1     2        0.837        0.837        0.186        0.248\n",
      "      1     3         1.35         1.35        0.239        0.314\n",
      "      1     4         1.28         1.28         0.22        0.306\n",
      "      1     5         1.41         1.41        0.225        0.321\n",
      "      1     6        0.751        0.751        0.189        0.235\n",
      "      1     7        0.593        0.593        0.165        0.209\n",
      "      1     8        0.899        0.899        0.195        0.257\n",
      "      1     9        0.871        0.871         0.18        0.253\n",
      "      1    10        0.396        0.396        0.144        0.171\n",
      "      1    11         1.05         1.05        0.177        0.277\n",
      "      1    12         1.15         1.15        0.193        0.291\n",
      "      1    13            1            1         0.18        0.271\n",
      "      1    14        0.762        0.762        0.157        0.237\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_e        e_mae       e_rmse\n",
      "      1     1          2.4          2.4        0.304         0.42\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_e         loss        e_mae       e_rmse\n",
      "! Train               1   40.102   0.0982        0.958        0.958         0.19        0.266\n",
      "! Validation          1   40.102   0.0982          2.4          2.4        0.304         0.42\n",
      "Wall time: 40.10266128999865\n",
      "! Best model        1    2.404\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_e        e_mae       e_rmse\n",
      "      2     1        0.637        0.637        0.164        0.216\n",
      "      2     2          1.3          1.3         0.21        0.309\n",
      "      2     3        0.821        0.821        0.175        0.245\n",
      "      2     4        0.985        0.985        0.201        0.269\n",
      "      2     5         1.03         1.03        0.218        0.275\n",
      "      2     6         1.48         1.48        0.244         0.33\n",
      "      2     7        0.831        0.831        0.182        0.247\n",
      "      2     8        0.958        0.958        0.198        0.265\n",
      "      2     9        0.719        0.719        0.162         0.23\n",
      "      2    10        0.649        0.649        0.152        0.218\n",
      "      2    11         1.01         1.01        0.188        0.273\n",
      "      2    12         1.62         1.62         0.24        0.345\n",
      "      2    13        0.611        0.611        0.155        0.212\n",
      "      2    14        0.598        0.598        0.145        0.209\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_e        e_mae       e_rmse\n",
      "      2     1         2.48         2.48        0.306        0.426\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_e         loss        e_mae       e_rmse\n",
      "! Train               2   75.168   0.0982        0.947        0.947        0.189        0.265\n",
      "! Validation          2   75.168   0.0982         2.48         2.48        0.306        0.426\n",
      "Wall time: 75.16832665699985\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_e        e_mae       e_rmse\n",
      "      3     1        0.698        0.698        0.144        0.226\n",
      "      3     2        0.923        0.923        0.172         0.26\n",
      "      3     3        0.899        0.899        0.177        0.257\n",
      "      3     4         1.36         1.36        0.227        0.316\n",
      "      3     5        0.545        0.545         0.16          0.2\n",
      "      3     6         0.33         0.33        0.136        0.156\n",
      "      3     7        0.928        0.928        0.188        0.261\n",
      "      3     8         1.45         1.45        0.226        0.327\n",
      "      3     9         1.15         1.15        0.194         0.29\n",
      "      3    10         1.14         1.14        0.208         0.29\n",
      "      3    11        0.977        0.977        0.183        0.268\n",
      "      3    12          1.2          1.2        0.223        0.297\n",
      "      3    13         1.01         1.01        0.195        0.272\n",
      "      3    14        0.563        0.563        0.156        0.203\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_e        e_mae       e_rmse\n",
      "      3     1          2.4          2.4        0.305         0.42\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_e         loss        e_mae       e_rmse\n",
      "! Train               3  110.703   0.0982        0.941        0.941        0.185        0.264\n",
      "! Validation          3  110.703   0.0982          2.4          2.4        0.305         0.42\n",
      "Wall time: 110.70282229800068\n",
      "! Best model        3    2.398\n",
      "! Stop training: max epochs\n",
      "Wall time: 110.74407699799849\n",
      "Cumulative wall time: 110.74407699799849\n",
      "running command: nequip-evaluate --train-dir project_resources/optuna/nequip/microsome/0 \n",
      "              --dataset-config project_resources/optuna/nequip/microsome/nequip_eval_0.yaml \n",
      "              --output project_resources/optuna/nequip/microsome/0/output.xyz \n",
      "              --log project_resources/optuna/nequip/microsome/0/evaluation_log.txt\n",
      "Using device: cpu\n",
      "Loading model... \n",
      "loaded model from training session\n",
      "Loading dataset...\n",
      "Processing dataset...\n",
      "Done!\n",
      "Loaded dataset specified in nequip_eval_0.yaml.\n",
      "Using all frames from the specified test dataset, yielding a test set size of 220 frames.\n",
      "Starting...\n",
      "  0%|                                                   | 0/220 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      " 23%|█████████▌                                | 50/220 [00:01<00:05, 30.14it/s]\n",
      " 45%|██████████████████▋                      | 100/220 [00:03<00:04, 26.92it/s]\n",
      " 68%|███████████████████████████▉             | 150/220 [00:04<00:02, 34.85it/s]\n",
      " 91%|█████████████████████████████████████▎   | 200/220 [00:05<00:00, 37.44it/s]\n",
      "100%|█████████████████████████████████████████| 220/220 [00:06<00:00, 35.90it/s]\n",
      "e_mae = 0.2112 | e_rmse = 0.3061\n",
      "\n",
      "--- Final result: ---\n",
      "               e_mae =  0.211152           \n",
      "              e_rmse =  0.306079           \n",
      "               e_mae =  0.211152           \n",
      "              e_rmse =  0.306079           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 19:22:35,908] Trial 0 finished with value: 0.306079 and parameters: {'batch_size': 45, 'learning_rate': 0.09820617685184492, 'num_layers': 5}. Best is trial 0 with value: 0.306079.\n",
      "[I 2023-12-04 19:22:35,915] A new study created in Journal with name: nequip_hepatocyte\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hepatocyte\n",
      "The train yaml file for the 0. iteration of hepatocyte\n",
      "        with project_resources/optuna/nequip/hepatocyte as root and was successfully created \n",
      "The evaluation yaml file for the 0. iteration of hepatocyte\n",
      "        with project_resources/optuna/nequip/hepatocyte as root and was successfully created \n",
      "running command: nequip-train project_resources/optuna/nequip/hepatocyte/nequip_train_0.yaml\n",
      "Torch device: cpu\n",
      "Processing dataset...\n",
      "Loaded data: Batch(atomic_numbers=[24917, 1], batch=[24917], cell=[849, 3, 3], edge_cell_shift=[208448, 3], edge_index=[2, 208448], pbc=[849, 3], pos=[24917, 3], ptr=[850], total_energy=[849, 1])\n",
      "    processed data size: ~6.27 MB\n",
      "Cached processed data to disk\n",
      "Done!\n",
      "Successfully loaded the data set of type ASEDataset(849)...\n",
      "Replace string dataset_per_atom_total_energy_std to 0.013990852050483227\n",
      "Replace string dataset_per_atom_total_energy_mean to 0.009903610683977604\n",
      "Atomic outputs are scaled by: [H, B, C, N, O, F, P, S, Cl, Br, I: 0.013991], shifted by [H, B, C, N, O, F, P, S, Cl, Br, I: 0.009904].\n",
      "Replace string dataset_total_energy_std to tensor([0.3265])\n",
      "Initially outputs are globally scaled by: tensor([0.3265]), total_energy are globally shifted by None.\n",
      "Successfully built the network...\n",
      "Number of weights: 296056\n",
      "Number of trainable weights: 296056\n",
      "! Starting training ...\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_e        e_mae       e_rmse\n",
      "      0     1         1.52         1.52        0.331        0.402\n",
      "\n",
      "\n",
      "  Initialization     #    Epoch      wal       LR       loss_e         loss        e_mae       e_rmse\n",
      "! Initial Validation          0    2.709   0.0251         1.52         1.52        0.331        0.402\n",
      "Wall time: 2.7088103759997466\n",
      "! Best model        0    1.518\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_e        e_mae       e_rmse\n",
      "      1     1        0.819        0.819        0.232        0.295\n",
      "      1     2         2.25         2.25        0.395         0.49\n",
      "      1     3        0.854        0.854        0.243        0.302\n",
      "      1     4         1.28         1.28        0.326         0.37\n",
      "      1     5        0.847        0.847         0.25        0.301\n",
      "      1     6        0.856        0.856        0.253        0.302\n",
      "      1     7        0.724        0.724        0.223        0.278\n",
      "      1     8         1.18         1.18        0.283        0.355\n",
      "      1     9         1.27         1.27        0.307        0.368\n",
      "      1    10        0.808        0.808        0.248        0.293\n",
      "      1    11         1.75         1.75        0.363        0.432\n",
      "      1    12         1.68         1.68        0.325        0.423\n",
      "      1    13         1.21         1.21        0.282        0.359\n",
      "      1    14        0.609        0.609        0.227        0.255\n",
      "      1    15         1.85         1.85        0.325        0.444\n",
      "      1    16         1.21         1.21        0.275        0.359\n",
      "      1    17        0.917        0.917        0.238        0.313\n",
      "      1    18         1.65         1.65        0.331        0.419\n",
      "      1    19        0.711        0.711        0.207        0.275\n",
      "      1    20        0.845        0.845        0.218          0.3\n",
      "      1    21          1.1          1.1        0.267        0.343\n",
      "      1    22        0.965        0.965        0.247        0.321\n",
      "      1    23         1.02         1.02         0.26         0.33\n",
      "      1    24        0.871        0.871        0.234        0.305\n",
      "      1    25        0.989        0.989        0.262        0.325\n",
      "      1    26        0.876        0.876        0.234        0.306\n",
      "      1    27          1.3          1.3        0.268        0.373\n",
      "      1    28        0.714        0.714        0.224        0.276\n",
      "      1    29        0.806        0.806        0.223        0.293\n",
      "      1    30        0.964        0.964         0.27        0.321\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_e        e_mae       e_rmse\n",
      "      1     1         1.52         1.52        0.293        0.402\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_e         loss        e_mae       e_rmse\n",
      "! Train               1   29.153   0.0251          1.1          1.1        0.268        0.342\n",
      "! Validation          1   29.153   0.0251         1.52         1.52        0.293        0.402\n",
      "Wall time: 29.1532696510003\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_e        e_mae       e_rmse\n",
      "      2     1        0.553        0.553        0.169        0.243\n",
      "      2     2         2.11         2.11        0.364        0.474\n",
      "      2     3         1.04         1.04        0.253        0.333\n",
      "      2     4         0.61         0.61        0.218        0.255\n",
      "      2     5        0.518        0.518        0.195        0.235\n",
      "      2     6         1.55         1.55        0.304        0.406\n",
      "      2     7         1.65         1.65        0.324        0.419\n",
      "      2     8          1.3          1.3        0.286        0.372\n",
      "      2     9         1.12         1.12         0.26        0.345\n",
      "      2    10         1.28         1.28        0.284        0.369\n",
      "      2    11         1.44         1.44        0.289        0.391\n",
      "      2    12        0.955        0.955        0.232        0.319\n",
      "      2    13        0.898        0.898        0.224        0.309\n",
      "      2    14          0.5          0.5        0.192        0.231\n",
      "      2    15         1.28         1.28        0.284         0.37\n",
      "      2    16         1.52         1.52        0.297        0.403\n",
      "      2    17        0.937        0.937        0.245        0.316\n",
      "      2    18          1.1          1.1        0.251        0.342\n",
      "      2    19         1.09         1.09        0.241        0.341\n",
      "      2    20        0.126        0.126        0.102        0.116\n",
      "      2    21         2.06         2.06        0.374        0.468\n",
      "      2    22         1.11         1.11        0.265        0.345\n",
      "      2    23        0.545        0.545        0.177        0.241\n",
      "      2    24        0.677        0.677        0.193        0.269\n",
      "      2    25         1.23         1.23        0.278        0.362\n",
      "      2    26         1.14         1.14        0.239        0.348\n",
      "      2    27         1.07         1.07        0.249        0.338\n",
      "      2    28        0.499        0.499         0.19        0.231\n",
      "      2    29        0.755        0.755        0.219        0.284\n",
      "      2    30        0.842        0.842        0.244          0.3\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_e        e_mae       e_rmse\n",
      "      2     1          1.5          1.5          0.3        0.399\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_e         loss        e_mae       e_rmse\n",
      "! Train               2   54.110   0.0251         1.05         1.05        0.248        0.335\n",
      "! Validation          2   54.110   0.0251          1.5          1.5          0.3        0.399\n",
      "Wall time: 54.110273573001905\n",
      "! Best model        2    1.497\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_e        e_mae       e_rmse\n",
      "      3     1         1.33         1.33        0.287        0.376\n",
      "      3     2         1.23         1.23        0.267        0.362\n",
      "      3     3         1.54         1.54        0.315        0.405\n",
      "      3     4         1.44         1.44        0.294        0.391\n",
      "      3     5         1.14         1.14        0.246        0.349\n",
      "      3     6         1.05         1.05        0.272        0.335\n",
      "      3     7        0.892        0.892        0.241        0.308\n",
      "      3     8        0.961        0.961        0.248         0.32\n",
      "      3     9         1.11         1.11        0.263        0.344\n",
      "      3    10         1.24         1.24        0.281        0.363\n",
      "      3    11         1.26         1.26         0.28        0.367\n",
      "      3    12        0.756        0.756        0.204        0.284\n",
      "      3    13        0.569        0.569        0.193        0.246\n",
      "      3    14         1.12         1.12        0.259        0.346\n",
      "      3    15        0.868        0.868        0.239        0.304\n",
      "      3    16         1.72         1.72        0.335        0.428\n",
      "      3    17        0.908        0.908        0.234        0.311\n",
      "      3    18        0.848        0.848        0.239        0.301\n",
      "      3    19        0.731        0.731        0.227        0.279\n",
      "      3    20        0.481        0.481        0.181        0.226\n",
      "      3    21        0.792        0.792        0.226        0.291\n",
      "      3    22        0.991        0.991        0.265        0.325\n",
      "      3    23         1.31         1.31          0.3        0.373\n",
      "      3    24        0.704        0.704          0.2        0.274\n",
      "      3    25         1.13         1.13         0.23        0.347\n",
      "      3    26        0.858        0.858        0.213        0.302\n",
      "      3    27         1.14         1.14        0.275        0.349\n",
      "      3    28         1.03         1.03        0.253        0.331\n",
      "      3    29         1.53         1.53        0.296        0.404\n",
      "      3    30        0.831        0.831        0.232        0.298\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_e        e_mae       e_rmse\n",
      "      3     1         1.52         1.52        0.291        0.402\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_e         loss        e_mae       e_rmse\n",
      "! Train               3   79.190   0.0251         1.05         1.05        0.254        0.335\n",
      "! Validation          3   79.190   0.0251         1.52         1.52        0.291        0.402\n",
      "Wall time: 79.19050715000049\n",
      "! Stop training: max epochs\n",
      "Wall time: 79.20995934500024\n",
      "Cumulative wall time: 79.20995934500024\n",
      "running command: nequip-evaluate --train-dir project_resources/optuna/nequip/hepatocyte/0 \n",
      "              --dataset-config project_resources/optuna/nequip/hepatocyte/nequip_eval_0.yaml \n",
      "              --output project_resources/optuna/nequip/hepatocyte/0/output.xyz \n",
      "              --log project_resources/optuna/nequip/hepatocyte/0/evaluation_log.txt\n",
      "Using device: cpu\n",
      "Loading model... \n",
      "loaded model from training session\n",
      "Loading dataset...\n",
      "Processing dataset...\n",
      "multiprocessing.pool.RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/data/dataset.py\", line 790, in _ase_dataset_reader\n",
      "    if global_index in include_frames\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/data/AtomicData.py\", line 449, in from_ase\n",
      "    **add_fields,\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/data/AtomicData.py\", line 318, in from_points\n",
      "    pbc=pbc,\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/data/AtomicData.py\", line 777, in neighbor_list_and_relative_vec\n",
      "    f\"Every single atom has no neighbors within the cutoff r_max={r_max} (after eliminating self edges, no edges remain in this system)\"\n",
      "ValueError: Every single atom has no neighbors within the cutoff r_max=0.4 (after eliminating self edges, no edges remain in this system)\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/utils/auto_init.py\", line 232, in instantiate\n",
      "    instance = builder(**positional_args, **final_optional_args)\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/data/dataset.py\", line 887, in __init__\n",
      "    type_mapper=type_mapper,\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/data/dataset.py\", line 166, in __init__\n",
      "    super().__init__(root=root, type_mapper=type_mapper)\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/data/dataset.py\", line 50, in __init__\n",
      "    super().__init__(root=root, transform=type_mapper)\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/utils/torch_geometric/dataset.py\", line 91, in __init__\n",
      "    self._process()\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/utils/torch_geometric/dataset.py\", line 176, in _process\n",
      "    self.process()\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/data/dataset.py\", line 218, in process\n",
      "    data = self.get_data()\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/data/dataset.py\", line 964, in get_data\n",
      "    datas = p.map(reader, list(range(n_proc)))\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/multiprocessing/pool.py\", line 268, in map\n",
      "    return self._map_async(func, iterable, mapstar, chunksize).get()\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/multiprocessing/pool.py\", line 657, in get\n",
      "    raise self._value\n",
      "ValueError: Every single atom has no neighbors within the cutoff r_max=0.4 (after eliminating self edges, no edges remain in this system)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lukas/anaconda3/envs/soc/bin/nequip-evaluate\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/scripts/evaluate.py\", line 257, in main\n",
      "    dataset = dataset_from_config(dataset_config)\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/data/_build.py\", line 82, in dataset_from_config\n",
      "    optional_args=config,\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/utils/auto_init.py\", line 236, in instantiate\n",
      "    ) from e\n",
      "RuntimeError: Failed to build object with prefix `dataset` using builder `ASEDataset`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-12-04 19:25:36,854] Trial 0 failed with parameters: {'batch_size': 23, 'learning_rate': 0.02506898471129209, 'num_layers': 4} because of the following error: AttributeError(\"'NoneType' object has no attribute 'group'\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_27390/992946843.py\", line 67, in objective\n",
      "    return self.train_test_return(train_yaml_name, eval_yaml_name, train_dir)\n",
      "  File \"/tmp/ipykernel_27390/992946843.py\", line 38, in train_test_return\n",
      "    rmse = parse_nequip_log(log_file)\n",
      "  File \"/tmp/ipykernel_27390/3300226570.py\", line 7, in parse_nequip_log\n",
      "    return float(rmse.group(1))\n",
      "AttributeError: 'NoneType' object has no attribute 'group'\n",
      "[W 2023-12-04 19:25:36,855] Trial 0 failed with value None.\n"
     ]
    }
   ],
   "source": [
    "sampler = samplers[\"TPESampler\"]\n",
    "pruner = pruners[\"BasePruner\"]\n",
    "n_trials = 1\n",
    "for benchmark in tdc_benchmarks:\n",
    "    print(benchmark)\n",
    "    root = f\"project_resources/optuna/nequip/{benchmark}\"\n",
    "    lock_obj = optuna.storages.JournalFileOpenLock(root + \"/nequip_journal.log\")\n",
    "    storage = JournalStorage(\n",
    "        JournalFileStorage(root + \"/nequip_journal.log\", lock_obj=lock_obj)\n",
    "    )\n",
    "\n",
    "    study = optuna.create_study(study_name=f\"nequip_{benchmark}\", directions=[\"minimize\"],\n",
    "                                pruner=pruner, storage=storage, load_if_exists=True)\n",
    "\n",
    "    run_name = benchmark\n",
    "    \n",
    "    num_mols = len(tdc_datasets[benchmark][\"train\"][\"Drug\"])\n",
    "    train_dataset_file_name = f\"project_resources/nequip/positions/{benchmark}_train.extxyz\"\n",
    "    eval_dataset_file_name = f\"project_resources/nequip/positions/{benchmark}_test.extxyz\"\n",
    "    tuner = NequIPTuner(run_name, unique_symbols[benchmark], num_mols, train_dataset_file_name, eval_dataset_file_name)\n",
    "\n",
    "    study.optimize(tuner.objective, n_trials=n_trials, n_jobs=-1)\n",
    "    joblib.dump(study, f\"{root}/nequip.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda5c822-db51-48e7-b7ce-acae2cbdfdfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
