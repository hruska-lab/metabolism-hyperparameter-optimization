{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ad2316e2-c727-4da5-9148-8354b9aa5579",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiprocessing.pool.RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/data/dataset.py\", line 790, in _ase_dataset_reader\n",
      "    if global_index in include_frames\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/data/AtomicData.py\", line 449, in from_ase\n",
      "    **add_fields,\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/data/AtomicData.py\", line 318, in from_points\n",
      "    pbc=pbc,\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/data/AtomicData.py\", line 777, in neighbor_list_and_relative_vec\n",
      "    f\"Every single atom has no neighbors within the cutoff r_max={r_max} (after eliminating self edges, no edges remain in this system)\"\n",
      "ValueError: Every single atom has no neighbors within the cutoff r_max=0.4 (after eliminating self edges, no edges remain in this system)\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/utils/auto_init.py\", line 232, in instantiate\n",
      "    instance = builder(**positional_args, **final_optional_args)\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/data/dataset.py\", line 887, in __init__\n",
      "    type_mapper=type_mapper,\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/data/dataset.py\", line 166, in __init__\n",
      "    super().__init__(root=root, type_mapper=type_mapper)\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/data/dataset.py\", line 50, in __init__\n",
      "    super().__init__(root=root, transform=type_mapper)\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/utils/torch_geometric/dataset.py\", line 91, in __init__\n",
      "    self._process()\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/utils/torch_geometric/dataset.py\", line 176, in _process\n",
      "    self.process()\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/data/dataset.py\", line 218, in process\n",
      "    data = self.get_data()\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/data/dataset.py\", line 964, in get_data\n",
      "    datas = p.map(reader, list(range(n_proc)))\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/multiprocessing/pool.py\", line 268, in map\n",
      "    return self._map_async(func, iterable, mapstar, chunksize).get()\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/multiprocessing/pool.py\", line 657, in get\n",
      "    raise self._value\n",
      "ValueError: Every single atom has no neighbors within the cutoff r_max=0.4 (after eliminating self edges, no edges remain in this system)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lukas/anaconda3/envs/soc/bin/nequip-train\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/scripts/train.py\", line 72, in main\n",
      "    trainer = fresh_start(config)\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/scripts/train.py\", line 148, in fresh_start\n",
      "    dataset = dataset_from_config(config, prefix=\"dataset\")\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/data/_build.py\", line 82, in dataset_from_config\n",
      "    optional_args=config,\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/utils/auto_init.py\", line 236, in instantiate\n",
      "    ) from e\n",
      "RuntimeError: Failed to build object with prefix `dataset` using builder `ASEDataset`\n",
      "running command: nequip-evaluate --train-dir project_resources/optuna/nequip/random/RLM/0 \n",
      "                  --dataset-config rand_RLM_nequip_eval_0.yaml \n",
      "                  --output project_resources/optuna/nequip/random/RLM/0/output.xyz \n",
      "                  --log project_resources/optuna/nequip/random/RLM/0/evaluation_log.txt\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lukas/anaconda3/envs/soc/bin/nequip-evaluate\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/nequip/scripts/evaluate.py\", line 142, in main\n",
      "    str(args.train_dir / \"trainer.pth\"), map_location=\"cpu\"\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/torch/serialization.py\", line 699, in load\n",
      "    with _open_file_like(f, 'rb') as opened_file:\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/torch/serialization.py\", line 231, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "  File \"/home/lukas/anaconda3/envs/soc/lib/python3.7/site-packages/torch/serialization.py\", line 212, in __init__\n",
      "    super(_open_file, self).__init__(open(name, mode))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'project_resources/optuna/nequip/random/RLM/0/trainer.pth'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import yaml\n",
    "import optuna\n",
    "import joblib\n",
    "import copy\n",
    "import subprocess\n",
    "import shutil\n",
    "from optuna.storages import JournalFileStorage, JournalStorage\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from ase import Atoms\n",
    "from ase.io import read, write\n",
    "from ase.calculators.singlepoint import SinglePointCalculator\n",
    "from project_resources.import_utils import NotebookFinder\n",
    "sys.meta_path.append(NotebookFinder())\n",
    "from project_resources.cytochrome_P450 import get_unique_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e003390-4867-4fb4-b0d0-8ab8bdf9af84",
   "metadata": {},
   "outputs": [],
   "source": [
    "isozymes = [\"3A4\", \"RLM\", \"HLC\"]\n",
    "data_splits = [\"train\", \"test\"]\n",
    "splitters = [\"rand\", \"scaff\", \"time\"]\n",
    "rel_paths = {\n",
    "    \"3A4\": r\"project_resources/3A4.csv\",\n",
    "    \"3A4_train_scaff\": r\"project_resources/base_splits/scaffold_splitter/3A4_train.csv\",\n",
    "    \"3A4_test_scaff\": r\"project_resources/base_splits/scaffold_splitter/3A4_test.csv\",\n",
    "    \"3A4_train_rand\": r\"project_resources/base_splits/random/3A4_train.csv\",\n",
    "    \"3A4_test_rand\": r\"project_resources/base_splits/random/3A4_test.csv\",\n",
    "    \"3A4_train_time\": r\"project_resources/base_splits/time_split/3A4_train.csv\",\n",
    "    \"3A4_test_time\": r\"project_resources/base_splits/time_split/3A4_test.csv\",\n",
    "    \"3A4_train_xyz_rand\": r\"project_resources/nequip/positions/random/3A4_train_mol_positions.extxyz\",\n",
    "    \"3A4_test_xyz_rand\": r\"project_resources/nequip/positions/random/3A4_test_mol_positions.extxyz\",\n",
    "    \"3A4_train_xyz_scaff\": r\"project_resources/nequip/positions/scaffold_splitter/3A4_train_mol_positions.extxyz\",\n",
    "    \"3A4_test_xyz_scaff\": r\"project_resources/nequip/positions/scaffold_splitter/3A4_test_mol_positions.extxyz\",\n",
    "    \"3A4_train_xyz_time\": r\"project_resources/nequip/positions/time_split/3A4_train_mol_positions.extxyz\",\n",
    "    \"3A4_test_xyz_time\": r\"project_resources/nequip/positions/time_split/3A4_test_mol_positions.extxyz\",\n",
    "    \"3A4_rand_config\": r\"project_resources/nequip/3A4_rand_config.yaml\",\n",
    "    \"3A4_scaff_config\": r\"project_resources/nequip/3A4_scaffold_config.yaml\",\n",
    "    \"3A4_time_config\": r\"project_resources/nequip/3A4_time_config.yaml\",\n",
    "\n",
    "    \"RLM\": r\"project_resources/RLM.csv\",\n",
    "    \"RLM_train_scaff\": r\"project_resources/base_splits/scaffold_splitter/RLM_train.csv\",\n",
    "    \"RLM_test_scaff\": r\"project_resources/base_splits/scaffold_splitter/RLM_test.csv\",\n",
    "    \"RLM_train_rand\": r\"project_resources/base_splits/random/RLM_train.csv\",\n",
    "    \"RLM_test_rand\": r\"project_resources/base_splits/random/RLM_test.csv\",\n",
    "    \"RLM_train_time\": r\"project_resources/base_splits/time_split/RLM_train.csv\",\n",
    "    \"RLM_test_time\": r\"project_resources/base_splits/time_split/RLM_test.csv\",\n",
    "    \"RLM_train_xyz_rand\": r\"project_resources/nequip/positions/random/RLM_train_mol_positions.extxyz\",\n",
    "    \"RLM_test_xyz_rand\": r\"project_resources/nequip/positions/random/RLM_test_mol_positions.extxyz\",\n",
    "    \"RLM_train_xyz_scaff\": r\"project_resources/nequip/positions/scaffold_splitter/RLM_train_mol_positions.extxyz\",\n",
    "    \"RLM_test_xyz_scaff\": r\"project_resources/nequip/positions/scaffold_splitter/RLM_test_mol_positions.extxyz\",\n",
    "    \"RLM_train_xyz_time\": r\"project_resources/nequip/positions/time_split/RLM_train_mol_positions.extxyz\",\n",
    "    \"RLM_test_xyz_time\": r\"project_resources/nequip/positions/time_split/RLM_test_mol_positions.extxyz\",\n",
    "    \"RLM_rand_config\": r\"project_resources/nequip/RLM_rand_config.yaml\",\n",
    "    \"RLM_scaff_config\": r\"project_resources/nequip/RLM_scaffold_config.yaml\",\n",
    "    \"RLM_time_config\": r\"project_resources/nequip/RLM_time_config.yaml\",\n",
    "\n",
    "    \"HLC\": r\"project_resources/HLC.csv\",\n",
    "    \"HLC_train_scaff\": r\"project_resources/base_splits/scaffold_splitter/HLC_train.csv\",\n",
    "    \"HLC_test_scaff\": r\"project_resources/base_splits/scaffold_splitter/HLC_test.csv\",\n",
    "    \"HLC_train_rand\": r\"project_resources/base_splits/random/HLC_train.csv\",\n",
    "    \"HLC_test_rand\": r\"project_resources/base_splits/random/HLC_test.csv\",\n",
    "    \"HLC_train_time\": r\"project_resources/base_splits/time_split/HLC_train.csv\",\n",
    "    \"HLC_test_time\": r\"project_resources/base_splits/time_split/HLC_test.csv\",\n",
    "    \"HLC_train_xyz_rand\": r\"project_resources/nequip/positions/random/HLC_train_mol_positions.extxyz\",\n",
    "    \"HLC_test_xyz_rand\": r\"project_resources/nequip/positions/random/HLC_test_mol_positions.extxyz\",\n",
    "    \"HLC_train_xyz_scaff\": r\"project_resources/nequip/positions/scaffold_splitter/HLC_train_mol_positions.extxyz\",\n",
    "    \"HLC_test_xyz_scaff\": r\"project_resources/nequip/positions/scaffold_splitter/HLC_test_mol_positions.extxyz\",\n",
    "    \"HLC_train_xyz_time\": r\"project_resources/nequip/positions/time_split/HLC_train_mol_positions.extxyz\",\n",
    "    \"HLC_test_xyz_time\": r\"project_resources/nequip/positions/time_split/HLC_test_mol_positions.extxyz\",\n",
    "    \"HLC_rand_config\": r\"project_resources/nequip/HLC_rand_config.yaml\",\n",
    "    \"HLC_scaff_config\": r\"project_resources/nequip/HLC_scaffold_config.yaml\",\n",
    "    \"HLC_time_config\": r\"project_resources/nequip/HLC_time_config.yaml\"\n",
    "}\n",
    "# sampler - a method used to generate new sets of hyperparameters in each iteration of the optimization process\n",
    "samplers = {\n",
    "    'RandomSampler': optuna.samplers.RandomSampler,          # Sampler that selects hyperparameters randomly from the search space.\n",
    "    'GridSampler': optuna.samplers.GridSampler,              # Sampler that performs a grid search over the hyperparameter space.\n",
    "    'TPESampler': optuna.samplers.TPESampler,                # Sampler that uses a tree-structured Parzen estimator to model the objective function and sample new points from the search space.\n",
    "    'CmaEsSampler': optuna.samplers.CmaEsSampler,            # Sampler that uses the Covariance Matrix Adaptation Evolution Strategy algorithm to efficiently search the hyperparameter space.\n",
    "    'NSGAIISampler': optuna.samplers.NSGAIISampler,          # Multi-objective evolutionary algorithm that generates new samples using non-dominated sorting and crowding distance selection.\n",
    "    'QMCSampler': optuna.samplers.QMCSampler,                # Quasi-Monte Carlo sampler that uses low-discrepancy sequences to sample the search space in a more efficient and evenly distributed way than random sampling.\n",
    "    'BoTorchSampler': optuna.integration.BoTorchSampler,     # Sampler that leverages the BoTorch library for Bayesian optimization and can handle both continuous and categorical hyperparameters.\n",
    "    'BruteForceSampler': optuna.samplers.BruteForceSampler,  # Sampler that exhaustively evaluates all possible combinations of hyperparameters in the search space.\n",
    "}\n",
    "# pruner - a technique used to eliminate unpromising trials during the course of hyperparameter optimization.\n",
    "pruners = {\n",
    "    'BasePruner': optuna.pruners.BasePruner,                            # This is the base class for all pruning strategies in Optuna. It provides a skeleton for implementing custom pruning strategies.\n",
    "    'MedianPruner': optuna.pruners.MedianPruner,                        # A pruner that prunes unpromising trials that have median objective values, as determined in previous steps.\n",
    "    'SuccessiveHalvingPruner': optuna.pruners.SuccessiveHalvingPruner,  # This pruner repeatedly splits trials into halves, discarding the lower performing half at each iteration.\n",
    "    'HyperbandPruner': optuna.pruners.HyperbandPruner,                  # This pruner implements the Hyperband algorithm, which selects promising trials and runs them with different resource allocation schemes to determine the best one.\n",
    "    'PercentilePruner': optuna.pruners.PercentilePruner,                # A pruner that prunes unpromising trials based on their percentile rank relative to all completed trials.\n",
    "    'NopPruner': optuna.pruners.NopPruner,                              # A pruner that does nothing and does not prune any trials.\n",
    "    'ThresholdPruner': optuna.pruners.ThresholdPruner,                  # This pruner prunes trials that have not reached a certain level of performance (i.e., objective value).\n",
    "    'PatientPruner': optuna.pruners.PatientPruner,                      # This pruner prunes trials that do not show improvement over a certain number of steps (or epochs).\n",
    "}\n",
    "smiles = {}\n",
    "halflives = {}\n",
    "split_indexes = {}\n",
    "position_blocks = {}\n",
    "rdkit_symbols = {}\n",
    "unique_symbols = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b0d5a2a-9902-4cb3-bcdd-b79e45e309d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "rand\n",
      "3A4\n",
      "56 [63, 31, 59, 36, 58, 48, 17, 35, 43, 29, 8, 54, 41, 45, 47, 20, 57, 40, 26, 39, 14, 51, 4, 18, 9]\n",
      "14 [23, 1, 50, 5, 55, 19, 11, 34, 46, 31, 32, 10, 68, 6]\n",
      "RLM\n",
      "1421 [439, 199, 16, 266, 544, 1106, 799, 1609, 1672, 753, 731, 1174, 200, 712, 487, 1330, 1581, 549, 1156, 950, 1186, 586, 1038, 911, 311]\n",
      "356 [66, 942, 833, 1700, 800, 1532, 1037, 240, 1205, 1478, 937, 1121, 1557, 1720, 1068, 918, 110, 1579, 555, 882, 352, 399, 1512, 1395, 323]\n",
      "HLC\n",
      "151 [36, 99, 105, 76, 94, 69, 30, 128, 165, 57, 32, 13, 42, 101, 127, 98, 126, 27, 39, 155, 156, 182, 86, 91, 3]\n",
      "38 [185, 164, 19, 16, 68, 109, 46, 77, 17, 133, 61, 124, 43, 157, 56, 67, 116, 160, 31, 120, 70, 153, 138, 20, 52]\n",
      "\n",
      "scaff\n",
      "3A4\n",
      "56 [30, 33, 38, 51, 54, 58, 60, 64, 67, 31, 63, 37, 31, 36, 37, 63, 65, 2, 12, 14, 16, 17, 20, 21, 4]\n",
      "14 [29, 26, 25, 23, 18, 15, 11, 10, 9, 8, 6, 5, 3, 1]\n",
      "RLM\n",
      "1421 [41, 59, 70, 320, 356, 417, 590, 638, 746, 778, 791, 926, 928, 946, 1016, 1031, 1071, 1084, 1094, 1102, 1111, 1149, 1193, 1195, 1200]\n",
      "356 [675, 673, 672, 670, 667, 666, 663, 661, 659, 658, 657, 656, 655, 654, 649, 645, 644, 643, 641, 639, 637, 636, 635, 633, 631]\n",
      "HLC\n",
      "151 [9, 13, 21, 24, 69, 99, 103, 111, 114, 157, 14, 25, 46, 104, 115, 135, 181, 188, 54, 63, 142, 151, 33, 68, 122]\n",
      "38 [20, 110, 19, 109, 18, 108, 17, 107, 16, 106, 15, 105, 12, 102, 11, 101, 10, 100, 8, 98, 7, 97, 6, 96, 5]\n",
      "\n",
      "time\n",
      "3A4\n",
      "56 [25, 48, 26, 18, 35, 50, 43, 8, 45, 41, 6, 3, 1, 49, 62, 47, 10, 9, 19, 14, 59, 54, 53, 2, 4]\n",
      "14 [44, 27, 31, 37, 39, 38, 37, 36, 69, 34, 33, 31, 51, 70]\n",
      "RLM\n",
      "1421 [865, 676, 426, 1, 994, 999, 1006, 1008, 1009, 1011, 1020, 1026, 1028, 1029, 1030, 1039, 993, 1042, 1047, 1051, 302, 301, 300, 297, 296]\n",
      "356 [963, 1242, 215, 216, 944, 834, 217, 1236, 952, 945, 218, 951, 1226, 335, 219, 964, 214, 213, 336, 996, 923, 924, 985, 925, 984]\n",
      "HLC\n",
      "151 [107, 55, 22, 17, 112, 143, 61, 49, 100, 36, 158, 76, 26, 163, 164, 82, 75, 138, 160, 72, 170, 10, 70, 44, 127]\n",
      "38 [104, 103, 102, 101, 99, 56, 94, 93, 92, 91, 90, 98, 87, 88, 60, 62, 64, 67, 69, 59, 78, 83, 84, 85, 71]\n"
     ]
    }
   ],
   "source": [
    "for splitter in splitters:\n",
    "    print(f\"\\n{splitter}\")\n",
    "    split_indexes[splitter] = {}\n",
    "    for isozyme in isozymes:\n",
    "        print(isozyme)\n",
    "        split_indexes[splitter][isozyme] = {}\n",
    "        for split in data_splits:\n",
    "            df = pd.read_csv(rel_paths[f\"{isozyme}_{split}_{splitter}\"])\n",
    "            split_indexes[splitter][isozyme][split] = list(df[\"index\"])\n",
    "            print(len(split_indexes[splitter][isozyme][split]), split_indexes[splitter][isozyme][split][:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a81b6cc-154e-49d8-9597-a9b60c03128c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rand\n",
      "3A4_train_mol_positions.extxyz already exists\n",
      "3A4_test_mol_positions.extxyz already exists\n",
      "RLM_train_mol_positions.extxyz already exists\n",
      "RLM_test_mol_positions.extxyz already exists\n",
      "HLC_train_mol_positions.extxyz already exists\n",
      "HLC_test_mol_positions.extxyz already exists\n",
      "scaff\n",
      "3A4_train_mol_positions.extxyz already exists\n",
      "3A4_test_mol_positions.extxyz already exists\n",
      "RLM_train_mol_positions.extxyz already exists\n",
      "RLM_test_mol_positions.extxyz already exists\n",
      "HLC_train_mol_positions.extxyz already exists\n",
      "HLC_test_mol_positions.extxyz already exists\n",
      "time\n",
      "3A4_train_mol_positions.extxyz already exists\n",
      "3A4_test_mol_positions.extxyz already exists\n",
      "RLM_train_mol_positions.extxyz already exists\n",
      "RLM_test_mol_positions.extxyz already exists\n",
      "HLC_train_mol_positions.extxyz already exists\n",
      "HLC_test_mol_positions.extxyz already exists\n"
     ]
    }
   ],
   "source": [
    "for splitter in splitters:\n",
    "    print(splitter)\n",
    "    smiles[splitter] = {}\n",
    "    halflives[splitter] = {}\n",
    "    position_blocks[splitter] = {}\n",
    "    rdkit_symbols[splitter] = {}\n",
    "    smiles[splitter] = {}\n",
    "\n",
    "    for isozyme in isozymes:\n",
    "        smiles[splitter][isozyme] = {}\n",
    "        halflives[splitter][isozyme] = {}\n",
    "        position_blocks[splitter][isozyme] = {}\n",
    "        rdkit_symbols[splitter][isozyme] = {}\n",
    "        smiles[splitter][isozyme] = {}\n",
    "\n",
    "        for split in data_splits:\n",
    "            try:\n",
    "                read(rel_paths[f\"{isozyme}_{split}_xyz_{splitter}\"])\n",
    "                df = pd.read_csv(rel_paths[f\"{isozyme}_{split}_{splitter}\"])\n",
    "                smiles[splitter][isozyme][split] = df[\"smiles\"]\n",
    "                print(f\"{isozyme}_{split}_mol_positions.extxyz already exists\")\n",
    "\n",
    "            except FileNotFoundError:\n",
    "                # create .extxyz files for each combination of splitter, isozyme and split\n",
    "                df = pd.read_csv(rel_paths[f\"{isozyme}_{split}_{splitter}\"])\n",
    "                smiles[splitter][isozyme][split] = list(df[\"smiles\"])\n",
    "                halflives[splitter][isozyme][split] = np.log(np.array(df[\"half-life\"]))\n",
    "                isozyme_positions = []\n",
    "                isozyme_symbols = []\n",
    "\n",
    "                for smi in smiles[splitter][isozyme][split]:\n",
    "                    # create mol\n",
    "                    mol = Chem.MolFromSmiles(smi)\n",
    "                    mol = Chem.AddHs(mol, explicitOnly=True)\n",
    "                    AllChem.EmbedMolecule(mol)\n",
    "\n",
    "                    # get x, y and z positions of each atom from mol\n",
    "                    xyz_string = Chem.MolToXYZBlock(mol)\n",
    "\n",
    "                    # for looping over the positions\n",
    "                    lines = xyz_string.strip().split(\"\\n\")[2:]\n",
    "\n",
    "                    mol_positions = []  # corrdinates of each atom of mol in 3D space\n",
    "                    mol_symbols = []  # list of atoms in mol\n",
    "\n",
    "                    # get each atom and its coordinates\n",
    "                    for line in lines:\n",
    "                        parts = line.split()\n",
    "                        symbol = parts[0]\n",
    "                        x, y, z = map(float, parts[1:4])\n",
    "                        mol_symbols.append(symbol)\n",
    "                        mol_positions.append([float(coord) for coord in parts[1:]])\n",
    "\n",
    "                    isozyme_positions.append(mol_positions)\n",
    "                    isozyme_symbols.append(mol_symbols)\n",
    "\n",
    "                position_blocks[splitter][isozyme] = {}\n",
    "                position_blocks[splitter][isozyme][split] = isozyme_positions\n",
    "                # save data to dicts\n",
    "                rdkit_symbols[splitter][isozyme] = {}\n",
    "                rdkit_symbols[splitter][isozyme][split] = isozyme_symbols\n",
    "\n",
    "                # generate .extxyz files for train+validation and test sets\n",
    "                out_filename_extxyz = rel_paths[f\"{isozyme}_{split}_xyz_{splitter}\"]\n",
    "                # get data\n",
    "                positions = position_blocks[splitter][isozyme][split]  # coordinates of each atom in 3D space\n",
    "                symbols = rdkit_symbols[splitter][isozyme][split]  # atomic symbols (e.g. C, O, H...)\n",
    "                energies = halflives[splitter][isozyme][split]  # log half-life\n",
    "\n",
    "                # iterate over data and write continuously to extxyz file\n",
    "                for pos_idx in range(len(positions)):\n",
    "                    curr_atoms = Atoms(\n",
    "                    # set atomic positions\n",
    "                    positions=positions[pos_idx],\n",
    "                    # set chemical symbols / species\n",
    "                    symbols=symbols[pos_idx],\n",
    "                    # assuming data with periodic boundary conditions, set to false for e.g. for molecules in vacuum\n",
    "                    pbc=True\n",
    "                    )\n",
    "\n",
    "                    # set calculator to assign targets\n",
    "                    calculator = SinglePointCalculator(curr_atoms, energy=energies[pos_idx])\n",
    "                    curr_atoms.calc = calculator\n",
    "\n",
    "                    write(out_filename_extxyz, curr_atoms, format='extxyz', append=True)\n",
    "                print(f\"{out_filename_extxyz} was successfully created\")\n",
    "\n",
    "                # create .txt files which are copies of the .extxyz files\n",
    "                # with molecule indexes for better corss-referencing with source .csv files\n",
    "                # in order to include mol indexes, the file is no longer correctly formatted\n",
    "                # therefore can't be used for training/testing a module\n",
    "                out_filename_txt = out_filename_extxyz.replace(\"extxyz\", \"txt\")\n",
    "                for mol_idx, pos_idx in zip(split_indexes[splitter][isozyme][split], range(len(positions))):\n",
    "                    curr_atoms = Atoms(\n",
    "                    # set atomic positions\n",
    "                    positions=positions[pos_idx],\n",
    "                    # set chemical symbols / species\n",
    "                    symbols=symbols[pos_idx],\n",
    "                    # assuming data with periodic boundary conditions, set to false for e.g. for molecules in vacuum\n",
    "                    pbc=True\n",
    "                    )\n",
    "\n",
    "                    # set calculator to assign targets\n",
    "                    calculator = SinglePointCalculator(curr_atoms, energy=energies[pos_idx])\n",
    "                    curr_atoms.calc = calculator\n",
    "\n",
    "                    with open(out_filename_txt, \"a\") as file:\n",
    "                        file.write(f\"molecule index: {mol_idx}\\n\")\n",
    "                        file.close()\n",
    "                    write(out_filename_txt, curr_atoms, format='extxyz', append=True)\n",
    "                print(f\"{out_filename_txt} was successfully created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec4ba01b-6a53-49a5-81eb-dcb1969332ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chemical elements present in 3A4: ['H', 'P', 'O', 'Cl', 'C', 'F', 'S', 'N']\n",
      "chemical elements present in RLM: ['H', 'Br', 'B', 'O', 'Cl', 'C', 'F', 'S', 'I', 'N']\n",
      "chemical elements present in HLC: ['H', 'O', 'C', 'F', 'S', 'N']\n"
     ]
    }
   ],
   "source": [
    "for isozyme in isozymes:\n",
    "    df = pd.read_csv(rel_paths[isozyme])\n",
    "    isozyme_smiles = list(df[\"smiles\"])\n",
    "    unique_symbs = get_unique_elements(isozyme_smiles)\n",
    "    unique_symbols[isozyme] = list(unique_symbs)\n",
    "    print(f\"chemical elements present in {isozyme}: {unique_symbols[isozyme]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ea19ddf4-57db-40e7-a3b0-37e868a4a912",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_nequip_eval_yaml(yaml_path, root, trial_number, eval_dataset_file_name, unique_symbols):\n",
    "    with open(yaml_path, \"w\") as out_f:\n",
    "        data = {}\n",
    "        \n",
    "        # root is the same as nequip-train, except at the end is also included the number of the run\n",
    "        data[\"root\"] = root\n",
    "        data[\"chemical_symbols\"] = unique_symbols\n",
    "        data[\"dataset_file_name\"] = eval_dataset_file_name\n",
    "        data[\"dataset\"] = \"ase\"\n",
    "        \n",
    "        yaml.dump(data, out_f)\n",
    "        print(f\"\"\"The evaluation yaml file for the {trial_number}. iteration of {splitter} {isozyme}\n",
    "        with {root} as root and was successfully created \"\"\")\n",
    "        out_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "761c3f13-7be7-444a-841b-9df42bd185d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_nequip_train_yaml(yaml_path, batch_size, learning_rate, num_layers, root, trial_number,\n",
    "                       train_dataset_file_name, unique_symbols, num_mols):\n",
    "    with open(yaml_path, \"w\") as out_f:\n",
    "        data = {}\n",
    "\n",
    "        data[\"batch_size\"] = batch_size\n",
    "        data[\"learning_rate\"] = learning_rate\n",
    "        data[\"num_layers\"] = num_layers\n",
    "\n",
    "        data[\"root\"] = root  # project_resources/nequip/{splitter}/{isozyme}\n",
    "        data[\"run_name\"] = str(trial_number)  # number of the trial for a given combination of splitter and isozyme\n",
    "        data[\"dataset_file_name\"] = train_dataset_file_name\n",
    "        data[\"seed\"] = 123\n",
    "        data[\"dataset_seed\"] = 456\n",
    "        data[\"append\"] = True\n",
    "        data[\"model_builders\"] = [\"SimpleIrrepsConfig\", \"EnergyModel\", \"PerSpeciesRescale\", \"RescaleEnergyEtc\"]\n",
    "\n",
    "        data[\"max_epochs\"] = 3\n",
    "        data[\"r_max\"] = 0.4  # cutoff radius in Angstroms; changing the value has no effect on validation_e_mae\n",
    "        data[\"l_max\"] = 2\n",
    "        data[\"parity\"] = False  # slighly worse performance than True, but training takes way less time\n",
    "        data[\"num_features\"] = 32\n",
    "        data[\"num_basis\"] = 8\n",
    "\n",
    "        data[\"dataset\"] = \"ase\"\n",
    "        key_mapping = {\"z\": \"atomic_numbers\", \"E\": \"total_energy\", \"R\": \"pos\"}\n",
    "        data[\"key_mapping\"] = key_mapping\n",
    "        data[\"npz_fixed_field_keys\"] = \"atomic_numbers\"\n",
    "        data[\"chemical_symbols\"] = unique_symbols\n",
    "        data[\"wandb\"] = False  # impossible to use wandb inside a notebook (cannot choose an option)\n",
    "\n",
    "        num_train_val = num_mols  # number of molecules in the dataset\n",
    "        num_train = int(np.floor(0.8 * num_train_val))\n",
    "        num_validation = int(num_train_val - num_train)\n",
    "        data[\"n_train\"] = num_train\n",
    "        data[\"n_val\"] = num_validation\n",
    "        data[\"train_val_split\"] = \"sequential\"\n",
    "        data[\"validation_batch_size\"] = num_validation\n",
    "        data[\"loss_coeffs\"] = \"total_energy\"\n",
    "        data[\"optimizer_name\"] = \"Adam\"\n",
    "\n",
    "        splitter = root.split(\"/\")[-2]\n",
    "        isozyme = root.split(\"/\")[-1]\n",
    "        \n",
    "        yaml.dump(data, out_f)\n",
    "        print(f\"\"\"The train yaml file for the {trial_number}. iteration of {splitter} {isozyme}\n",
    "        with {root} as root and was successfully created \"\"\")\n",
    "        out_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "42967962-bdf4-457d-827c-53e23f58c510",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_nequip_xyz_out(xyz):\n",
    "    # get the predicted half-lives from xyz file created after using nequip-evaluate --output\n",
    "    with open(xyz) as f:\n",
    "        energies = [float(re.search(r'energy=(-?\\d+\\.\\d+)', line).group(1)) for line in f if \"energy\" in line]\n",
    "    return energies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dbc66bfd-14db-49d5-b308-97040b85633c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_nequip_log(log):\n",
    "    # get the rmse value from the log file created after using nequip-evaluate --log\n",
    "    with open(log) as l:\n",
    "        file_content = l.read()\n",
    "    # use regular expressions to find the rmse value\n",
    "    rmse = re.search(r'\\s*e_rmse\\s*=\\s*(\\d+\\.\\d+)', file_content)\n",
    "    return float(rmse.group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "f2e6c1ad-90fb-4b18-8a05-6e326f120c91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NequIPTuner():\n",
    "    def __init__(self, run_name, root, unique_symbols, num_mols, train_dataset_file_name, eval_dataset_file_name):\n",
    "        self.run_name = run_name  # {splitter}_{isozyme}\n",
    "        self.root = root\n",
    "        self.unique_symbols = unique_symbols\n",
    "        self.num_mols = num_mols  # number of mols present in the train .extxyz file\n",
    "        self.train_dataset_file_name = train_dataset_file_name\n",
    "        self.eval_dataset_file_name = eval_dataset_file_name\n",
    "\n",
    "    def sample_params(self, trial: optuna.Trial):\n",
    "        batch_size = trial.suggest_int(\"batch_size\", 2, 50)\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 0.001, 1)\n",
    "        num_layers = trial.suggest_int(\"num_layers\", 3, 5)\n",
    "        return {\n",
    "            \"batch_size\": batch_size,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"num_layers\": num_layers\n",
    "        }\n",
    "\n",
    "    def train_test_return(self, train_yaml_name, eval_yaml_name, train_dir, return_predictions=False):\n",
    "        xyz_out = f\"{train_dir}/output.xyz\"\n",
    "        log_file = f\"{train_dir}/evaluation_log.txt\"\n",
    "        \n",
    "        print(f\"running command: nequip-train {train_yaml_name}\")\n",
    "        !nequip-train {train_yaml_name}\n",
    "        # {train_dir} ... project_resources/optuna/nequip/random/3A4/1\n",
    "        # {eval_yaml_name} ... eval_dataset.yaml\n",
    "        # {xyz_out} ... project_resources/optuna/nequip/random/3A4/1/output.xyz\n",
    "        # ^ Parse this to get the predicted half-lives ^\n",
    "        # {log} ... project_resources/optuna/nequip/random/3A4/1/eval_log.txt\n",
    "        \n",
    "        print(f\"\"\"running command: nequip-evaluate --train-dir {train_dir} \n",
    "              --dataset-config {eval_yaml_name} \n",
    "              --output {xyz_out} \n",
    "              --log {log_file}\"\"\")\n",
    "        !nequip-evaluate --train-dir {train_dir} --dataset-config {eval_yaml_name} --output {xyz_out} --log {log_file}\n",
    "\n",
    "        rmse = parse_nequip_log(log_file)\n",
    "        \n",
    "        # delete the dirs and files that were created, as they are no longer needed\n",
    "        shutil.rmtree(train_dir)\n",
    "        os.remove(train_yaml_name)\n",
    "        os.remove(eval_yaml_name)\n",
    "        \n",
    "        if return_predictions:\n",
    "            predictions = parse_nequip_xyz_out(xyz_out)\n",
    "            return rmse, predictions\n",
    "        else:\n",
    "            return rmse\n",
    "\n",
    "    def objective(self, trial=None):\n",
    "        parameters = self.sample_params(trial)\n",
    "    \n",
    "        run_splitter = self.run_name.split(\"_\")[0]\n",
    "        run_isozyme = self.run_name.split(\"_\")[1]\n",
    "        if run_splitter == \"rand\":\n",
    "            run_splitter_name = \"random\"\n",
    "        elif run_splitter == \"scaff\":\n",
    "            run_splitter_name = \"scaffold_splitter\"\n",
    "        else:\n",
    "            run_splitter_name = \"time_split\"\n",
    "\n",
    "        working_dir = f\"project_resources/optuna/nequip/{run_splitter_name}/{run_isozyme}\"\n",
    "        train_yaml_name = f\"{working_dir}/{self.run_name}_nequip_train_{trial.number}.yaml\"\n",
    "        eval_yaml_name = f\"{working_dir}/{self.run_name}_nequip_eval_{trial.number}.yaml\"\n",
    "        train_dir = self.root + f\"/{trial.number}\"\n",
    "        \n",
    "        # create train yaml file for this trial\n",
    "        create_nequip_train_yaml(train_yaml_name, parameters[\"batch_size\"], parameters[\"learning_rate\"],\n",
    "                           parameters[\"num_layers\"], self.root, trial.number, self.train_dataset_file_name,\n",
    "                           self.unique_symbols, self.num_mols)\n",
    "\n",
    "        # create evaluation yaml file for this trial\n",
    "        create_nequip_eval_yaml(eval_yaml_name, self.root, trial.number,\n",
    "                                self.eval_dataset_file_name, self.unique_symbols)\n",
    "\n",
    "        return self.train_test_return(train_yaml_name, eval_yaml_name, train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "0a1558bd-e1fd-4b75-bca9-5b8b224ce04e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-22 22:10:00,107] Using an existing study with name 'nequip_rand_3A4' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train yaml file for the 1. iteration of random 3A4\n",
      "        with project_resources/optuna/nequip/random/3A4 as root and was successfully created \n",
      "The evaluation yaml file for the 1. iteration of rand 3A4\n",
      "        with project_resources/optuna/nequip/random/3A4 as root and was successfully created \n",
      "running command: nequip-train project_resources/optuna/nequip/random/3A4/rand_3A4_nequip_train_1.yaml\n",
      "Torch device: cpu\n",
      "Successfully loaded the data set of type ASEDataset(56)...\n",
      "Replace string dataset_per_atom_total_energy_std to 0.04218994081020355\n",
      "Replace string dataset_per_atom_total_energy_mean to -0.08114522695541382\n",
      "Atomic outputs are scaled by: [H, C, N, O, F, P, S, Cl: 0.042190], shifted by [H, C, N, O, F, P, S, Cl: -0.081145].\n",
      "Replace string dataset_total_energy_std to tensor([1.4029])\n",
      "Initially outputs are globally scaled by: tensor([1.4029]), total_energy are globally shifted by None.\n",
      "Successfully built the network...\n",
      "Number of weights: 240664\n",
      "Number of trainable weights: 240664\n",
      "! Starting training ...\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_e        e_mae       e_rmse\n",
      "      0     1         1.15         1.15         1.32         1.51\n",
      "\n",
      "\n",
      "  Initialization     #    Epoch      wal       LR       loss_e         loss        e_mae       e_rmse\n",
      "! Initial Validation          0    0.780    0.564         1.15         1.15         1.32         1.51\n",
      "Wall time: 0.7797690739971586\n",
      "! Best model        0    1.153\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_e        e_mae       e_rmse\n",
      "      1     1         1.09         1.09         1.28         1.47\n",
      "      1     2            4            4         2.53         2.81\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_e        e_mae       e_rmse\n",
      "      1     1         1.89         1.89         1.41         1.93\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_e         loss        e_mae       e_rmse\n",
      "! Train               1    6.905    0.564         2.55         2.55         1.82         2.15\n",
      "! Validation          1    6.905    0.564         1.89         1.89         1.41         1.93\n",
      "Wall time: 6.9050437479963875\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_e        e_mae       e_rmse\n",
      "      2     1         1.44         1.44         1.39         1.68\n",
      "      2     2         2.77         2.77         2.05         2.33\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_e        e_mae       e_rmse\n",
      "      2     1         2.74         2.74         1.72         2.32\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_e         loss        e_mae       e_rmse\n",
      "! Train               2   10.732    0.564          2.1          2.1         1.67         1.99\n",
      "! Validation          2   10.732    0.564         2.74         2.74         1.72         2.32\n",
      "Wall time: 10.732151192001766\n",
      "\n",
      "training\n",
      "# Epoch batch         loss       loss_e        e_mae       e_rmse\n",
      "      3     1         2.33         2.33         1.77         2.14\n",
      "      3     2         2.46         2.46         1.89          2.2\n",
      "\n",
      "validation\n",
      "# Epoch batch         loss       loss_e        e_mae       e_rmse\n",
      "      3     1         1.15         1.15         1.26          1.5\n",
      "\n",
      "\n",
      "  Train      #    Epoch      wal       LR       loss_e         loss        e_mae       e_rmse\n",
      "! Train               3   14.331    0.564         2.39         2.39         1.82         2.17\n",
      "! Validation          3   14.331    0.564         1.15         1.15         1.26          1.5\n",
      "Wall time: 14.33137340599933\n",
      "! Best model        3    1.150\n",
      "! Stop training: max epochs\n",
      "Wall time: 14.359901279000042\n",
      "Cumulative wall time: 14.359901279000042\n",
      "running command: nequip-evaluate --train-dir project_resources/optuna/nequip/random/3A4/1 \n",
      "              --dataset-config project_resources/optuna/nequip/random/3A4/rand_3A4_nequip_eval_1.yaml \n",
      "              --output project_resources/optuna/nequip/random/3A4/1/output.xyz \n",
      "              --log project_resources/optuna/nequip/random/3A4/1/evaluation_log.txt\n",
      "Using device: cpu\n",
      "Loading model... \n",
      "loaded model from training session\n",
      "Loading dataset...\n",
      "Loaded dataset specified in rand_3A4_nequip_eval_1.yaml.\n",
      "Using all frames from the specified test dataset, yielding a test set size of 14 frames.\n",
      "Starting...\n",
      "  0%|                                                    | 0/14 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "100%|███████████████████████████████████████████| 14/14 [00:00<00:00, 16.02it/s]\n",
      "e_mae = 1.2147 | e_rmse = 1.6266\n",
      "\n",
      "--- Final result: ---\n",
      "               e_mae =  1.214671           \n",
      "              e_rmse =  1.626565           \n",
      "               e_mae =  1.214671           \n",
      "              e_rmse =  1.626565           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-11-22 22:10:25,662] Trial 1 finished with value: 1.626565 and parameters: {'batch_size': 25, 'learning_rate': 0.5642759064409827, 'num_layers': 4}. Best is trial 0 with value: 1.588775.\n",
      "[I 2023-11-22 22:10:25,668] Using an existing study with name 'nequip_rand_RLM' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train yaml file for the 1. iteration of random RLM\n",
      "        with project_resources/optuna/nequip/random/RLM as root and was successfully created \n",
      "The evaluation yaml file for the 1. iteration of rand RLM\n",
      "        with project_resources/optuna/nequip/random/RLM as root and was successfully created \n",
      "running command: nequip-train project_resources/optuna/nequip/random/RLM/rand_RLM_nequip_train_1.yaml\n",
      "Torch device: cpu\n",
      "Processing dataset...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23533/428338992.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtuner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNequIPTuner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique_symbols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misozyme\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_mols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_dataset_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{root}/nequip.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/soc/lib/python3.7/site-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgc_after_trial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m             \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m         )\n\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/soc/lib/python3.7/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    115\u001b[0m                             \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                             \u001b[0mtime_start\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                             \u001b[0mprogress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m                         )\n\u001b[1;32m    119\u001b[0m                     )\n",
      "\u001b[0;32m~/anaconda3/envs/soc/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/soc/lib/python3.7/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_threads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                 \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/soc/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/soc/lib/python3.7/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1058\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sampler = samplers[\"TPESampler\"]\n",
    "pruner = pruners[\"BasePruner\"]\n",
    "n_trials = 1\n",
    "for splitter in splitters:\n",
    "    if splitter == \"rand\":\n",
    "        splitter_name = \"random\"\n",
    "    elif splitter == \"scaff\":\n",
    "        splitter_name = \"scaffold_splitter\"\n",
    "    else:\n",
    "        splitter_name = \"time_split\"\n",
    "\n",
    "    for isozyme in isozymes:\n",
    "        root = f\"project_resources/optuna/nequip/{splitter_name}/{isozyme}\"\n",
    "        lock_obj = optuna.storages.JournalFileOpenLock(root + \"/nequip_journal.log\")\n",
    "        storage = JournalStorage(\n",
    "            JournalFileStorage(root + \"/nequip_journal.log\", lock_obj=lock_obj)\n",
    "        )\n",
    "        \n",
    "        study = optuna.create_study(study_name=f\"nequip_{splitter}_{isozyme}\", directions=[\"minimize\"],\n",
    "                                    pruner=pruner, storage=storage, load_if_exists=True)\n",
    "\n",
    "        run_name = f\"{splitter}_{isozyme}\"\n",
    "        num_mols = len(smiles[splitter][isozyme][\"train\"])\n",
    "        train_dataset_file_name = f\"project_resources/nequip/positions/{splitter_name}/{isozyme}_train_mol_positions.extxyz\"\n",
    "        eval_dataset_file_name = f\"project_resources/nequip/positions/{splitter_name}/{isozyme}_test_mol_positions.extxyz\"\n",
    "        tuner = NequIPTuner(run_name, root, unique_symbols[isozyme], num_mols, train_dataset_file_name, eval_dataset_file_name)\n",
    "        \n",
    "        study.optimize(tuner.objective, n_trials=n_trials, n_jobs=-1)\n",
    "        joblib.dump(study, f\"{root}/nequip.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda5c822-db51-48e7-b7ce-acae2cbdfdfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
