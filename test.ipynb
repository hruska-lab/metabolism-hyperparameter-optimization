{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f671933b-f70e-4101-aa34-66066e45160b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62fdaba6-1e6d-426d-8d91-63e57a6ecce3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukas\\anaconda3\\envs\\soc\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from C:\\Users\\Lukas\\Documents\\datacytochromy\\project_resources\\cytochrome_P450.ipynb\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import joblib\n",
    "import requests\n",
    "import warnings\n",
    "import py3Dmol\n",
    "from ase import Atoms\n",
    "from ase.io import read, write\n",
    "from ase.calculators.singlepoint import SinglePointCalculator\n",
    "from chembl_webresource_client.new_client import new_client as client\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem.MolStandardize.rdMolStandardize import FragmentParent\n",
    "from rdkit import DataStructs\n",
    "from jazzy.api import molecular_vector_from_smiles as mol_vect\n",
    "import numpy as np\n",
    "import pubchempy as pcp\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import optuna\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from optuna.storages import JournalStorage, JournalFileStorage\n",
    "from tdc.single_pred import ADME\n",
    "from project_resources.import_utils import NotebookFinder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sys.meta_path.append(NotebookFinder())\n",
    "from project_resources.cytochrome_P450 import fp_from_smiles, parse_jazzy_df, optuna_trial_logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca8413a4-e26d-4131-939b-bb83c3e8c166",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_types = [\"morgan\", \"jazzy\"]\n",
    "tdc_benchmarks = [\"obach\", \"microsome\", \"hepatocyte\"]\n",
    "model_identifiers = [\"linear\", \"KRR\", \"GB\", \"RF\", \"ANN\"]\n",
    "tdc_datasets = {}\n",
    "smiles = {}\n",
    "halflives = {}\n",
    "mol_features = {}\n",
    "\n",
    "# sampler - a method used to generate new sets of hyperparameters in each iteration of the optimization process\n",
    "samplers = {\n",
    "    'RandomSampler': optuna.samplers.RandomSampler,          # Sampler that selects hyperparameters randomly from the search space.\n",
    "    'GridSampler': optuna.samplers.GridSampler,              # Sampler that performs a grid search over the hyperparameter space.\n",
    "    'TPESampler': optuna.samplers.TPESampler,                # Sampler that uses a tree-structured Parzen estimator to model the objective function and sample new points from the search space.\n",
    "    'CmaEsSampler': optuna.samplers.CmaEsSampler,            # Sampler that uses the Covariance Matrix Adaptation Evolution Strategy algorithm to efficiently search the hyperparameter space.\n",
    "    'NSGAIISampler': optuna.samplers.NSGAIISampler,          # Multi-objective evolutionary algorithm that generates new samples using non-dominated sorting and crowding distance selection.\n",
    "    'QMCSampler': optuna.samplers.QMCSampler,                # Quasi-Monte Carlo sampler that uses low-discrepancy sequences to sample the search space in a more efficient and evenly distributed way than random sampling.\n",
    "    'BoTorchSampler': optuna.integration.BoTorchSampler,     # Sampler that leverages the BoTorch library for Bayesian optimization and can handle both continuous and categorical hyperparameters.\n",
    "    'BruteForceSampler': optuna.samplers.BruteForceSampler,  # Sampler that exhaustively evaluates all possible combinations of hyperparameters in the search space.\n",
    "}\n",
    "# pruner - a technique used to eliminate unpromising trials during the course of hyperparameter optimization.\n",
    "pruners = {\n",
    "    'BasePruner': optuna.pruners.BasePruner,                            # This is the base class for all pruning strategies in Optuna. It provides a skeleton for implementing custom pruning strategies.\n",
    "    'MedianPruner': optuna.pruners.MedianPruner,                        # A pruner that prunes unpromising trials that have median objective values, as determined in previous steps.\n",
    "    'SuccessiveHalvingPruner': optuna.pruners.SuccessiveHalvingPruner,  # This pruner repeatedly splits trials into halves, discarding the lower performing half at each iteration.\n",
    "    'HyperbandPruner': optuna.pruners.HyperbandPruner,                  # This pruner implements the Hyperband algorithm, which selects promising trials and runs them with different resource allocation schemes to determine the best one.\n",
    "    'PercentilePruner': optuna.pruners.PercentilePruner,                # A pruner that prunes unpromising trials based on their percentile rank relative to all completed trials.\n",
    "    'NopPruner': optuna.pruners.NopPruner,                              # A pruner that does nothing and does not prune any trials.\n",
    "    'ThresholdPruner': optuna.pruners.ThresholdPruner,                  # This pruner prunes trials that have not reached a certain level of performance (i.e., objective value).\n",
    "    'PatientPruner': optuna.pruners.PatientPruner,                      # This pruner prunes trials that do not show improvement over a certain number of steps (or epochs).\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03579a1f-49c1-4951-a87e-1bbbc5588780",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n",
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n",
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "obach = ADME(name='Half_Life_Obach')\n",
    "obach_split = obach.get_split()\n",
    "tdc_datasets[\"obach\"] = obach_split\n",
    "microsome = ADME(name='Clearance_Microsome_AZ')\n",
    "microsome_split = microsome.get_split()\n",
    "tdc_datasets[\"microsome\"] = microsome_split\n",
    "hepatocyte = ADME(name='Clearance_Hepatocyte_AZ')\n",
    "hepatocyte_split = hepatocyte.get_split()\n",
    "tdc_datasets[\"hepatocyte\"] = hepatocyte_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "828c0176-8f0c-4767-9c37-cbabf3a3b639",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obach\n",
      "dict_keys(['obach'])\n",
      "(467,) (467,) (133,) (133,)\n",
      "microsome\n",
      "dict_keys(['obach', 'microsome'])\n",
      "(772,) (772,) (220,) (220,)\n",
      "hepatocyte\n",
      "dict_keys(['obach', 'microsome', 'hepatocyte'])\n",
      "(849,) (849,) (243,) (243,)\n"
     ]
    }
   ],
   "source": [
    "halflives[\"morgan\"] = {}\n",
    "for benchmark in tdc_benchmarks:\n",
    "    print(benchmark)\n",
    "    smiles[benchmark] = {}\n",
    "    halflives[\"morgan\"][benchmark] = {}\n",
    "    \n",
    "    benchmark_train_smiles = tdc_datasets[benchmark][\"train\"][\"Drug\"]\n",
    "    benchmark_test_smiles = tdc_datasets[benchmark][\"test\"][\"Drug\"]\n",
    "    smiles[benchmark][\"train\"] = np.array(benchmark_train_smiles)\n",
    "    smiles[benchmark][\"test\"] = np.array(benchmark_test_smiles)\n",
    "    \n",
    "    benchmark_train_halflives = tdc_datasets[benchmark][\"train\"][\"Y\"]\n",
    "    benchmark_test_halflives = tdc_datasets[benchmark][\"test\"][\"Y\"]\n",
    "    \n",
    "    reshaped_train_halflife = np.array(benchmark_train_halflives).reshape(-1, 1)\n",
    "    scaler = MinMaxScaler().fit(reshaped_train_halflife)\n",
    "    train_halflife_scaled = scaler.transform(reshaped_train_halflife)\n",
    "    train_halflives_scaled = np.array([val[0] for val in train_halflife_scaled])\n",
    "\n",
    "    reshaped_test_halflife = np.array(benchmark_test_halflives).reshape(-1, 1)\n",
    "    scaler = MinMaxScaler().fit(reshaped_test_halflife)\n",
    "    test_halflife_scaled = scaler.transform(reshaped_test_halflife)\n",
    "    test_halflives_scaled = np.array([val[0] for val in test_halflife_scaled])\n",
    "    \n",
    "    halflives[\"morgan\"][benchmark][\"train\"] = np.array(train_halflives_scaled)\n",
    "    halflives[\"morgan\"][benchmark][\"test\"] = np.array(test_halflives_scaled)\n",
    "    \n",
    "    print(halflives[\"morgan\"].keys())\n",
    "    \n",
    "    print(benchmark_train_smiles.shape, benchmark_train_halflives.shape, benchmark_test_smiles.shape, benchmark_test_halflives.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34575477-5ab1-473e-82f8-f5e1d9622939",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obach\n",
      "(467, 124) (133, 124)\n",
      "microsome\n",
      "(772, 124) (220, 124)\n",
      "hepatocyte\n",
      "(849, 124) (243, 124)\n"
     ]
    }
   ],
   "source": [
    "mol_features[\"morgan\"] = {}\n",
    "for benchmark in tdc_benchmarks:\n",
    "    print(benchmark)\n",
    "    mol_features[\"morgan\"][benchmark] = {}\n",
    "    train_morgan_fps = np.array(fp_from_smiles(smiles[benchmark][\"train\"]))\n",
    "    test_morgan_fps = np.array(fp_from_smiles(smiles[benchmark][\"test\"]))\n",
    "    mol_features[\"morgan\"][benchmark][\"train\"] = train_morgan_fps\n",
    "    mol_features[\"morgan\"][benchmark][\"test\"] = test_morgan_fps\n",
    "    print(train_morgan_fps.shape, test_morgan_fps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05970aa3-cc6d-413c-92df-cd2793ce3d8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obach\n",
      "     525, [0.0033169983665033, 11.3305, 0.0, 3.3351, -4.054, -69.5826, -60.2942]\n",
      "     130, [0.0078064793190067, 10.9705, 1.8136, 5.8249, -16.4281, -118.9807, -121.6516]\n",
      "(525, 6) (525,) (130, 6) (130,)\n",
      "microsome\n",
      "     882, [0.0652380952380952, 10.5072, 1.4478, 4.6964, -13.5025, -92.8889, -106.3914]\n",
      "     220, [0.0884353741496598, 3.5084, 3.5128, 4.5042, -12.6756, -80.3865, -84.2257]\n",
      "(882, 6) (882,) (220, 6) (220,)\n",
      "hepatocyte\n",
      "     970, [0.0, 9.8552, 1.4451, 4.4407, -15.1209, -91.0733, -102.926]\n",
      "     243, [0.0825850340136054, 10.2098, 1.292, 5.2199, -18.4498, -95.9609, -98.199]\n",
      "(970, 6) (970,) (243, 6) (243,)\n"
     ]
    }
   ],
   "source": [
    "mol_features[\"jazzy\"] = {}\n",
    "halflives[\"jazzy\"] = {}\n",
    "for benchmark in tdc_benchmarks:\n",
    "    print(benchmark)\n",
    "    train_jazzy_df = pd.read_csv(f\"project_resources/jazzy_splits/{benchmark}_train.csv\")\n",
    "    test_jazzy_df = pd.read_csv(f\"project_resources/jazzy_splits/{benchmark}_test.csv\")\n",
    "    train_fts, train_jazzy_thalfs, contains_nan = parse_jazzy_df(train_jazzy_df, no_idx_smi=True)\n",
    "    test_fts, test_jazzy_thalfs, contains_nan = parse_jazzy_df(test_jazzy_df, no_idx_smi=True)\n",
    "    \n",
    "    mol_features[\"jazzy\"][benchmark] = {}\n",
    "    mol_features[\"jazzy\"][benchmark][\"train\"] = train_fts\n",
    "    mol_features[\"jazzy\"][benchmark][\"test\"] = test_fts\n",
    "    halflives[\"jazzy\"][benchmark] = {}\n",
    "    halflives[\"jazzy\"][benchmark][\"train\"] = train_jazzy_thalfs\n",
    "    halflives[\"jazzy\"][benchmark][\"test\"] = test_jazzy_thalfs\n",
    "    \n",
    "    print(np.array(train_fts).shape, np.array(train_jazzy_thalfs).shape, np.array(test_fts).shape, np.array(test_jazzy_thalfs).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f73bab42-d44c-4db3-ade2-148ca57e8e44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HyperparamTuner():\n",
    "    def __init__(self, log_csv_path, model_identifier, X_train, y_train, X_test, y_test):\n",
    "        self.log_csv_path = log_csv_path\n",
    "        self.model_identifier = model_identifier\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "\n",
    "    def sample_params(self, trial: optuna.Trial, model_identifier):\n",
    "        if model_identifier == 'linear':\n",
    "            alpha = trial.suggest_float('alpha', 1e-5, 1e-1)\n",
    "            l1_ratio = trial.suggest_float('l1_ratio', 0, 1)\n",
    "            return {\n",
    "                \"alpha\": alpha,\n",
    "                \"l1_ratio\": l1_ratio\n",
    "            }, ElasticNet(alpha=alpha, l1_ratio=l1_ratio)\n",
    "\n",
    "        if model_identifier == 'KRR':\n",
    "            alpha = trial.suggest_float(\"alpha\", 1e-4, 1)\n",
    "            gamma = trial.suggest_float(\"gamma\", 0, 1e-14)\n",
    "            kernel = trial.suggest_categorical(\"kernel\", [\"linear\", \"laplacian\", \"rbf\"])\n",
    "            return {\n",
    "                \"alpha\": alpha,\n",
    "                \"gamma\": gamma,\n",
    "                \"kernel\": kernel\n",
    "            }, KernelRidge(alpha=alpha, gamma=gamma, kernel=kernel)\n",
    "\n",
    "        if model_identifier == 'GB':\n",
    "            n_estimators = trial.suggest_categorical(\"n_estimators\", [10, 20, 50, 200, 500])\n",
    "            learning_rate = trial.suggest_float(\"learning_rate\", 0.005, 1)\n",
    "            max_depth = trial.suggest_categorical(\"max_depth\", [1, 2, 3, 4, 5])\n",
    "            return {\n",
    "                \"n_estimators\": n_estimators,\n",
    "                \"learning_rate\": learning_rate,\n",
    "                \"max_depth\": max_depth\n",
    "            }, GradientBoostingRegressor(n_estimators=n_estimators, learning_rate=learning_rate, max_depth=max_depth)\n",
    "\n",
    "        if model_identifier == 'RF':\n",
    "            n_estimators = trial.suggest_categorical(\"n_estimators\", [10, 20, 50, 200, 500])\n",
    "            max_features = trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\"])\n",
    "            max_depth = trial.suggest_categorical(\"max_depth\", [None, 2, 3, 4, 5, 10])\n",
    "            return {\n",
    "                \"n_estimators\": n_estimators,\n",
    "                \"max_features\": max_features,\n",
    "                \"max_depth\": max_depth\n",
    "            }, RandomForestRegressor(n_estimators=n_estimators, max_features=max_features, max_depth=max_depth)\n",
    "\n",
    "        if model_identifier == 'ANN':\n",
    "            learning_rate_init = trial.suggest_float(\"learning_rate_init\", 0.001, 0.1)\n",
    "            hidden_layer_sizes = trial.suggest_categorical(\"hidden_layer_sizes\",\n",
    "                                                           [[5], [10], [20], [50], [5]*2, [10]*2, [20]*2, [50]*2, [5]*3, [10]*3, [50]*3])\n",
    "            return {\n",
    "            \"learning_rate_init\": learning_rate_init,\n",
    "            \"hidden_layer_sizes\": hidden_layer_sizes\n",
    "            }, MLPRegressor(learning_rate_init=learning_rate_init, hidden_layer_sizes=hidden_layer_sizes)\n",
    "\n",
    "    def cross_validation_splits(self, X_train, X_test, y_train, y_test, cv_splits=5):\n",
    "        \"\"\"\n",
    "        Splits the data into cv_splits different combinations for cross-validation.\n",
    "\n",
    "        Parameters:\n",
    "        - X_train: Training data features\n",
    "        - X_test: Testing data features\n",
    "        - y_train: Training data labels\n",
    "        - y_test: Testing data labels\n",
    "        - cv_splits: Number of cross-validation splits\n",
    "\n",
    "        Returns:\n",
    "        - List of tuples, where each tuple contains (X_train_fold, X_test_fold, y_train_fold, y_test_fold)\n",
    "        \"\"\"\n",
    "        # Initialize StratifiedKFold with the desired number of splits\n",
    "        kf = KFold(n_splits=cv_splits, shuffle=True)  # random_state=42)\n",
    "\n",
    "        # Initialize an empty list to store the data splits\n",
    "        data_splits = []\n",
    "\n",
    "        # Loop through the cross-validation splits\n",
    "        for train_index, test_index in kf.split(X_train, y_train):\n",
    "            X_train_fold, X_val_fold = X_train[train_index], X_train[test_index]\n",
    "            y_train_fold, y_val_fold = y_train[train_index], y_train[test_index]\n",
    "\n",
    "            # Append the current split to the list\n",
    "            data_splits.append((X_train_fold, X_val_fold, y_train_fold, y_val_fold))\n",
    "\n",
    "        # Append the original test data to the list\n",
    "        data_splits.append((X_train, X_test, y_train, y_test))\n",
    "\n",
    "        return data_splits\n",
    "\n",
    "    def evaluate(self, model, X_test, y_test, return_predictions=False):\n",
    "        predictions = model.predict(X_test)\n",
    "        rmsd = mean_squared_error(y_test, predictions, squared=False)\n",
    "        return rmsd, predictions\n",
    "\n",
    "    def train_test_return(self, parameters, model, trial_number):\n",
    "        runs = 3\n",
    "        # average over all runs\n",
    "        runs_results = []\n",
    "        y_tests_predicted = []\n",
    "\n",
    "        for run in range(runs):\n",
    "            validation_splits = self.cross_validation_splits(self.X_train, self.X_test, self.y_train, self.y_test)\n",
    "            # average over all splits in a given run\n",
    "            cv_fold_results = []\n",
    "            y_test_predicted = []\n",
    "            fold_num = 0\n",
    "\n",
    "            # cross-validation\n",
    "            for (X_train_val, X_test_val, y_train_val, y_test_val) in validation_splits:\n",
    "                fold_num += 1\n",
    "                \n",
    "                # train the model on the given validation split\n",
    "                model.fit(X_train_val, y_train_val)\n",
    "                cv_fold_rmsd, validation_predictions = self.evaluate(model, X_test_val, y_test_val)\n",
    "                \n",
    "                # and save the result of that split\n",
    "                cv_fold_results.append(cv_fold_rmsd)\n",
    "                \n",
    "                # after all five folds, append the final predictions\n",
    "                if fold_num == 6:\n",
    "                    y_test_predicted.append(validation_predictions)\n",
    "\n",
    "            runs_results.append(np.mean(cv_fold_results))\n",
    "            y_tests_predicted.append(y_test_predicted)\n",
    "\n",
    "        # calculate the standard deviation of predictions\n",
    "        y_tests_predicted = np.array(y_tests_predicted)\n",
    "        std = np.std(y_tests_predicted, axis=0)[0]\n",
    "        \n",
    "        average_predictions = np.average(y_tests_predicted, axis=0)[0]\n",
    "        average_result = np.mean(runs_results)\n",
    "        \n",
    "        # write the result and hyperparameters of a run to csv file\n",
    "        optuna_trial_logging(self.log_csv_path, trial_number, parameters, average_result, average_predictions, std)\n",
    "\n",
    "        return average_result\n",
    "\n",
    "    def objective(self, trial=None):\n",
    "        parameters, model = self.sample_params(trial, self.model_identifier)\n",
    "        return self.train_test_return(parameters, model, trial.number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48936b61-fcf6-410c-bb54-f05751a023ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-17 17:12:15,733] Using an existing study with name 'linear' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "morgan obach linear\n",
      "morgan obach KRR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-17 17:12:15,950] Using an existing study with name 'KRR' instead of creating a new one.\n",
      "[I 2023-12-17 17:12:16,119] Using an existing study with name 'GB' instead of creating a new one.\n",
      "[I 2023-12-17 17:12:16,121] Trial 1202 finished with value: 0.06667361802284581 and parameters: {'alpha': 0.018537127958601657, 'l1_ratio': 0.518882288269876}. Best is trial 1007 with value: 0.05832439668234344.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "morgan obach GB\n",
      "Successfully updated linear.csv with results of trial 1202\n",
      "morgan obach RF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-17 17:12:16,302] Using an existing study with name 'RF' instead of creating a new one.\n",
      "[I 2023-12-17 17:12:16,489] Using an existing study with name 'ANN' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "morgan obach ANN\n",
      "morgan microsome linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-17 17:12:16,755] Using an existing study with name 'linear' instead of creating a new one.\n",
      "[I 2023-12-17 17:12:16,953] Trial 1201 finished with value: 0.06349276693262527 and parameters: {'alpha': 0.2625922342887563, 'gamma': 2.252945740857649e-16, 'kernel': 'rbf'}. Best is trial 1154 with value: 0.05763325633365717.\n",
      "[I 2023-12-17 17:12:16,956] Using an existing study with name 'KRR' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "morgan microsome KRR\n",
      "Successfully updated KRR.csv with results of trial 1201\n",
      "morgan microsome GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-17 17:12:17,167] Using an existing study with name 'GB' instead of creating a new one.\n",
      "[I 2023-12-17 17:12:17,260] Trial 1201 finished with value: 0.2890649394527991 and parameters: {'alpha': 0.06130801761853888, 'l1_ratio': 0.05322476970582926}. Best is trial 1159 with value: 0.28783839269336703.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "morgan microsome RF\n",
      "Successfully updated linear.csv with results of trial 1201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-17 17:12:17,398] Using an existing study with name 'RF' instead of creating a new one.\n",
      "[I 2023-12-17 17:12:17,617] Using an existing study with name 'ANN' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "morgan microsome ANN\n",
      "morgan hepatocyte linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-17 17:12:17,952] Using an existing study with name 'linear' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "morgan hepatocyte KRR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-17 17:12:18,184] Using an existing study with name 'KRR' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "morgan hepatocyte GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-17 17:12:18,429] Using an existing study with name 'GB' instead of creating a new one.\n",
      "[I 2023-12-17 17:12:18,548] Trial 1201 finished with value: 0.3261168240349056 and parameters: {'alpha': 0.04604510437675812, 'l1_ratio': 0.10319759046147191}. Best is trial 1107 with value: 0.32443212075276023.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "morgan hepatocyte RF\n",
      "Successfully updated linear.csv with results of trial 1201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-17 17:12:18,722] Using an existing study with name 'RF' instead of creating a new one.\n",
      "[I 2023-12-17 17:12:18,788] Trial 1201 finished with value: 0.30546019610756997 and parameters: {'alpha': 0.1785407332397495, 'gamma': 6.141941910422675e-15, 'kernel': 'laplacian'}. Best is trial 580 with value: 0.3041671844687009.\n",
      "[I 2023-12-17 17:12:18,844] Trial 1001 finished with value: 0.09304810932026464 and parameters: {'n_estimators': 10, 'learning_rate': 0.6901048398192198, 'max_depth': 3}. Best is trial 590 with value: 0.06843477254509721.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully updated KRR.csv with results of trial 1201\n",
      "morgan hepatocyte ANN\n",
      "Successfully updated GB.csv with results of trial 1001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-17 17:12:18,982] Using an existing study with name 'ANN' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jazzy obach linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-17 17:12:19,408] Using an existing study with name 'linear' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jazzy obach KRR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-17 17:12:19,648] Using an existing study with name 'KRR' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jazzy obach GB\n",
      "Successfully updated KRR.csv with results of trial 1201\n",
      "Successfully updated linear.csv with results of trial 1201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-17 17:12:19,924] Trial 1201 finished with value: 0.3386709898012134 and parameters: {'alpha': 0.5025235230219876, 'gamma': 9.658854848797873e-15, 'kernel': 'rbf'}. Best is trial 1094 with value: 0.33833623719315026.\n",
      "[I 2023-12-17 17:12:19,930] Using an existing study with name 'GB' instead of creating a new one.\n",
      "[I 2023-12-17 17:12:19,935] Trial 1201 finished with value: 0.06353120657874102 and parameters: {'alpha': 0.0681160254857361, 'l1_ratio': 0.33006347800048336}. Best is trial 76 with value: 0.05617515370850514.\n",
      "[I 2023-12-17 17:12:20,038] Trial 1201 finished with value: 0.06515377107546293 and parameters: {'n_estimators': 20, 'max_features': 'sqrt', 'max_depth': 2}. Best is trial 1103 with value: 0.058874125865533845.\n",
      "[I 2023-12-17 17:12:20,172] Using an existing study with name 'RF' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jazzy obach RF\n",
      "Successfully updated RF.csv with results of trial 1201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-17 17:12:20,424] Using an existing study with name 'ANN' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jazzy obach ANN\n",
      "jazzy microsome linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-17 17:12:20,735] Using an existing study with name 'linear' instead of creating a new one.\n",
      "[I 2023-12-17 17:12:20,774] Trial 1201 finished with value: 0.06164069792792839 and parameters: {'alpha': 0.9366166495009393, 'gamma': 8.507692915949687e-15, 'kernel': 'rbf'}. Best is trial 1144 with value: 0.05888163806771656.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully updated KRR.csv with results of trial 1201\n",
      "jazzy microsome KRR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-17 17:12:21,113] Using an existing study with name 'KRR' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jazzy microsome GB\n",
      "Successfully updated linear.csv with results of trial 1201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-17 17:12:21,380] Using an existing study with name 'GB' instead of creating a new one.\n",
      "[I 2023-12-17 17:12:21,388] Trial 1201 finished with value: 0.29887574015642876 and parameters: {'alpha': 0.08193895939576343, 'l1_ratio': 0.4105370634966366}. Best is trial 1182 with value: 0.2980294497536738.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jazzy microsome RF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-17 17:12:21,695] Using an existing study with name 'RF' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jazzy microsome ANN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-17 17:12:22,003] Using an existing study with name 'ANN' instead of creating a new one.\n",
      "[I 2023-12-17 17:12:22,154] Trial 1201 finished with value: 0.29670224365887593 and parameters: {'alpha': 0.7715950286933926, 'gamma': 2.319782213079134e-15, 'kernel': 'linear'}. Best is trial 724 with value: 0.2956542734080018.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jazzy hepatocyte linear\n",
      "Successfully updated KRR.csv with results of trial 1201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-17 17:12:22,279] Using an existing study with name 'linear' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jazzy hepatocyte KRR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-17 17:12:22,542] Using an existing study with name 'KRR' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jazzy hepatocyte GB\n",
      "Successfully updated linear.csv with results of trial 1201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-17 17:12:22,815] Trial 1201 finished with value: 0.3338284599330294 and parameters: {'alpha': 0.030252963491125964, 'l1_ratio': 0.5941734280432983}. Best is trial 1080 with value: 0.3329620582740734.\n",
      "[I 2023-12-17 17:12:22,823] Using an existing study with name 'GB' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jazzy hepatocyte RF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-17 17:12:23,299] Using an existing study with name 'RF' instead of creating a new one.\n",
      "[I 2023-12-17 17:12:23,436] Trial 1102 finished with value: 0.0615650650155006 and parameters: {'n_estimators': 10, 'learning_rate': 0.027130580419708512, 'max_depth': 3}. Best is trial 99 with value: 0.05917832364524326.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jazzy hepatocyte ANN\n",
      "Successfully updated GB.csv with results of trial 1102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-17 17:12:23,636] Using an existing study with name 'ANN' instead of creating a new one.\n",
      "[I 2023-12-17 17:12:23,920] Trial 1201 finished with value: 0.33884006001965034 and parameters: {'alpha': 0.332320881970816, 'gamma': 3.5223164507414932e-15, 'kernel': 'laplacian'}. Best is trial 1003 with value: 0.338662400699703.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully updated KRR.csv with results of trial 1201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-17 17:12:25,174] Trial 1201 finished with value: 0.3307067912995554 and parameters: {'n_estimators': 10, 'max_features': 'sqrt', 'max_depth': 3}. Best is trial 1029 with value: 0.32781508966158346.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully updated RF.csv with results of trial 1201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-17 17:12:27,181] Trial 1102 finished with value: 0.30094807935879303 and parameters: {'n_estimators': 20, 'learning_rate': 0.24192275021364867, 'max_depth': 5}. Best is trial 861 with value: 0.2879621591584269.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully updated GB.csv with results of trial 1102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-17 17:12:27,532] Trial 1201 finished with value: 0.07417765890305218 and parameters: {'learning_rate_init': 0.030703527554322707, 'hidden_layer_sizes': [10]}. Best is trial 1021 with value: 0.06688075265089947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully updated ANN.csv with results of trial 1201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-17 17:12:28,054] Trial 1102 finished with value: 0.3309167455382222 and parameters: {'n_estimators': 20, 'learning_rate': 0.07065431629303412, 'max_depth': 5}. Best is trial 1069 with value: 0.3283807093809974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully updated GB.csv with results of trial 1102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-17 17:12:31,533] Trial 1201 finished with value: 0.3388470384405495 and parameters: {'learning_rate_init': 0.09969259003136288, 'hidden_layer_sizes': [10]}. Best is trial 1186 with value: 0.3377094095552518.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully updated ANN.csv with results of trial 1201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-17 17:12:35,548] Trial 1173 finished with value: 0.10585132272506141 and parameters: {'learning_rate_init': 0.027868774372077538, 'hidden_layer_sizes': [10, 10, 10]}. Best is trial 1008 with value: 0.07157636029273418.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully updated ANN.csv with results of trial 1173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-17 17:12:41,342] Trial 1147 finished with value: 0.35143903593513026 and parameters: {'learning_rate_init': 0.02104829827015474, 'hidden_layer_sizes': [5, 5, 5]}. Best is trial 1067 with value: 0.3363088992610564.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully updated ANN.csv with results of trial 1147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-17 17:12:45,146] Trial 1102 finished with value: 0.31984236232106117 and parameters: {'n_estimators': 200, 'learning_rate': 0.472893147995886, 'max_depth': 3}. Best is trial 331 with value: 0.2829879661329832.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully updated GB.csv with results of trial 1102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-17 17:12:49,663] Trial 1124 finished with value: 0.3192601294758885 and parameters: {'learning_rate_init': 0.063007092141672, 'hidden_layer_sizes': [50, 50]}. Best is trial 1003 with value: 0.3055344753665534.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully updated ANN.csv with results of trial 1124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-17 17:12:50,246] Trial 1142 finished with value: 0.3285672191442115 and parameters: {'learning_rate_init': 0.040824442352958824, 'hidden_layer_sizes': [50, 50, 50]}. Best is trial 1108 with value: 0.3017441531185905.\n",
      "[I 2023-12-17 17:12:50,345] Trial 1127 finished with value: 0.06363277359066731 and parameters: {'n_estimators': 500, 'max_features': 'sqrt', 'max_depth': 2}. Best is trial 1091 with value: 0.059055450943547895.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully updated ANN.csv with results of trial 1142\n",
      "Successfully updated RF.csv with results of trial 1127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-17 17:12:53,034] Trial 1123 finished with value: 0.27316324886330395 and parameters: {'n_estimators': 200, 'max_features': 'sqrt', 'max_depth': None}. Best is trial 1006 with value: 0.27173792118810963.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully updated RF.csv with results of trial 1123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-17 17:12:54,249] Trial 1122 finished with value: 0.2822410657749162 and parameters: {'n_estimators': 200, 'max_features': 'log2', 'max_depth': None}. Best is trial 1115 with value: 0.27912236584646655.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully updated RF.csv with results of trial 1122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-17 17:12:56,259] Trial 1102 finished with value: 0.334031268298576 and parameters: {'n_estimators': 500, 'learning_rate': 0.11466223750037101, 'max_depth': 2}. Best is trial 949 with value: 0.3188718800318547.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully updated GB.csv with results of trial 1102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-17 17:13:07,375] Trial 1151 finished with value: 0.3188620998283546 and parameters: {'n_estimators': 500, 'max_features': 'sqrt', 'max_depth': None}. Best is trial 1083 with value: 0.3165965720272566.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully updated RF.csv with results of trial 1151\n"
     ]
    }
   ],
   "source": [
    "sampler = samplers['TPESampler']\n",
    "pruner = pruners[\"HyperbandPruner\"]\n",
    "n_trials = 1\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    futures = []\n",
    "    for _type in feature_types:\n",
    "        for benchmark in tdc_benchmarks:\n",
    "            X_train = mol_features[_type][benchmark][\"train\"]\n",
    "            y_train = halflives[_type][benchmark][\"train\"]\n",
    "            X_test = mol_features[_type][benchmark][\"test\"]\n",
    "            y_test = halflives[_type][benchmark][\"test\"]\n",
    "\n",
    "            for model_identifier in model_identifiers:\n",
    "                print(_type, benchmark, model_identifier)\n",
    "                lock_obj = optuna.storages.JournalFileOpenLock(\n",
    "                    f\"./project_resources/optuna/{_type}/{benchmark}/{model_identifier}_journal.log\"\n",
    "                )\n",
    "\n",
    "                storage = JournalStorage(\n",
    "                                JournalFileStorage(f\"./project_resources/optuna/{_type}/{benchmark}/{model_identifier}_journal.log\", lock_obj=lock_obj)\n",
    "                            )\n",
    "                study = optuna.create_study(study_name=model_identifier, directions=['minimize'], pruner=pruner,\n",
    "                                            storage=storage, load_if_exists=True)\n",
    "                \n",
    "                trial_log_csv_path = f\"./project_resources/optuna/{_type}/{benchmark}/{model_identifier}.csv\"\n",
    "                tuner = HyperparamTuner(trial_log_csv_path, model_identifier, X_train, y_train, X_test, y_test)\n",
    "                \n",
    "                futures.append(executor.submit(study.optimize, tuner.objective, n_trials=n_trials))\n",
    "                joblib.dump(study, f\"./project_resources/optuna/{_type}/{benchmark}/{model_identifier}.pkl\")\n",
    "    for future in futures:\n",
    "        future.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3828d27c-7977-4b5c-9a5d-993be9bec01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FOO():\n",
    "    def __init__(self, log_csv_path, model_identifier, X_train, y_train, X_test, y_test):\n",
    "        self.log_csv_path = log_csv_path\n",
    "        self.model_identifier = model_identifier\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "\n",
    "    def sample_params(self, trial: optuna.Trial, model_identifier):\n",
    "        if model_identifier == 'linear':\n",
    "            alpha = trial.suggest_float('alpha', 1e-5, 1e-1)\n",
    "            l1_ratio = trial.suggest_float('l1_ratio', 0, 1)\n",
    "            return {\n",
    "                \"alpha\": alpha,\n",
    "                \"l1_ratio\": l1_ratio\n",
    "            }, ElasticNet(alpha=alpha, l1_ratio=l1_ratio)\n",
    "\n",
    "        if model_identifier == 'KRR':\n",
    "            alpha = trial.suggest_float(\"alpha\", 1e-4, 1)\n",
    "            gamma = trial.suggest_float(\"gamma\", 0, 1e-14)\n",
    "            kernel = trial.suggest_categorical(\"kernel\", [\"linear\", \"laplacian\", \"rbf\"])\n",
    "            return {\n",
    "                \"alpha\": alpha,\n",
    "                \"gamma\": gamma,\n",
    "                \"kernel\": kernel\n",
    "            }, KernelRidge(alpha=alpha, gamma=gamma, kernel=kernel)\n",
    "\n",
    "        if model_identifier == 'GB':\n",
    "            n_estimators = trial.suggest_categorical(\"n_estimators\", [10, 20, 50, 200, 500])\n",
    "            learning_rate = trial.suggest_float(\"learning_rate\", 0.005, 1)\n",
    "            max_depth = trial.suggest_categorical(\"max_depth\", [1, 2, 3, 4, 5])\n",
    "            return {\n",
    "                \"n_estimators\": n_estimators,\n",
    "                \"learning_rate\": learning_rate,\n",
    "                \"max_depth\": max_depth\n",
    "            }, GradientBoostingRegressor(n_estimators=n_estimators, learning_rate=learning_rate, max_depth=max_depth)\n",
    "\n",
    "        if model_identifier == 'RF':\n",
    "            n_estimators = trial.suggest_categorical(\"n_estimators\", [10, 20, 50, 200, 500])\n",
    "            max_features = trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\"])\n",
    "            max_depth = trial.suggest_categorical(\"max_depth\", [None, 2, 3, 4, 5, 10])\n",
    "            return {\n",
    "                \"n_estimators\": n_estimators,\n",
    "                \"max_features\": max_features,\n",
    "                \"max_depth\": max_depth\n",
    "            }, RandomForestRegressor(n_estimators=n_estimators, max_features=max_features, max_depth=max_depth)\n",
    "\n",
    "        if model_identifier == 'ANN':\n",
    "            learning_rate_init = trial.suggest_float(\"learning_rate_init\", 0.001, 0.1)\n",
    "            hidden_layer_sizes = trial.suggest_categorical(\"hidden_layer_sizes\",\n",
    "                                                           [[5], [10], [20], [50], [5]*2, [10]*2, [20]*2, [50]*2, [5]*3, [10]*3, [50]*3])\n",
    "            return {\n",
    "            \"learning_rate_init\": learning_rate_init,\n",
    "            \"hidden_layer_sizes\": hidden_layer_sizes\n",
    "            }, MLPRegressor(learning_rate_init=learning_rate_init, hidden_layer_sizes=hidden_layer_sizes)\n",
    "\n",
    "    def cross_validation_splits(self, X_train, X_test, y_train, y_test, cv_splits=5):\n",
    "        \"\"\"\n",
    "        Splits the data into cv_splits different combinations for cross-validation.\n",
    "\n",
    "        Parameters:\n",
    "        - X_train: Training data features\n",
    "        - X_test: Testing data features\n",
    "        - y_train: Training data labels\n",
    "        - y_test: Testing data labels\n",
    "        - cv_splits: Number of cross-validation splits\n",
    "\n",
    "        Returns:\n",
    "        - List of tuples, where each tuple contains (X_train_fold, X_test_fold, y_train_fold, y_test_fold)\n",
    "        \"\"\"\n",
    "        # Initialize StratifiedKFold with the desired number of splits\n",
    "        kf = KFold(n_splits=cv_splits, shuffle=True)  # random_state=42)\n",
    "\n",
    "        # Initialize an empty list to store the data splits\n",
    "        data_splits = []\n",
    "\n",
    "        # Loop through the cross-validation splits\n",
    "        for train_index, test_index in kf.split(X_train, y_train):\n",
    "            X_train_fold, X_val_fold = X_train[train_index], X_train[test_index]\n",
    "            y_train_fold, y_val_fold = y_train[train_index], y_train[test_index]\n",
    "\n",
    "            # Append the current split to the list\n",
    "            data_splits.append((X_train_fold, X_val_fold, y_train_fold, y_val_fold))\n",
    "\n",
    "        # Append the original test data to the list\n",
    "        data_splits.append((X_train, X_test, y_train, y_test))\n",
    "\n",
    "        return data_splits\n",
    "\n",
    "    def evaluate(self, model, X_test, y_test, return_predictions=False):\n",
    "        predictions = model.predict(X_test)\n",
    "        rmsd = mean_squared_error(y_test, predictions, squared=False)\n",
    "        return rmsd, predictions\n",
    "\n",
    "    def train_predict(self, model, X_train, X_test, y_train, y_test):\n",
    "        validation_splits = self.cross_validation_splits(self.X_train, self.X_test, self.y_train, self.y_test)\n",
    "        # average over all splits in a given run\n",
    "        cv_fold_results = []\n",
    "        y_test_predicted = []\n",
    "        fold_num = 0\n",
    "\n",
    "        # cross-validation\n",
    "        for (X_train_val, X_test_val, y_train_val, y_test_val) in validation_splits:\n",
    "            fold_num += 1\n",
    "\n",
    "            # train the model on the given validation split\n",
    "            model.fit(X_train_val, y_train_val)\n",
    "            cv_fold_rmsd, validation_predictions = self.evaluate(model, X_test_val, y_test_val)\n",
    "\n",
    "            # and save the result of that split\n",
    "            cv_fold_results.append(cv_fold_rmsd)\n",
    "\n",
    "            # after all five folds, append the final predictions\n",
    "            if fold_num == 6:\n",
    "                y_test_predicted.append(validation_predictions)\n",
    "\n",
    "        return np.mean(cv_fold_results), y_test_predicted\n",
    "    \n",
    "    def train_test_return(self, parameters, model, trial_number):\n",
    "        runs = 3\n",
    "        # average over all runs\n",
    "        runs_rmsds = []\n",
    "        y_tests_predicted = []\n",
    "        \n",
    "        partial_train_predict = partial(self.train_predict, model, self.X_train, self.X_test, self.y_train, self.y_test)\n",
    "        \n",
    "        runs_results = Parallel(n_jobs=-1)(\n",
    "            delayed(partial_train_predict)() for run in range(runs)\n",
    "        )\n",
    "\n",
    "        runs_rmsds, y_tests_predicted = zip(*results)\n",
    "\n",
    "        # calculate the standard deviation of predictions\n",
    "        y_tests_predicted = np.array(y_tests_predicted)\n",
    "        std = np.std(y_tests_predicted, axis=0)[0]\n",
    "        \n",
    "        average_predictions = np.average(y_tests_predicted, axis=0)[0]\n",
    "        average_rmsd = np.mean(runs_rmsds)\n",
    "        \n",
    "        # write the result and hyperparameters of a run to csv file\n",
    "        optuna_trial_logging(self.log_csv_path, trial_number, parameters, average_rmsd, average_predictions, std)\n",
    "\n",
    "        return average_rmsd\n",
    "\n",
    "    def objective(self, trial=None):\n",
    "        parameters, model = self.sample_params(trial, self.model_identifier)\n",
    "        return self.train_test_return(parameters, model, trial.number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "52df392b-4063-4dc6-97f6-20b1c6e4e243",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-17 18:32:17,042] Using an existing study with name 'linear' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "morgan obach linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-12-17 18:32:19,088] Trial 1212 failed with parameters: {'alpha': 0.01420339305330507, 'l1_ratio': 0.8060740387091849} because of the following error: BrokenProcessPool('A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.').\n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lukas\\anaconda3\\envs\\soc\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 426, in _process_worker\n",
      "    call_item = call_queue.get(block=True, timeout=timeout)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lukas\\anaconda3\\envs\\soc\\Lib\\multiprocessing\\queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ModuleNotFoundError: No module named 'project_resources.cytochrome_P450'\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lukas\\anaconda3\\envs\\soc\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lukas\\AppData\\Local\\Temp\\ipykernel_1392\\1303823665.py\", line 148, in objective\n",
      "    return self.train_test_return(parameters, model, trial.number)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lukas\\AppData\\Local\\Temp\\ipykernel_1392\\1303823665.py\", line 128, in train_test_return\n",
      "    runs_results = Parallel(n_jobs=-1)(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lukas\\anaconda3\\envs\\soc\\Lib\\site-packages\\joblib\\parallel.py\", line 1952, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lukas\\anaconda3\\envs\\soc\\Lib\\site-packages\\joblib\\parallel.py\", line 1595, in _get_outputs\n",
      "    yield from self._retrieve()\n",
      "  File \"C:\\Users\\Lukas\\anaconda3\\envs\\soc\\Lib\\site-packages\\joblib\\parallel.py\", line 1699, in _retrieve\n",
      "    self._raise_error_fast()\n",
      "  File \"C:\\Users\\Lukas\\anaconda3\\envs\\soc\\Lib\\site-packages\\joblib\\parallel.py\", line 1734, in _raise_error_fast\n",
      "    error_job.get_result(self.timeout)\n",
      "  File \"C:\\Users\\Lukas\\anaconda3\\envs\\soc\\Lib\\site-packages\\joblib\\parallel.py\", line 736, in get_result\n",
      "    return self._return_or_raise()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lukas\\anaconda3\\envs\\soc\\Lib\\site-packages\\joblib\\parallel.py\", line 754, in _return_or_raise\n",
      "    raise self._result\n",
      "joblib.externals.loky.process_executor.BrokenProcessPool: A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.\n",
      "[W 2023-12-17 18:32:19,090] Trial 1212 failed with value None.\n",
      "[I 2023-12-17 18:32:19,208] Using an existing study with name 'KRR' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "morgan obach KRR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-12-17 18:32:21,262] Trial 1210 failed with parameters: {'alpha': 0.2153667170857962, 'gamma': 3.2602102606395927e-16, 'kernel': 'rbf'} because of the following error: BrokenProcessPool('A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.').\n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lukas\\anaconda3\\envs\\soc\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 426, in _process_worker\n",
      "    call_item = call_queue.get(block=True, timeout=timeout)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lukas\\anaconda3\\envs\\soc\\Lib\\multiprocessing\\queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ModuleNotFoundError: No module named 'project_resources.cytochrome_P450'\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lukas\\anaconda3\\envs\\soc\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lukas\\AppData\\Local\\Temp\\ipykernel_1392\\1303823665.py\", line 148, in objective\n",
      "    return self.train_test_return(parameters, model, trial.number)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lukas\\AppData\\Local\\Temp\\ipykernel_1392\\1303823665.py\", line 128, in train_test_return\n",
      "    runs_results = Parallel(n_jobs=-1)(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lukas\\anaconda3\\envs\\soc\\Lib\\site-packages\\joblib\\parallel.py\", line 1952, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lukas\\anaconda3\\envs\\soc\\Lib\\site-packages\\joblib\\parallel.py\", line 1595, in _get_outputs\n",
      "    yield from self._retrieve()\n",
      "  File \"C:\\Users\\Lukas\\anaconda3\\envs\\soc\\Lib\\site-packages\\joblib\\parallel.py\", line 1699, in _retrieve\n",
      "    self._raise_error_fast()\n",
      "  File \"C:\\Users\\Lukas\\anaconda3\\envs\\soc\\Lib\\site-packages\\joblib\\parallel.py\", line 1734, in _raise_error_fast\n",
      "    error_job.get_result(self.timeout)\n",
      "  File \"C:\\Users\\Lukas\\anaconda3\\envs\\soc\\Lib\\site-packages\\joblib\\parallel.py\", line 736, in get_result\n",
      "    return self._return_or_raise()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lukas\\anaconda3\\envs\\soc\\Lib\\site-packages\\joblib\\parallel.py\", line 754, in _return_or_raise\n",
      "    raise self._result\n",
      "joblib.externals.loky.process_executor.BrokenProcessPool: A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.\n",
      "[W 2023-12-17 18:32:21,265] Trial 1210 failed with value None.\n",
      "[I 2023-12-17 18:32:21,402] Using an existing study with name 'GB' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "morgan obach GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2023-12-17 18:32:23,462] Trial 1010 failed with parameters: {'n_estimators': 10, 'learning_rate': 0.49728044655000503, 'max_depth': 1} because of the following error: BrokenProcessPool('A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.').\n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lukas\\anaconda3\\envs\\soc\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 426, in _process_worker\n",
      "    call_item = call_queue.get(block=True, timeout=timeout)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lukas\\anaconda3\\envs\\soc\\Lib\\multiprocessing\\queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ModuleNotFoundError: No module named 'project_resources.cytochrome_P450'\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lukas\\anaconda3\\envs\\soc\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lukas\\AppData\\Local\\Temp\\ipykernel_1392\\1303823665.py\", line 148, in objective\n",
      "    return self.train_test_return(parameters, model, trial.number)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lukas\\AppData\\Local\\Temp\\ipykernel_1392\\1303823665.py\", line 128, in train_test_return\n",
      "    runs_results = Parallel(n_jobs=-1)(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lukas\\anaconda3\\envs\\soc\\Lib\\site-packages\\joblib\\parallel.py\", line 1952, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lukas\\anaconda3\\envs\\soc\\Lib\\site-packages\\joblib\\parallel.py\", line 1595, in _get_outputs\n",
      "    yield from self._retrieve()\n",
      "  File \"C:\\Users\\Lukas\\anaconda3\\envs\\soc\\Lib\\site-packages\\joblib\\parallel.py\", line 1699, in _retrieve\n",
      "    self._raise_error_fast()\n",
      "  File \"C:\\Users\\Lukas\\anaconda3\\envs\\soc\\Lib\\site-packages\\joblib\\parallel.py\", line 1734, in _raise_error_fast\n",
      "    error_job.get_result(self.timeout)\n",
      "  File \"C:\\Users\\Lukas\\anaconda3\\envs\\soc\\Lib\\site-packages\\joblib\\parallel.py\", line 736, in get_result\n",
      "    return self._return_or_raise()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lukas\\anaconda3\\envs\\soc\\Lib\\site-packages\\joblib\\parallel.py\", line 754, in _return_or_raise\n",
      "    raise self._result\n",
      "joblib.externals.loky.process_executor.BrokenProcessPool: A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.\n",
      "[W 2023-12-17 18:32:23,465] Trial 1010 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "morgan obach RF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-17 18:32:23,853] Using an existing study with name 'RF' instead of creating a new one.\n",
      "[W 2023-12-17 18:32:25,891] Trial 1210 failed with parameters: {'n_estimators': 20, 'max_features': 'sqrt', 'max_depth': 2} because of the following error: BrokenProcessPool('A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.').\n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lukas\\anaconda3\\envs\\soc\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 426, in _process_worker\n",
      "    call_item = call_queue.get(block=True, timeout=timeout)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lukas\\anaconda3\\envs\\soc\\Lib\\multiprocessing\\queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ModuleNotFoundError: No module named 'project_resources.cytochrome_P450'\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lukas\\anaconda3\\envs\\soc\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lukas\\AppData\\Local\\Temp\\ipykernel_1392\\1303823665.py\", line 148, in objective\n",
      "    return self.train_test_return(parameters, model, trial.number)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lukas\\AppData\\Local\\Temp\\ipykernel_1392\\1303823665.py\", line 128, in train_test_return\n",
      "    runs_results = Parallel(n_jobs=-1)(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lukas\\anaconda3\\envs\\soc\\Lib\\site-packages\\joblib\\parallel.py\", line 1952, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lukas\\anaconda3\\envs\\soc\\Lib\\site-packages\\joblib\\parallel.py\", line 1595, in _get_outputs\n",
      "    yield from self._retrieve()\n",
      "  File \"C:\\Users\\Lukas\\anaconda3\\envs\\soc\\Lib\\site-packages\\joblib\\parallel.py\", line 1699, in _retrieve\n",
      "    self._raise_error_fast()\n",
      "  File \"C:\\Users\\Lukas\\anaconda3\\envs\\soc\\Lib\\site-packages\\joblib\\parallel.py\", line 1734, in _raise_error_fast\n",
      "    error_job.get_result(self.timeout)\n",
      "  File \"C:\\Users\\Lukas\\anaconda3\\envs\\soc\\Lib\\site-packages\\joblib\\parallel.py\", line 736, in get_result\n",
      "    return self._return_or_raise()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Lukas\\anaconda3\\envs\\soc\\Lib\\site-packages\\joblib\\parallel.py\", line 754, in _return_or_raise\n",
      "    raise self._result\n",
      "joblib.externals.loky.process_executor.BrokenProcessPool: A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.\n",
      "[W 2023-12-17 18:32:25,894] Trial 1210 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m trial_log_csv_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./project_resources/optuna/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbenchmark\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_identifier\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     24\u001b[0m tuner \u001b[38;5;241m=\u001b[39m FOO(trial_log_csv_path, model_identifier, X_train, y_train, X_test, y_test)\n\u001b[1;32m---> 26\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(tuner\u001b[38;5;241m.\u001b[39mobjective, n_trials\u001b[38;5;241m=\u001b[39mn_trials, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     27\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(study, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./project_resources/optuna/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbenchmark\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_identifier\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\soc\\Lib\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     _optimize(\n\u001b[0;32m    452\u001b[0m         study\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    453\u001b[0m         func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m    454\u001b[0m         n_trials\u001b[38;5;241m=\u001b[39mn_trials,\n\u001b[0;32m    455\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    456\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[0;32m    457\u001b[0m         catch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(catch) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(catch, Iterable) \u001b[38;5;28;01melse\u001b[39;00m (catch,),\n\u001b[0;32m    458\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    459\u001b[0m         gc_after_trial\u001b[38;5;241m=\u001b[39mgc_after_trial,\n\u001b[0;32m    460\u001b[0m         show_progress_bar\u001b[38;5;241m=\u001b[39mshow_progress_bar,\n\u001b[0;32m    461\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\soc\\Lib\\site-packages\\optuna\\study\\_optimize.py:85\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     82\u001b[0m time_start \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m     83\u001b[0m futures: Set[Future] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m---> 85\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mn_jobs) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m n_submitted_trials \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mcount():\n\u001b[0;32m     87\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m study\u001b[38;5;241m.\u001b[39m_stop_flag:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\soc\\Lib\\concurrent\\futures\\_base.py:647\u001b[0m, in \u001b[0;36mExecutor.__exit__\u001b[1;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[1;32m--> 647\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshutdown(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\soc\\Lib\\concurrent\\futures\\thread.py:235\u001b[0m, in \u001b[0;36mThreadPoolExecutor.shutdown\u001b[1;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads:\n\u001b[1;32m--> 235\u001b[0m         t\u001b[38;5;241m.\u001b[39mjoin()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\soc\\Lib\\threading.py:1112\u001b[0m, in \u001b[0;36mThread.join\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1109\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock()\n\u001b[0;32m   1113\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1114\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[0;32m   1115\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[0;32m   1116\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\soc\\Lib\\threading.py:1132\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m   1129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lock\u001b[38;5;241m.\u001b[39macquire(block, timeout):\n\u001b[0;32m   1133\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m   1134\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sampler = samplers['TPESampler']\n",
    "pruner = pruners[\"HyperbandPruner\"]\n",
    "n_trials = 1\n",
    "for _type in feature_types:\n",
    "    for benchmark in tdc_benchmarks:\n",
    "        X_train = mol_features[_type][benchmark][\"train\"]\n",
    "        y_train = halflives[_type][benchmark][\"train\"]\n",
    "        X_test = mol_features[_type][benchmark][\"test\"]\n",
    "        y_test = halflives[_type][benchmark][\"test\"]\n",
    "\n",
    "        for model_identifier in model_identifiers:\n",
    "            print(_type, benchmark, model_identifier)\n",
    "            lock_obj = optuna.storages.JournalFileOpenLock(\n",
    "                f\"./project_resources/optuna/{_type}/{benchmark}/{model_identifier}_journal.log\"\n",
    "            )\n",
    "\n",
    "            storage = JournalStorage(\n",
    "                            JournalFileStorage(f\"./project_resources/optuna/{_type}/{benchmark}/{model_identifier}_journal.log\", lock_obj=lock_obj)\n",
    "                        )\n",
    "            study = optuna.create_study(study_name=model_identifier, directions=['minimize'], pruner=pruner,\n",
    "                                        storage=storage, load_if_exists=True)\n",
    "\n",
    "            trial_log_csv_path = f\"./project_resources/optuna/{_type}/{benchmark}/{model_identifier}.csv\"\n",
    "            tuner = FOO(trial_log_csv_path, model_identifier, X_train, y_train, X_test, y_test)\n",
    "\n",
    "            study.optimize(tuner.objective, n_trials=n_trials, n_jobs=-1)\n",
    "            joblib.dump(study, f\"./project_resources/optuna/{_type}/{benchmark}/{model_identifier}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc27701-6344-4702-9abc-a36e071d11ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
