{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9008c0c-217a-4a82-8efa-1bdd0d28a545",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import warnings\n",
    "import py3Dmol\n",
    "from chembl_webresource_client.new_client import new_client as client\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem.MolStandardize.rdMolStandardize import FragmentParent\n",
    "from rdkit import DataStructs\n",
    "from jazzy.api import molecular_vector_from_smiles as mol_vect\n",
    "import numpy as np\n",
    "import pubchempy as pcp\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddc091cb-0b13-470e-af3c-59c794cd6005",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22e6b12d-890c-41ce-8611-9ea45d8466fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "working_dir = os.getcwd()\n",
    "# dir for useful stuff for the actual essay\n",
    "graphs_rel_path = r\"project_results/graphs\"\n",
    "project_results_graphs = os.path.join(working_dir, graphs_rel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "230cfb56-2882-490c-a465-d3d816c4488f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def abs_file_path(rel_path):\n",
    "    working_dir = os.getcwd()\n",
    "    abs_file_path = os.path.join(working_dir, rel_path.replace(\"\\\\\", \"/\"))\n",
    "    return abs_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "152a7416-ab30-445c-90ce-a447c28f7cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_publication_years(list_mol_id):\n",
    "    pub_years = []\n",
    "    if type(list_mol_id[0]) == int:\n",
    "        for cid in list_mol_id:\n",
    "            # call the PubChem API for info about molecule\n",
    "            url = f\"https://pubchem.ncbi.nlm.nih.gov/rest/pug_view/data/compound/{cid}/JSON\"\n",
    "            response = requests.get(url)\n",
    "            data = response.json()\n",
    "            # get the publication year\n",
    "            for section in data['Record']['Section']:\n",
    "                if section[\"TOCHeading\"] == 'Names and Identifiers':\n",
    "                    publication_year = int(section[\"Section\"][-2][\"Information\"][0][\"Value\"][\"DateISO8601\"][0][0:4])\n",
    "                    if not publication_year:\n",
    "                        pub_years.append(None)\n",
    "                        print(f\"could not find publication year of CID {cid}\")\n",
    "                    else:\n",
    "                        pub_years.append(publication_year)\n",
    "    else:\n",
    "        for chembl_id in list_mol_id:\n",
    "            # get activities associated with the molecule\n",
    "            activities = client.activity.filter(molecule_chembl_id=chembl_id)\n",
    "            # extract publication year\n",
    "            publication_year = activities[0][\"document_year\"]\n",
    "            # print the molecule ID if document_year is not specified\n",
    "            if not publication_year:\n",
    "                pub_years.append(None)\n",
    "                print(f\"could not find publication year of ChEMLB ID {chembl_id}\")\n",
    "            else:\n",
    "                pub_years.append(publication_year)\n",
    "    return pub_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c381d76-cdf3-44f5-aec4-ccec30e5fa15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def smiles_from_mol_id(list_mol_id):\n",
    "    # returns a list of smiles strings for given list of mol ids\n",
    "    list_smiles = []\n",
    "    if type(list_mol_id[0]) == int:\n",
    "        for cid in list_mol_id:\n",
    "            compound = pcp.Compound.from_cid(cid)\n",
    "            smiles = compound.isomeric_smiles\n",
    "            list_smiles.append(smiles)\n",
    "    else:\n",
    "        for chembl_id in list_mol_id:\n",
    "            molecule = client.molecule\n",
    "            compound = molecule.filter(chembl_id=chembl_id)[0]\n",
    "            list_smiles.append(compound['molecule_structures'][\"canonical_smiles\"])\n",
    "    return list_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "969e73cd-b678-4f29-9593-4526f26255fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def halflife_formatting(source_df, isozyme):\n",
    "    # creates a correctly formatted list of half-life values from df\n",
    "    halflife = []\n",
    "    if isozyme == \"3A4\":\n",
    "        df_adjusted = source_df\n",
    "        halflife = df_adjusted[\"Standard Value\"]\n",
    "    if isozyme == \"RLM\":\n",
    "        df_adjusted = source_df.replace({\">30\": '30'})\n",
    "        halflife = df_adjusted[\"Half-life (minutes)\"]\n",
    "    if isozyme == \"HLC\":\n",
    "        df_adjusted = source_df\n",
    "        halflife = df_adjusted[\"Half-life\"]\n",
    "\n",
    "    # scale the half-lives so that the values are always between 0 and 1 (inclusive - the min value is 0, max is 1)\n",
    "    reshaped_halflife = np.array(halflife).reshape(-1, 1)\n",
    "    scaler = MinMaxScaler().fit(reshaped_halflife)\n",
    "    halflife_scaled = scaler.transform(reshaped_halflife)\n",
    "    halflife_scaled = [val[0] for val in halflife_scaled]\n",
    "    return halflife_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d723ee2-cd0c-449d-9c77-855f047c75a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def isz_csv_data_formatting(source_csv_file, isozyme, sep=\",\"):\n",
    "    # creates a csv with the columns mol_idx, smiles, half-life, published\n",
    "    source_df = pd.read_csv(abs_file_path(source_csv_file), sep=sep)\n",
    "\n",
    "    if isozyme + \".csv\" in os.listdir(abs_file_path(\"project_resources\")):\n",
    "        print(f\"{isozyme}.csv already exists in dir\")\n",
    "\n",
    "    else:\n",
    "        if isozyme == \"3A4\":\n",
    "            # additional formatting, since not all molecules have the desired property\n",
    "            source_df = source_df[source_df[\"Standard Type\"] == \"T1/2\"]\n",
    "\n",
    "        elif isozyme == \"RLM\":\n",
    "            # get rid of mols with half-life \">30\"\n",
    "            source_df = source_df[source_df[\"Half-life (minutes)\"] != \">30\"]\n",
    "\n",
    "        try:\n",
    "            mol_ids = list(source_df[\"Molecule ChEMBL ID\"])\n",
    "        except KeyError:\n",
    "            mol_ids = list(source_df[\"PUBCHEM_CID\"])\n",
    "\n",
    "        final_df = pd.DataFrame()\n",
    "        publication_years = get_publication_years(mol_ids)\n",
    "        orig_smiles = smiles_from_mol_id(mol_ids)\n",
    "        cleaned_smiles = []\n",
    "\n",
    "        # replace smiles with two disjoint parts with only the more relevant one\n",
    "        for smi in orig_smiles:\n",
    "            mol = Chem.MolFromSmiles(smi)\n",
    "            cleaned_mol = FragmentParent(mol)  # for further info on how this is done see this class\n",
    "            cleaned_smi = Chem.MolToSmiles(cleaned_mol)\n",
    "            cleaned_smiles.append(cleaned_smi)\n",
    "\n",
    "        final_df[\"mol_idx\"] = list(range(1, len(mol_ids)+1))\n",
    "        final_df[\"smiles\"] = cleaned_smiles\n",
    "        final_df[\"half-life\"] = list(halflife_formatting(source_df, isozyme))\n",
    "        final_df[\"published\"] = publication_years\n",
    "        final_df.to_csv(abs_file_path(f\"project_resources/{isozyme}.csv\"), index=False)\n",
    "        print(f\"{isozyme}.csv was successfully created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edb562e5-1b30-4f60-9556-cc1e7eacb33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_csv_data_formatting(isozyme, smiles_as_index, split_smiles, split_type, include_year=False, years=None):\n",
    "    # saves ML splits as csv files containing mol idxs, smiles and half-lives\n",
    "    for split in [\"train\", \"test\"]:\n",
    "        location = f\"project_resources/base_splits/{split_type}\"\n",
    "        file_name = f\"{isozyme}_{split}.csv\"\n",
    "        # check if file already exists\n",
    "        try:\n",
    "            with open(f\"{location}/{file_name}\") as f:\n",
    "                f.close()\n",
    "            print(f\"{file_name} already exists in {location}\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            split_df = pd.DataFrame()\n",
    "            split_indexes = []\n",
    "            split_halflifes = []\n",
    "            isz_scaff_split_smiles = split_smiles[split]\n",
    "\n",
    "            # get the index and half-life values for each smiles in data split\n",
    "            for smi in isz_scaff_split_smiles:\n",
    "                smi_idx = smiles_as_index[isozyme][smi][0]  # numerical index of smiles\n",
    "                split_indexes.append(smi_idx)\n",
    "                mol_halflife = smiles_as_index[isozyme][smi][1]  # half-life value for the specific molecule\n",
    "                split_halflifes.append(mol_halflife)\n",
    "\n",
    "            split_df[\"index\"] = split_indexes\n",
    "            split_df[\"smiles\"] = isz_scaff_split_smiles\n",
    "            split_df[\"half-life\"] = split_halflifes\n",
    "            if include_year:\n",
    "                split_df[\"published\"] = years[isozyme][split]\n",
    "            split_df.to_csv(abs_file_path(f\"{location}/{file_name}\"), index=False)\n",
    "            print(f\"{file_name} was successfully created in {location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d7d951f-ec96-4980-ba4a-327bdb7675da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fp_from_smiles(list_smiles):\n",
    "    list_fingerprint = []\n",
    "    for smi in list_smiles:\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        fingerprint = AllChem.GetMorganFingerprintAsBitVect(mol, useChirality=True, radius=2, nBits=124)\n",
    "        vector = np.array(fingerprint)\n",
    "        list_fingerprint.append(vector)\n",
    "    # takes a list of smiles strings,output is a corresponding Morgan fingerprint as a list\n",
    "    return list_fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70ec53c3-7297-4dba-90d0-fff41d2a1e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactive_scatter_plot(x_data, y_data, x_label, y_label, title, data_legend, include_diagonal=False, y_error=None):\n",
    "    # Create the scatter plot\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add scatter trace\n",
    "    scatter_trace = go.Scatter(x=x_data, y=y_data, mode='markers', name=data_legend)\n",
    "\n",
    "    # Add error bars if y_error is provided\n",
    "    if y_error is not None:\n",
    "        scatter_trace.error_y = dict(type='data', array=y_error, visible=True)\n",
    "\n",
    "    fig.add_trace(scatter_trace)\n",
    "\n",
    "    # Add diagonal line at x=y if include_diagonal is True\n",
    "    if include_diagonal:\n",
    "        diagonal_trace = go.Scatter(x=[min(x_data), max(x_data)], y=[min(x_data), max(x_data)],\n",
    "                                    mode='lines', name='The Diagonal', line=dict(color='orange', dash='dash'))\n",
    "        fig.add_trace(diagonal_trace)\n",
    "\n",
    "    # Update layout with labels and title\n",
    "    fig.update_layout(\n",
    "        xaxis=dict(title=x_label, range=[min(x_data)-0.03, max(x_data)+0.03]),\n",
    "        yaxis=dict(title=y_label, range=[min(y_data)-0.01, max(y_data)+0.01]),\n",
    "        title=title,\n",
    "        showlegend=True,\n",
    "        width=800,  # Set the width of the plot\n",
    "        height=600,  # Set the height of the plot\n",
    "    )\n",
    "\n",
    "    # Show the interactive plot\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56d699bd-3b9b-49ca-be5e-b6e000a065f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_rdkit_fingerprint(fps):\n",
    "    rdkit_fingerprints = []\n",
    "    for prnt in fps:\n",
    "        bitstring = \"\".join(prnt.astype(str))\n",
    "        fp = DataStructs.cDataStructs.CreateFromBitString(bitstring)\n",
    "        rdkit_fingerprints.append(fp)\n",
    "    return rdkit_fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8519f105-50fe-44cb-89f5-d08a48ccf5dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tanimoto(fps, fp2s):\n",
    "    tanimoto_similarities = []\n",
    "    fps = to_rdkit_fingerprint(fps)\n",
    "    fp2s = to_rdkit_fingerprint(fp2s)\n",
    "    for x in fps:\n",
    "        fpsx = []\n",
    "        for y in fp2s:\n",
    "            fpsx.append(DataStructs.TanimotoSimilarity(x, y))\n",
    "        max_tanimoto = max(fpsx)\n",
    "        tanimoto_similarities.append(round(max_tanimoto, 3))\n",
    "    return tanimoto_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "239c2f81-6aa3-48ae-86f6-5c879f6780be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def param_tuning(x_train, x_test, y_train, y_test, type_ml_use):\n",
    "    # !!! určování hodnot pro param tuning, lze vylepšit pomocí np.random.randint\n",
    "\n",
    "    if type_ml_use == 'linear':\n",
    "        param_grid = {\n",
    "            'fit_intercept': [True],\n",
    "            'alpha': [1e-5, 1e-4, 1e-3, 1e-2],\n",
    "            'l1_ratio': [0, 0.1, 0.5, 0.9, 1]\n",
    "        }\n",
    "        reg = ElasticNet()\n",
    "\n",
    "    if type_ml_use == 'KRR':\n",
    "        param_grid = {\n",
    "            \"alpha\": np.logspace(-4, 1, 20),\n",
    "            \"gamma\": np.logspace(-14, 0, 20),\n",
    "            \"kernel\": ['linear', 'laplacian', 'rbf']\n",
    "        }\n",
    "        reg = KernelRidge()\n",
    "\n",
    "    if type_ml_use == 'GB':\n",
    "        param_grid = {\n",
    "            'n_estimators': [10, 20, 50, 200, 400],\n",
    "            'learning_rate': [0.02, 0.05],\n",
    "            'max_depth': [1, 2, 3, 5],\n",
    "        }\n",
    "        reg = GradientBoostingRegressor()\n",
    "\n",
    "    if type_ml_use == 'RF':\n",
    "        param_grid = {\n",
    "            'max_depth': [None, 2, 3, 5, 10],\n",
    "            'max_features': ['auto', 'sqrt', 'log2'],\n",
    "            'n_estimators': [10, 20, 50, 100, 200],\n",
    "        }\n",
    "        reg = RandomForestRegressor()\n",
    "\n",
    "    if type_ml_use == 'ANN':\n",
    "        param_grid = {\n",
    "            'learning_rate_init': [0.001, 0.002, 0.005, 0.01, 0.02, 0.05],\n",
    "            'hidden_layer_sizes': [[5], [10], [20], [50], [5]*2, [10]*2, [20]*2, [50]*2, [5]*3, [10]*3]\n",
    "        }\n",
    "        reg = MLPRegressor()\n",
    "\n",
    "    grid = RandomizedSearchCV(reg, param_grid, cv=KFold(n_splits=5, shuffle=True), verbose=0)\n",
    "    grid.fit(x_train, y_train)\n",
    "    best_reg = grid.best_estimator_\n",
    "    y_train_predict = best_reg.predict(x_train)\n",
    "    y_test_predict = best_reg.predict(x_test)\n",
    "    abs_error = np.abs(y_test_predict-y_test)\n",
    "    print(f\"     best {type_ml_use} hyperparams: {best_reg}\")\n",
    "    # retrain on best hyperparameters\n",
    "    best_reg.fit(x_train, y_train)\n",
    "\n",
    "    return y_train_predict, y_test_predict, abs_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a670f02b-9788-46fa-b641-bdcba9445350",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mol_predict_and_std(models, x_train, x_test, y_train, y_test):\n",
    "    y_test_avg_predict_dict = {}\n",
    "    std_dict = {}\n",
    "    rmsd_dict = {}\n",
    "    for model in models:\n",
    "        y_test_predicts = []\n",
    "\n",
    "        for i in range(3):\n",
    "            asdf, y_test_predict, ghjk = param_tuning(x_train, x_test, y_train, y_test, model)\n",
    "            # asdf, ghjk ... dummy variables, are not needed here\n",
    "            y_test_predicts.append(y_test_predict)\n",
    "\n",
    "        y_test_predicts_array = np.array(y_test_predicts)\n",
    "\n",
    "        y_test_avg_predict = np.average(y_test_predicts_array, axis=0)\n",
    "        standard_deviation = np.std(y_test_predicts_array, axis=0)\n",
    "        rmsd = np.sqrt(np.average(np.square(y_test_avg_predict-y_test)))\n",
    "        # root-mean-square deviation\n",
    "\n",
    "        y_test_avg_predict_dict[model] = y_test_avg_predict\n",
    "        std_dict[model] = standard_deviation\n",
    "        rmsd_dict[model] = rmsd\n",
    "    return y_test_avg_predict_dict, std_dict, rmsd_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9a263bc-ae56-42b2-bd1f-59dd4bf34397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(smi, style='stick'):\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    mol = Chem.AddHs(mol)\n",
    "    AllChem.EmbedMolecule(mol)\n",
    "    AllChem.MMFFOptimizeMolecule(mol, maxIters=200)\n",
    "    mblock = Chem.MolToMolBlock(mol)\n",
    "\n",
    "    view = py3Dmol.view(width=500, height=400)\n",
    "    view.addModel(mblock, 'mol')\n",
    "    view.setStyle({style: {}})\n",
    "    view.zoomTo()\n",
    "    view.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e49f942-3694-4bd5-912c-6abeec5864ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE SPECIFIC TO JAZZY:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7baac92b-d38c-4e5d-afbf-4f14ab6f144a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mol_fts(smiles):\n",
    "    features = []\n",
    "    for smi in smiles:\n",
    "        try:\n",
    "            features.append(mol_vect(smi))\n",
    "        except:\n",
    "            # \"except JazzyError\" gives NameError: name 'JazzyError' is not defined\n",
    "            features.append(np.nan)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be7b2de1-83f5-49bd-97fb-5881119bd212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_jazzy_df(df):\n",
    "    cols = df.columns\n",
    "    data = {}  # all data from csv file (i.e. mol indexes, smiles, half-lives and features)\n",
    "    for col in cols:\n",
    "        data[col] = list(df[col])\n",
    "    nan_idxs = np.argwhere(np.isnan(data[\"dgtot\"]))\n",
    "    nan_idxs = [int(idx) for idx in nan_idxs]\n",
    "    data_clumped = []  # same as data, but in the form [[idx1, smi1, thalf1, fts1], [idx2, smi2, thalf2, fts2],...]]\n",
    "    for col in cols:\n",
    "        for i, foo in zip(range(len(data[col])), data[col]):\n",
    "            if len(data_clumped) < i+1:\n",
    "                data_clumped.append([])\n",
    "            data_clumped[i].append(foo)\n",
    "\n",
    "    # remove all mols for which Jazzy features generation wasn't successful\n",
    "    num_pops = 0\n",
    "    for nan_idx in nan_idxs:\n",
    "        data_clumped.pop(nan_idx - num_pops)\n",
    "        num_pops += 1\n",
    "        print(f\"     removed index {nan_idx} corresponding to NaN\")\n",
    "    print(f\"     {len(data_clumped)}, {data_clumped[0]}\")\n",
    "\n",
    "    # filter out only the features\n",
    "    mol_features = np.array([feature[3:11] for feature in data_clumped])\n",
    "    halflives = np.array([feature[2] for feature in data_clumped])\n",
    "    smiles = np.array([feature[1] for feature in data_clumped])\n",
    "    contains_nan = np.any(np.isnan(mol_features))\n",
    "\n",
    "    return smiles, mol_features, halflives, contains_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bf09b11-afa9-40bd-897e-b81f331b39ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CODE SPECIFIC TO NEQUIP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdad295a-c2f7-40cf-8ae9-5527b0d93815",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def list_splitter(list_to_split, ratio):\n",
    "    elements = len(list_to_split)\n",
    "    middle = int(elements * ratio)\n",
    "    return [list_to_split[:middle], list_to_split[middle:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "633fe58f-db07-4141-ab4a-74f48a1e6606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_elements(list_smiles):\n",
    "    # gets unique symbols from every mol in the list e.g. [\"C1=CC=C(C=C1)O\", \"C1=CSC=C1\"] -> [\"C\", \"O\", \"S\"]\n",
    "    formulae = \"\"\n",
    "    for smiles in list_smiles:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        chemical_formula = Chem.rdMolDescriptors.CalcMolFormula(mol)\n",
    "        formulae += chemical_formula\n",
    "    unique_elements = [part for part in formulae if part.isalpha()]\n",
    "    for idx, element in zip(range(len(unique_elements)), unique_elements):\n",
    "        two_letter_element = \"\"\n",
    "        if element.islower():\n",
    "            two_letter_element += unique_elements[idx - 1]\n",
    "            two_letter_element += unique_elements[idx]\n",
    "            unique_elements.remove(element)\n",
    "            unique_elements.append(two_letter_element)\n",
    "    return set(unique_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "552f6d9a-4041-4fd3-a117-b61338db31e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE SPECIFIC TO OPTUNA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a009a1b-2fd2-4da3-ad95-36060231fbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperparamTuner():\n",
    "    def __init__(self, model_identifier, X_train, y_train, X_test, y_test):\n",
    "        self.model_identifier = model_identifier\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "\n",
    "    def sample_params(self, trial: optuna.Trial, model_identifier):\n",
    "        if model_identifier == 'linear':\n",
    "            fit_intercept = trial.set_user_attr(\"fit_intercept\", True)\n",
    "            alpha = trial.suggest_float('alpha', 1e-5, 1e-1)\n",
    "            l1_ratio = trial.suggest_float('l1_ratio', 0, 1)\n",
    "            return {\n",
    "                \"fit_intercept\": fit_intercept,\n",
    "                \"alpha\": alpha,\n",
    "                \"l1_ratio\": l1_ratio\n",
    "            }, ElasticNet()\n",
    "\n",
    "        if model_identifier == 'KRR':\n",
    "            alpha = trial.suggest_float(\"alpha\", 1e-4, 1)\n",
    "            gamma = trial.suggest_float(\"gamma\", 0, 1e-14)\n",
    "            kernel = trial.suggest_categorical(\"kernel\", [\"linear\", \"laplacian\", \"rbf\"])\n",
    "            return {\n",
    "                \"alpha\": alpha,\n",
    "                \"gamma\": gamma,\n",
    "                \"kernel\": kernel\n",
    "            }, KernelRidge()\n",
    "\n",
    "        if model_identifier == 'GB':\n",
    "            n_estimators = trial.suggest_categorical(\"n_estimators\", [10, 20, 50, 200, 500])\n",
    "            learning_rate = trial.suggest_float(\"learning_rate\", 0.005, 1)\n",
    "            max_depth = trial.suggest_categorical(\"max_depth\", [1, 2, 3, 4, 5])\n",
    "            return {\n",
    "                \"n_estimators\": n_estimators,\n",
    "                \"learning_rate\": learning_rate,\n",
    "                \"max_depth\": max_depth\n",
    "            }, GradientBoostingRegressor()\n",
    "\n",
    "        if model_identifier == 'RF':\n",
    "            n_estimators = trial.suggest_categorical(\"n_estimators\", [10, 20, 50, 200, 500])\n",
    "            max_features = trial.suggest_categorical(\"max_features\", [\"auto\", \"sqrt\", \"log2\"])\n",
    "            max_depth = trial.suggest_categorical(\"max_depth\", [None, 2, 3, 4, 5, 10])\n",
    "            return {\n",
    "                \"n_estimators\": n_estimators,\n",
    "                \"max_features\": max_features,\n",
    "                \"max_depth\": max_depth\n",
    "            }, RandomForestRegressor()\n",
    "\n",
    "        if model_identifier == 'ANN':\n",
    "            learning_rate_init = trial.suggest_float(\"learning_rate_init\", 0.001, 0.1)\n",
    "            hidden_layer_sizes = trial.suggest_categorical(\"hidden_layer_sizes\",\n",
    "                                                           [[5], [10], [20], [50], [5]*2, [10]*2, [20]*2, [50]*2, [5]*3, [10]*3, [50]*3])\n",
    "            return {\n",
    "            \"learning_rate_init\": learning_rate_init,\n",
    "            \"hidden_layer_sizes\": hidden_layer_sizes\n",
    "            }, MLPRegressor()\n",
    "\n",
    "    def cross_validation_splits(self, X_train, X_test, y_train, y_test, cv_splits=5):\n",
    "        \"\"\"\n",
    "        Splits the data into cv_splits different combinations for cross-validation.\n",
    "\n",
    "        Parameters:\n",
    "        - X_train: Training data features\n",
    "        - X_test: Testing data features\n",
    "        - y_train: Training data labels\n",
    "        - y_test: Testing data labels\n",
    "        - cv_splits: Number of cross-validation splits\n",
    "\n",
    "        Returns:\n",
    "        - List of tuples, where each tuple contains (X_train_fold, X_test_fold, y_train_fold, y_test_fold)\n",
    "        \"\"\"\n",
    "        # Initialize StratifiedKFold with the desired number of splits\n",
    "        kf = KFold(n_splits=cv_splits, shuffle=True)  # random_state=42)\n",
    "\n",
    "        # Initialize an empty list to store the data splits\n",
    "        data_splits = []\n",
    "\n",
    "        # Loop through the cross-validation splits\n",
    "        for train_index, test_index in kf.split(X_train, y_train):\n",
    "            X_train_fold, X_val_fold = X_train[train_index], X_train[test_index]\n",
    "            y_train_fold, y_val_fold = y_train[train_index], y_train[test_index]\n",
    "\n",
    "            # Append the current split to the list\n",
    "            data_splits.append((X_train_fold, X_val_fold, y_train_fold, y_val_fold))\n",
    "\n",
    "        # Append the original test data to the list\n",
    "        data_splits.append((X_train, X_test, y_train, y_test))\n",
    "\n",
    "        return data_splits\n",
    "\n",
    "    def evaluate(self, model, X_test, y_test, return_predictions=False):\n",
    "        predictions = model.predict(X_test)\n",
    "        rmsd = mean_squared_error(y_test, predictions, squared=False)\n",
    "        if return_predictions:\n",
    "            return rmsd, predictions\n",
    "        else:\n",
    "            return rmsd\n",
    "\n",
    "    def train_test_return(self, parameters, model, return_predictions=False):\n",
    "        runs = 3\n",
    "        runs_results = []\n",
    "        y_tests_predicted = []\n",
    "\n",
    "        for run in range(runs):\n",
    "            validation_splits = self.cross_validation_splits(self.X_train, self.X_test, self.y_train, self.y_test)\n",
    "            cv_fold_results = []\n",
    "            y_test_predicted = []\n",
    "            fold_num = 0\n",
    "\n",
    "            for (X_train_val, X_test_val, y_train_val, y_test_val) in validation_splits:\n",
    "                fold_num += 1\n",
    "                model.fit(X_train_val, y_train_val)\n",
    "\n",
    "                if return_predictions and fold_num == 6:\n",
    "                    cv_fold_rmsd, validation_predictions = self.evaluate(model, X_test_val, y_test_val, return_predictions=return_predictions)\n",
    "                    y_test_predicted.append(validation_predictions)\n",
    "                else:\n",
    "                    cv_fold_rmsd = self.evaluate(model, X_test_val, y_test_val, return_predictions=False)\n",
    "\n",
    "                cv_fold_results.append(cv_fold_rmsd)\n",
    "\n",
    "            runs_results.append(np.mean(cv_fold_results))\n",
    "            y_tests_predicted.append(y_test_predicted)\n",
    "\n",
    "        # Calculate the standard deviation of predictions\n",
    "        y_tests_predicted = np.array(y_tests_predicted)\n",
    "        std = np.std(y_tests_predicted, axis=0)\n",
    "\n",
    "        if return_predictions:\n",
    "            # Return the mean RMSD, average predictions, and standard deviations\n",
    "            return np.mean(runs_results), np.average(y_tests_predicted, axis=0)[0], std[0]\n",
    "        else:\n",
    "            # Return the mean objective/s of these runs\n",
    "            return np.mean(runs_results)\n",
    "\n",
    "    def objective(self, trial=None):\n",
    "        parameters, model = self.sample_params(trial, self.model_identifier)\n",
    "        return self.train_test_return(parameters, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9918c044-90ad-4a32-9213-c1c1bcca30c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
