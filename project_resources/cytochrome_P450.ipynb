{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9008c0c-217a-4a82-8efa-1bdd0d28a545",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import joblib\n",
    "import py3Dmol\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d7d951f-ec96-4980-ba4a-327bdb7675da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fp_from_smiles(list_smiles):\n",
    "    # converts a list of SMILES strings into a list of Morgan fingerprint bit arrays\n",
    "\n",
    "    list_fingerprint = []\n",
    "    for smi in list_smiles:\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        fingerprint = AllChem.GetMorganFingerprintAsBitVect(mol, useChirality=True, radius=2, nBits=124)\n",
    "        vector = np.array(fingerprint)\n",
    "        list_fingerprint.append(vector)\n",
    "    return list_fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "239c2f81-6aa3-48ae-86f6-5c879f6780be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def param_tuning(x_train, x_test, y_train, y_test, type_ml_use):\n",
    "    # LEGACY HYPERPARAMETER OPTIMIZATION\n",
    "\n",
    "    if type_ml_use == 'linear':\n",
    "        param_grid = {\n",
    "            'fit_intercept': [True],\n",
    "            'alpha': [1e-5, 1e-4, 1e-3, 1e-2],\n",
    "            'l1_ratio': [0, 0.1, 0.5, 0.9, 1]\n",
    "        }\n",
    "        reg = ElasticNet()\n",
    "\n",
    "    if type_ml_use == 'KRR':\n",
    "        param_grid = {\n",
    "            \"alpha\": np.logspace(-4, 1, 20),\n",
    "            \"gamma\": np.logspace(-14, 0, 20),\n",
    "            \"kernel\": ['linear', 'laplacian', 'rbf']\n",
    "        }\n",
    "        reg = KernelRidge()\n",
    "\n",
    "    if type_ml_use == 'GB':\n",
    "        param_grid = {\n",
    "            'n_estimators': [10, 20, 50, 200, 400],\n",
    "            'learning_rate': [0.02, 0.05],\n",
    "            'max_depth': [1, 2, 3, 5],\n",
    "        }\n",
    "        reg = GradientBoostingRegressor()\n",
    "\n",
    "    if type_ml_use == 'RF':\n",
    "        param_grid = {\n",
    "            'max_depth': [None, 2, 3, 5, 10],\n",
    "            'max_features': ['auto', 'sqrt', 'log2'],\n",
    "            'n_estimators': [10, 20, 50, 100, 200],\n",
    "        }\n",
    "        reg = RandomForestRegressor()\n",
    "\n",
    "    if type_ml_use == 'ANN':\n",
    "        param_grid = {\n",
    "            'learning_rate_init': [0.001, 0.002, 0.005, 0.01, 0.02, 0.05],\n",
    "            'hidden_layer_sizes': [[5], [10], [20], [50], [5]*2, [10]*2, [20]*2, [50]*2, [5]*3, [10]*3]\n",
    "        }\n",
    "        reg = MLPRegressor()\n",
    "\n",
    "    grid = RandomizedSearchCV(reg, param_grid, cv=KFold(n_splits=5, shuffle=True), verbose=0)\n",
    "    grid.fit(x_train, y_train)\n",
    "    best_reg = grid.best_estimator_\n",
    "    y_train_predict = best_reg.predict(x_train)\n",
    "    y_test_predict = best_reg.predict(x_test)\n",
    "    abs_error = np.abs(y_test_predict-y_test)\n",
    "    print(f\"     best {type_ml_use} hyperparams: {best_reg}\")\n",
    "    # retrain on best hyperparameters\n",
    "    best_reg.fit(x_train, y_train)\n",
    "\n",
    "    return y_train_predict, y_test_predict, abs_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a670f02b-9788-46fa-b641-bdcba9445350",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mol_predict_and_std(models, x_train, x_test, y_train, y_test):\n",
    "    # LEGACY HYPERPARAMETER OPTIMIZATION\n",
    "    \n",
    "    y_test_avg_predict_dict = {}\n",
    "    std_dict = {}\n",
    "    rmsd_dict = {}\n",
    "    for model in models:\n",
    "        y_test_predicts = []\n",
    "\n",
    "        for i in range(3):\n",
    "            asdf, y_test_predict, ghjk = param_tuning(x_train, x_test, y_train, y_test, model)\n",
    "            # asdf, ghjk ... dummy variables, are not needed here\n",
    "            y_test_predicts.append(y_test_predict)\n",
    "\n",
    "        y_test_predicts_array = np.array(y_test_predicts)\n",
    "\n",
    "        y_test_avg_predict = np.average(y_test_predicts_array, axis=0)\n",
    "        standard_deviation = np.std(y_test_predicts_array, axis=0)\n",
    "        rmsd = np.sqrt(np.average(np.square(y_test_avg_predict-y_test)))\n",
    "        # root-mean-square deviation\n",
    "\n",
    "        y_test_avg_predict_dict[model] = y_test_avg_predict\n",
    "        std_dict[model] = standard_deviation\n",
    "        rmsd_dict[model] = rmsd\n",
    "    return y_test_avg_predict_dict, std_dict, rmsd_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9a263bc-ae56-42b2-bd1f-59dd4bf34397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(smi, style='stick'):\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    mol = Chem.AddHs(mol)\n",
    "    AllChem.EmbedMolecule(mol)\n",
    "    AllChem.MMFFOptimizeMolecule(mol, maxIters=200)\n",
    "    mblock = Chem.MolToMolBlock(mol)\n",
    "\n",
    "    view = py3Dmol.view(width=500, height=400)\n",
    "    view.addModel(mblock, 'mol')\n",
    "    view.setStyle({style: {}})\n",
    "    view.zoomTo()\n",
    "    view.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e49f942-3694-4bd5-912c-6abeec5864ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE SPECIFIC TO JAZZY:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be7b2de1-83f5-49bd-97fb-5881119bd212",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_jazzy_df(df, no_idx_smi = False):\n",
    "    cols = df.columns\n",
    "    data = {}  # all data from csv file (i.e. mol indexes, smiles, half-lives and features)\n",
    "    for col in cols:\n",
    "        data[col] = list(df[col])\n",
    "    nan_idxs = np.argwhere(np.isnan(data[\"dgtot\"]))\n",
    "    nan_idxs = [int(idx) for idx in nan_idxs]\n",
    "    data_clumped = []  # same as data, but in the form [[idx1, smi1, thalf1, fts1], [idx2, smi2, thalf2, fts2],...]]\n",
    "    for col in cols:\n",
    "        for i, foo in zip(range(len(data[col])), data[col]):\n",
    "            if len(data_clumped) < i+1:\n",
    "                data_clumped.append([])\n",
    "            data_clumped[i].append(foo)\n",
    "\n",
    "    # remove all mols for which Jazzy features generation wasn't successful\n",
    "    num_pops = 0\n",
    "    for nan_idx in nan_idxs:\n",
    "        data_clumped.pop(nan_idx - num_pops)\n",
    "        num_pops += 1\n",
    "        print(f\"     removed index {nan_idx} corresponding to NaN\")\n",
    "    print(f\"     {len(data_clumped)}, {data_clumped[0]}\")\n",
    "\n",
    "    # filter out only the features\n",
    "    if no_idx_smi:\n",
    "        mol_features = np.array([feature[1:7] for feature in data_clumped])\n",
    "        halflives = np.array([feature[0] for feature in data_clumped])\n",
    "        contains_nan = np.any(np.isnan(mol_features))\n",
    "        return mol_features, halflives, contains_nan\n",
    "    else:\n",
    "        mol_features = np.array([feature[3:9] for feature in data_clumped])\n",
    "        halflives = np.array([feature[2] for feature in data_clumped])\n",
    "        smiles = np.array([feature[1] for feature in data_clumped])\n",
    "        contains_nan = np.any(np.isnan(mol_features))\n",
    "        return smiles, mol_features, halflives, contains_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "552f6d9a-4041-4fd3-a117-b61338db31e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE SPECIFIC TO OPTUNA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e22c231-9f9e-42d4-9f15-2883261378b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def optuna_trial_logging(log_csv_path, trial_number, parameters, rmsd, predictions, std):\n",
    "    # Check if the CSV file exists\n",
    "    is_new_file = not os.path.isfile(log_csv_path)\n",
    "\n",
    "    # Open the CSV file in append mode\n",
    "    with open(log_csv_path, 'a', newline='') as csvfile:\n",
    "        fieldnames = ['Trial Number', 'Parameters', 'RMSD', 'Predictions', 'Std']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        # If the file is newly created, write the header\n",
    "        if is_new_file:\n",
    "            writer.writeheader()\n",
    "\n",
    "        # Write the data for the current trial\n",
    "        writer.writerow({\n",
    "            'Trial Number': trial_number,\n",
    "            'Parameters': parameters,\n",
    "            'RMSD': rmsd,\n",
    "            'Predictions': predictions.tolist(),  # Convert numpy array to a list for CSV\n",
    "            'Std': std.tolist()  # Convert numpy array to a list for CSV\n",
    "        })\n",
    "\n",
    "    # Extract only the filename using regular expressions\n",
    "    file_name_match = re.search(r'[^\\\\/:*?\"<>|\\r\\n]+$', log_csv_path)\n",
    "    file_name = file_name_match.group() if file_name_match else log_csv_path\n",
    "\n",
    "    # Print appropriate success message with the filename\n",
    "    if is_new_file:\n",
    "        print(f\"Successfully created {file_name} with results of trial {trial_number} as the first entry\")\n",
    "    else:\n",
    "        print(f\"Successfully updated {file_name} with results of trial {trial_number}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35165f13-9db6-449b-96a2-60971aafde3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HyperparamTuner():\n",
    "    def __init__(self, log_csv_path, model_identifier, X_train, y_train, X_val, y_val):\n",
    "        self.log_csv_path = log_csv_path\n",
    "        self.model_identifier = model_identifier\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "\n",
    "    def sample_params(self, trial: optuna.Trial, model_identifier):\n",
    "        if model_identifier == 'linear':\n",
    "            alpha = trial.suggest_float('alpha', 0.085, 0.15)\n",
    "            l1_ratio = trial.suggest_float('l1_ratio', 0, 0.1)\n",
    "            return {\n",
    "                \"alpha\": alpha,\n",
    "                \"l1_ratio\": l1_ratio\n",
    "            }, ElasticNet(alpha=alpha, l1_ratio=l1_ratio)\n",
    "\n",
    "        if model_identifier == 'KRR':\n",
    "            alpha = trial.suggest_float(\"alpha\", 0.3, 1)\n",
    "            gamma = trial.suggest_float(\"gamma\", 0.1, 0.3)\n",
    "            kernel = trial.suggest_categorical(\"kernel\", [\"laplacian\", \"rbf\"])\n",
    "            return {\n",
    "                \"alpha\": alpha,\n",
    "                \"gamma\": gamma,\n",
    "                \"kernel\": kernel\n",
    "            }, KernelRidge(alpha=alpha, gamma=gamma, kernel=kernel)\n",
    "\n",
    "        if model_identifier == 'GB':\n",
    "            n_estimators = trial.suggest_categorical(\"n_estimators\", [5, 10, 20, 50])\n",
    "            learning_rate = trial.suggest_float(\"learning_rate\", 0.05, 0.175)\n",
    "            max_depth = trial.suggest_categorical(\"max_depth\", [1, 2, 3])\n",
    "            return {\n",
    "                \"n_estimators\": n_estimators,\n",
    "                \"learning_rate\": learning_rate,\n",
    "                \"max_depth\": max_depth\n",
    "            }, GradientBoostingRegressor(n_estimators=n_estimators, learning_rate=learning_rate, max_depth=max_depth)\n",
    "\n",
    "        if model_identifier == 'RF':\n",
    "            n_estimators = trial.suggest_categorical(\"n_estimators\", [500, 750, 1000])\n",
    "            max_features = trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\"])\n",
    "            max_depth = trial.suggest_categorical(\"max_depth\", [None, 2, 5, 10, 20])\n",
    "            return {\n",
    "                \"n_estimators\": n_estimators,\n",
    "                \"max_features\": max_features,\n",
    "                \"max_depth\": max_depth\n",
    "            }, RandomForestRegressor(n_estimators=n_estimators, max_features=max_features, max_depth=max_depth)\n",
    "\n",
    "        if model_identifier == 'ANN':\n",
    "            learning_rate_init = trial.suggest_float(\"learning_rate_init\", 0.05, 0.15)\n",
    "            hidden_layer_sizes = trial.suggest_categorical(\"hidden_layer_sizes\",\n",
    "                                                           [[3]*3, [5]*3, [3]*5, [5]*5, [10]*3])\n",
    "            return {\n",
    "            \"learning_rate_init\": learning_rate_init,\n",
    "            \"hidden_layer_sizes\": hidden_layer_sizes\n",
    "            }, MLPRegressor(learning_rate_init=learning_rate_init, hidden_layer_sizes=hidden_layer_sizes)\n",
    "\n",
    "    def cross_validation_splits(self, X_train, X_val, y_train, y_val, cv_splits=5):\n",
    "        \"\"\"\n",
    "        Splits the data into cv_splits different combinations for cross-validation.\n",
    "\n",
    "        Parameters:\n",
    "        - X_train: Training data features\n",
    "        - X_val: Testing data features\n",
    "        - y_train: Training data labels\n",
    "        - y_val: Testing data labels\n",
    "        - cv_splits: Number of cross-validation splits\n",
    "\n",
    "        Returns:\n",
    "        - List of tuples, where each tuple contains (X_train_fold, X_val_fold, y_train_fold, y_val_fold)\n",
    "        \"\"\"\n",
    "        # Initialize StratifiedKFold with the desired number of splits\n",
    "        kf = KFold(n_splits=cv_splits, shuffle=True)  # random_state=42)\n",
    "\n",
    "        # Initialize an empty list to store the data splits\n",
    "        data_splits = []\n",
    "\n",
    "        # Loop through the cross-validation splits\n",
    "        for train_index, val_index in kf.split(X_train, y_train):\n",
    "            X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "            y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "\n",
    "            # Append the current split to the list\n",
    "            data_splits.append((X_train_fold, X_val_fold, y_train_fold, y_val_fold))\n",
    "\n",
    "        # Append the original val data to the list\n",
    "        data_splits.append((X_train, X_val, y_train, y_val))\n",
    "\n",
    "        return data_splits\n",
    "\n",
    "    def evaluate(self, model, X_val, y_val):\n",
    "        predictions = model.predict(X_val)\n",
    "        rmsd = mean_squared_error(y_val, predictions, squared=False)\n",
    "        return rmsd, predictions\n",
    "\n",
    "    def train_val_return(self, parameters, model, trial_number):\n",
    "        runs = 3\n",
    "        # average over all runs\n",
    "        runs_results = []\n",
    "        y_vals_predicted = []\n",
    "\n",
    "        for run in range(runs):\n",
    "            validation_splits = self.cross_validation_splits(self.X_train, self.X_val, self.y_train, self.y_val)\n",
    "            # average over all splits in a given run\n",
    "            cv_fold_results = []\n",
    "            y_val_predicted = []\n",
    "            fold_num = 0\n",
    "\n",
    "            # cross-validation\n",
    "            for (X_train_split, X_val_split, y_train_split, y_val_split) in validation_splits:\n",
    "                fold_num += 1\n",
    "                \n",
    "                # train the model on the given validation split\n",
    "                model.fit(X_train_split, y_train_split)\n",
    "                cv_fold_rmsd, validation_predictions = self.evaluate(model, X_val_split, y_val_split)\n",
    "                \n",
    "                # and save the result of that split\n",
    "                cv_fold_results.append(cv_fold_rmsd)\n",
    "                \n",
    "                # after all five folds, append the final predictions\n",
    "                if fold_num == 6:\n",
    "                    y_val_predicted.append(validation_predictions)\n",
    "\n",
    "            runs_results.append(np.mean(cv_fold_results))\n",
    "            y_vals_predicted.append(y_val_predicted)\n",
    "\n",
    "        # calculate the standard deviation of predictions\n",
    "        y_vals_predicted = np.array(y_vals_predicted)\n",
    "        std = np.std(y_vals_predicted, axis=0)[0]\n",
    "        \n",
    "        average_predictions = np.average(y_vals_predicted, axis=0)[0]\n",
    "        average_result = np.mean(runs_results)\n",
    "        \n",
    "        # write the result and hyperparameters of a run to csv file\n",
    "        optuna_trial_logging(self.log_csv_path, trial_number, parameters, average_result, average_predictions, std)\n",
    "\n",
    "        return average_result\n",
    "\n",
    "    def objective(self, trial=None):\n",
    "        parameters, model = self.sample_params(trial, self.model_identifier)\n",
    "        return self.train_val_return(parameters, model, trial.number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980ef532-4907-4090-8706-d0bd3b5b2696",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
