{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9008c0c-217a-4a82-8efa-1bdd0d28a545",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import joblib\n",
    "import py3Dmol\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import quote\n",
    "from functools import partial\n",
    "import ipywidgets as widgets\n",
    "from tdc.single_pred import ADME\n",
    "from IPython.display import display\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from jazzy.api import molecular_vector_from_smiles as mol_vect\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d7d951f-ec96-4980-ba4a-327bdb7675da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fp_from_smiles(list_smiles):\n",
    "    # converts a list of SMILES strings into a list of Morgan fingerprint bit arrays\n",
    "\n",
    "    list_fingerprint = []\n",
    "    for smi in list_smiles:\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        fingerprint = AllChem.GetMorganFingerprintAsBitVect(mol, useChirality=True, radius=2, nBits=124)\n",
    "        vector = np.array(fingerprint)\n",
    "        list_fingerprint.append(vector)\n",
    "    return list_fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "239c2f81-6aa3-48ae-86f6-5c879f6780be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def param_tuning(x_train, x_test, y_train, y_test, type_ml_use):\n",
    "    # LEGACY HYPERPARAMETER OPTIMIZATION\n",
    "\n",
    "    if type_ml_use == 'linear':\n",
    "        param_grid = {\n",
    "            'fit_intercept': [True],\n",
    "            'alpha': [1e-5, 1e-4, 1e-3, 1e-2],\n",
    "            'l1_ratio': [0, 0.1, 0.5, 0.9, 1]\n",
    "        }\n",
    "        reg = ElasticNet()\n",
    "\n",
    "    if type_ml_use == 'KRR':\n",
    "        param_grid = {\n",
    "            \"alpha\": np.logspace(-4, 1, 20),\n",
    "            \"gamma\": np.logspace(-14, 0, 20),\n",
    "            \"kernel\": ['linear', 'laplacian', 'rbf']\n",
    "        }\n",
    "        reg = KernelRidge()\n",
    "\n",
    "    if type_ml_use == 'GB':\n",
    "        param_grid = {\n",
    "            'n_estimators': [10, 20, 50, 200, 400],\n",
    "            'learning_rate': [0.02, 0.05],\n",
    "            'max_depth': [1, 2, 3, 5],\n",
    "        }\n",
    "        reg = GradientBoostingRegressor()\n",
    "\n",
    "    if type_ml_use == 'RF':\n",
    "        param_grid = {\n",
    "            'max_depth': [None, 2, 3, 5, 10],\n",
    "            'max_features': ['auto', 'sqrt', 'log2'],\n",
    "            'n_estimators': [10, 20, 50, 100, 200],\n",
    "        }\n",
    "        reg = RandomForestRegressor()\n",
    "\n",
    "    if type_ml_use == 'ANN':\n",
    "        param_grid = {\n",
    "            'learning_rate_init': [0.001, 0.002, 0.005, 0.01, 0.02, 0.05],\n",
    "            'hidden_layer_sizes': [[5], [10], [20], [50], [5]*2, [10]*2, [20]*2, [50]*2, [5]*3, [10]*3]\n",
    "        }\n",
    "        reg = MLPRegressor()\n",
    "\n",
    "    grid = RandomizedSearchCV(reg, param_grid, cv=KFold(n_splits=5, shuffle=True), verbose=0)\n",
    "    grid.fit(x_train, y_train)\n",
    "    best_reg = grid.best_estimator_\n",
    "    y_train_predict = best_reg.predict(x_train)\n",
    "    y_test_predict = best_reg.predict(x_test)\n",
    "    abs_error = np.abs(y_test_predict-y_test)\n",
    "    print(f\"     best {type_ml_use} hyperparams: {best_reg}\")\n",
    "    # retrain on best hyperparameters\n",
    "    best_reg.fit(x_train, y_train)\n",
    "\n",
    "    return y_train_predict, y_test_predict, abs_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a670f02b-9788-46fa-b641-bdcba9445350",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mol_predict_and_std(models, x_train, x_test, y_train, y_test):\n",
    "    # LEGACY HYPERPARAMETER OPTIMIZATION\n",
    "    \n",
    "    y_test_avg_predict_dict = {}\n",
    "    std_dict = {}\n",
    "    rmsd_dict = {}\n",
    "    for model in models:\n",
    "        y_test_predicts = []\n",
    "\n",
    "        for i in range(3):\n",
    "            asdf, y_test_predict, ghjk = param_tuning(x_train, x_test, y_train, y_test, model)\n",
    "            # asdf, ghjk ... dummy variables, are not needed here\n",
    "            y_test_predicts.append(y_test_predict)\n",
    "\n",
    "        y_test_predicts_array = np.array(y_test_predicts)\n",
    "\n",
    "        y_test_avg_predict = np.average(y_test_predicts_array, axis=0)\n",
    "        standard_deviation = np.std(y_test_predicts_array, axis=0)\n",
    "        rmsd = np.sqrt(np.average(np.square(y_test_avg_predict-y_test)))\n",
    "        # root-mean-square deviation\n",
    "\n",
    "        y_test_avg_predict_dict[model] = y_test_avg_predict\n",
    "        std_dict[model] = standard_deviation\n",
    "        rmsd_dict[model] = rmsd\n",
    "    return y_test_avg_predict_dict, std_dict, rmsd_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9a263bc-ae56-42b2-bd1f-59dd4bf34397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(smi, style='stick'):\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    mol = Chem.AddHs(mol)\n",
    "    AllChem.EmbedMolecule(mol)\n",
    "    AllChem.MMFFOptimizeMolecule(mol, maxIters=200)\n",
    "    mblock = Chem.MolToMolBlock(mol)\n",
    "\n",
    "    view = py3Dmol.view(width=500, height=400)\n",
    "    view.addModel(mblock, 'mol')\n",
    "    view.setStyle({style: {}})\n",
    "    view.zoomTo()\n",
    "    view.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2aa904d0-3d2d-4d4b-8a97-5ba453c8e133",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CODE SPECIFIC TO INFERENCE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c9a80d0-908c-42ec-98bf-cc81e07c3212",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def retrain_and_save(model, file_path, X_train, y_train):\n",
    "    # retrain a model on the best hyperparameters and save the model as pkl\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    joblib.dump(model, file_path)\n",
    "    print(file_path, \" was successfully created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5ce6648-c198-416f-9a51-abf462977fc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def CIRconvert(ids):\n",
    "    # convert molecule name written by user into SMILES\n",
    "    \n",
    "    try:\n",
    "        url = 'http://cactus.nci.nih.gov/chemical/structure/' + quote(ids) + '/smiles'\n",
    "        ans = urlopen(url).read().decode('utf8')\n",
    "        return ans\n",
    "    except:\n",
    "        print(f'Něco se nepovedlo. Jste si jistí, že nemáte v názvu \"{ids}\" překlep?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12d2ea89-b399-4e8b-b7c7-eaf9755ffa4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def contains_nan(lst):\n",
    "    for item in lst:\n",
    "        if isinstance(item, dict):\n",
    "            for v in item.values():\n",
    "                if isinstance(v, (int, float)) and np.isnan(v):\n",
    "                    return True\n",
    "                elif isinstance(v, np.ndarray) and np.isnan(v).any():\n",
    "                    return True\n",
    "        elif isinstance(item, np.ndarray) and np.isnan(item).any():\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53ec98c5-848c-4b7d-a9ba-d064441b10f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inference_predict(_type, benchmark, model_id, user_smi):\n",
    "\n",
    "    model_path = f\"project_resources/optuna/{_type}/{benchmark}/{model_id}.joblib\"\n",
    "    model = joblib.load(model_path)\n",
    "    dataset_names = {\"obach\": 'Half_Life_Obach', \"microsome\": 'Clearance_Microsome_AZ', \"hepatocyte\": 'Clearance_Hepatocyte_AZ'}\n",
    "    dataset_units = {\"obach\": \"h\", \"microsome\": \"ml.min-1.g-1\", \"hepatocyte\": \"μl.min-1.(10^6 buněk)-1\"}\n",
    "\n",
    "    if _type == \"morgan\":\n",
    "        user_fp = fp_from_smiles([user_smi])\n",
    "        y_predict = model.predict(user_fp)\n",
    "    else:\n",
    "        user_fp = np.array([list(fts.values()) for fts in mol_fts([user_smi])])\n",
    "        y_predict = model.predict(user_fp)\n",
    "        adme_dataset = ADME(name=dataset_names[benchmark])\n",
    "        adme_split = adme_dataset.get_split()\n",
    "        adme_train_y = adme_split[\"train\"][\"Y\"]\n",
    "        adme_test_y = adme_split[\"test\"][\"Y\"]\n",
    "        adme_train_test_y = list(adme_train_y) + list(adme_test_y)\n",
    "        reshaped_halflife = np.array(adme_train_test_y).reshape(-1, 1)\n",
    "        scaler = MinMaxScaler().fit(reshaped_halflife)\n",
    "        reshaped_predict = np.array(y_predict).reshape(-1, 1)\n",
    "        y_predict = scaler.inverse_transform(reshaped_predict)[0]\n",
    "    \n",
    "    if not contains_nan(user_fp):\n",
    "        y_predict_rounded = np.round(np.abs(y_predict)[0], decimals=1)\n",
    "        benchmark_units = dataset_units[benchmark]\n",
    "        if benchmark == \"obach\":\n",
    "            print(f\"Predikovaná hodnota eliminačního poločasu: {y_predict_rounded} {benchmark_units}\")\n",
    "        else:\n",
    "            print(f\"Predikovaná hodnota clearance: {y_predict_rounded} {benchmark_units}\")\n",
    "    else:\n",
    "        print(\"X_test obsahuje hodnotu NaN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bbc5c60-0f7d-48e2-85b6-8fb02e4ca687",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Handle the button click event\n",
    "def on_button_click(selected, dropdowns, _):\n",
    "    selected.clear()\n",
    "    for dropdown in dropdowns:\n",
    "        selected.append(dropdown.value)\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a7b9dfa-e490-4f7d-9303-1a589b9c05bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inference_dataset_selection():  \n",
    "    model_identifiers = [\"linear\", \"KRR\", \"GB\", \"RF\", \"ANN\"]\n",
    "    tdc_benchmarks = [\"obach\", \"microsome\", \"hepatocyte\"]\n",
    "    feature_types = [\"morgan\", \"jazzy\"]\n",
    "\n",
    "    # Create and display dropdown menus using a loop\n",
    "    dropdowns = []\n",
    "\n",
    "    for options, description in zip([feature_types, tdc_benchmarks, model_identifiers],\n",
    "                                    ['Features:', 'Dataset:', 'Model:']):\n",
    "        dropdown = widgets.Dropdown(\n",
    "            options=options,\n",
    "            value=options[0],\n",
    "            description=description\n",
    "        )\n",
    "        dropdowns.append(dropdown)\n",
    "        display(dropdown)\n",
    "\n",
    "    # Create a button to trigger the action\n",
    "    selected = []  # Define an empty list to store selected options\n",
    "    button = widgets.Button(description=\"Parse Output\")\n",
    "    button.on_click(partial(on_button_click, selected, dropdowns))\n",
    "    display(button)\n",
    "\n",
    "    return selected  # Return the selected options list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e49f942-3694-4bd5-912c-6abeec5864ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE SPECIFIC TO JAZZY:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "049ae9cf-d52a-495c-826b-7540418b7d34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mol_fts(smiles):\n",
    "    features = []\n",
    "    for smi in smiles:\n",
    "        try:\n",
    "            features.append(mol_vect(smi))\n",
    "        except:\n",
    "            # \"except JazzyError\" gives NameError: name 'JazzyError' is not defined\n",
    "            features.append(np.nan)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be7b2de1-83f5-49bd-97fb-5881119bd212",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_jazzy_df(df, no_idx_smi = False):\n",
    "    cols = df.columns\n",
    "    data = {}  # all data from csv file (i.e. mol indexes, smiles, half-lives and features)\n",
    "    for col in cols:\n",
    "        data[col] = list(df[col])\n",
    "    nan_idxs = np.argwhere(np.isnan(data[\"dgtot\"]))\n",
    "    nan_idxs = [int(idx) for idx in nan_idxs]\n",
    "    data_clumped = []  # same as data, but in the form [[idx1, smi1, thalf1, fts1], [idx2, smi2, thalf2, fts2],...]]\n",
    "    for col in cols:\n",
    "        for i, foo in zip(range(len(data[col])), data[col]):\n",
    "            if len(data_clumped) < i+1:\n",
    "                data_clumped.append([])\n",
    "            data_clumped[i].append(foo)\n",
    "\n",
    "    # remove all mols for which Jazzy features generation wasn't successful\n",
    "    num_pops = 0\n",
    "    for nan_idx in nan_idxs:\n",
    "        data_clumped.pop(nan_idx - num_pops)\n",
    "        num_pops += 1\n",
    "        print(f\"     removed index {nan_idx} corresponding to NaN\")\n",
    "    print(f\"     {len(data_clumped)}, {data_clumped[0]}\")\n",
    "\n",
    "    # filter out only the features\n",
    "    if no_idx_smi:\n",
    "        mol_features = np.array([feature[1:7] for feature in data_clumped])\n",
    "        halflives = np.array([feature[0] for feature in data_clumped])\n",
    "        contains_nan = np.any(np.isnan(mol_features))\n",
    "        return mol_features, halflives, contains_nan\n",
    "    else:\n",
    "        mol_features = np.array([feature[3:9] for feature in data_clumped])\n",
    "        halflives = np.array([feature[2] for feature in data_clumped])\n",
    "        smiles = np.array([feature[1] for feature in data_clumped])\n",
    "        contains_nan = np.any(np.isnan(mol_features))\n",
    "        return smiles, mol_features, halflives, contains_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "552f6d9a-4041-4fd3-a117-b61338db31e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE SPECIFIC TO OPTUNA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e22c231-9f9e-42d4-9f15-2883261378b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def optuna_trial_logging(log_csv_path, trial_number, parameters, rmsd, predictions, std):\n",
    "    # Check if the CSV file exists\n",
    "    is_new_file = not os.path.isfile(log_csv_path)\n",
    "\n",
    "    # Open the CSV file in append mode\n",
    "    with open(log_csv_path, 'a', newline='') as csvfile:\n",
    "        fieldnames = ['Trial Number', 'Parameters', 'RMSD', 'Predictions', 'Std']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        # If the file is newly created, write the header\n",
    "        if is_new_file:\n",
    "            writer.writeheader()\n",
    "\n",
    "        # Write the data for the current trial\n",
    "        writer.writerow({\n",
    "            'Trial Number': trial_number,\n",
    "            'Parameters': parameters,\n",
    "            'RMSD': rmsd,\n",
    "            'Predictions': predictions.tolist(),  # Convert numpy array to a list for CSV\n",
    "            'Std': std.tolist()  # Convert numpy array to a list for CSV\n",
    "        })\n",
    "\n",
    "    # Extract only the filename using regular expressions\n",
    "    file_name_match = re.search(r'[^\\\\/:*?\"<>|\\r\\n]+$', log_csv_path)\n",
    "    file_name = file_name_match.group() if file_name_match else log_csv_path\n",
    "\n",
    "    # Print appropriate success message with the filename\n",
    "    if is_new_file:\n",
    "        print(f\"Successfully created {file_name} with results of trial {trial_number} as the first entry\")\n",
    "    else:\n",
    "        print(f\"Successfully updated {file_name} with results of trial {trial_number}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35165f13-9db6-449b-96a2-60971aafde3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HyperparamTuner():\n",
    "    def __init__(self, log_csv_path, model_identifier, X_train, y_train, X_val, y_val):\n",
    "        self.log_csv_path = log_csv_path\n",
    "        self.model_identifier = model_identifier\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "\n",
    "    def sample_params(self, trial: optuna.Trial, model_identifier):\n",
    "        if model_identifier == 'linear':\n",
    "            alpha = trial.suggest_float('alpha', 0.085, 0.15)\n",
    "            l1_ratio = trial.suggest_float('l1_ratio', 0, 0.1)\n",
    "            return {\n",
    "                \"alpha\": alpha,\n",
    "                \"l1_ratio\": l1_ratio\n",
    "            }, ElasticNet(alpha=alpha, l1_ratio=l1_ratio)\n",
    "\n",
    "        if model_identifier == 'KRR':\n",
    "            alpha = trial.suggest_float(\"alpha\", 0.3, 1)\n",
    "            gamma = trial.suggest_float(\"gamma\", 0.1, 0.3)\n",
    "            kernel = trial.suggest_categorical(\"kernel\", [\"laplacian\", \"rbf\"])\n",
    "            return {\n",
    "                \"alpha\": alpha,\n",
    "                \"gamma\": gamma,\n",
    "                \"kernel\": kernel\n",
    "            }, KernelRidge(alpha=alpha, gamma=gamma, kernel=kernel)\n",
    "\n",
    "        if model_identifier == 'GB':\n",
    "            n_estimators = trial.suggest_categorical(\"n_estimators\", [5, 10, 20, 50])\n",
    "            learning_rate = trial.suggest_float(\"learning_rate\", 0.05, 0.175)\n",
    "            max_depth = trial.suggest_categorical(\"max_depth\", [1, 2, 3])\n",
    "            return {\n",
    "                \"n_estimators\": n_estimators,\n",
    "                \"learning_rate\": learning_rate,\n",
    "                \"max_depth\": max_depth\n",
    "            }, GradientBoostingRegressor(n_estimators=n_estimators, learning_rate=learning_rate, max_depth=max_depth)\n",
    "\n",
    "        if model_identifier == 'RF':\n",
    "            n_estimators = trial.suggest_categorical(\"n_estimators\", [500, 750, 1000])\n",
    "            max_features = trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\"])\n",
    "            max_depth = trial.suggest_categorical(\"max_depth\", [None, 2, 5, 10, 20])\n",
    "            return {\n",
    "                \"n_estimators\": n_estimators,\n",
    "                \"max_features\": max_features,\n",
    "                \"max_depth\": max_depth\n",
    "            }, RandomForestRegressor(n_estimators=n_estimators, max_features=max_features, max_depth=max_depth)\n",
    "\n",
    "        if model_identifier == 'ANN':\n",
    "            learning_rate_init = trial.suggest_float(\"learning_rate_init\", 0.05, 0.15)\n",
    "            hidden_layer_sizes = trial.suggest_categorical(\"hidden_layer_sizes\",\n",
    "                                                           [[3]*3, [5]*3, [3]*5, [5]*5, [10]*3])\n",
    "            return {\n",
    "            \"learning_rate_init\": learning_rate_init,\n",
    "            \"hidden_layer_sizes\": hidden_layer_sizes\n",
    "            }, MLPRegressor(learning_rate_init=learning_rate_init, hidden_layer_sizes=hidden_layer_sizes)\n",
    "\n",
    "    def cross_validation_splits(self, X_train, X_val, y_train, y_val, cv_splits=5):\n",
    "        \"\"\"\n",
    "        Splits the data into cv_splits different combinations for cross-validation.\n",
    "\n",
    "        Parameters:\n",
    "        - X_train: Training data features\n",
    "        - X_val: Testing data features\n",
    "        - y_train: Training data labels\n",
    "        - y_val: Testing data labels\n",
    "        - cv_splits: Number of cross-validation splits\n",
    "\n",
    "        Returns:\n",
    "        - List of tuples, where each tuple contains (X_train_fold, X_val_fold, y_train_fold, y_val_fold)\n",
    "        \"\"\"\n",
    "        # Initialize StratifiedKFold with the desired number of splits\n",
    "        kf = KFold(n_splits=cv_splits, shuffle=True)  # random_state=42)\n",
    "\n",
    "        # Initialize an empty list to store the data splits\n",
    "        data_splits = []\n",
    "\n",
    "        # Loop through the cross-validation splits\n",
    "        for train_index, val_index in kf.split(X_train, y_train):\n",
    "            X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "            y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "\n",
    "            # Append the current split to the list\n",
    "            data_splits.append((X_train_fold, X_val_fold, y_train_fold, y_val_fold))\n",
    "\n",
    "        # Append the original val data to the list\n",
    "        data_splits.append((X_train, X_val, y_train, y_val))\n",
    "\n",
    "        return data_splits\n",
    "\n",
    "    def evaluate(self, model, X_val, y_val):\n",
    "        predictions = model.predict(X_val)\n",
    "        rmsd = mean_squared_error(y_val, predictions, squared=False)\n",
    "        return rmsd, predictions\n",
    "\n",
    "    def train_val_return(self, parameters, model, trial_number):\n",
    "        runs = 3\n",
    "        # average over all runs\n",
    "        runs_results = []\n",
    "        y_vals_predicted = []\n",
    "\n",
    "        for run in range(runs):\n",
    "            validation_splits = self.cross_validation_splits(self.X_train, self.X_val, self.y_train, self.y_val)\n",
    "            # average over all splits in a given run\n",
    "            cv_fold_results = []\n",
    "            y_val_predicted = []\n",
    "            fold_num = 0\n",
    "\n",
    "            # cross-validation\n",
    "            for (X_train_split, X_val_split, y_train_split, y_val_split) in validation_splits:\n",
    "                fold_num += 1\n",
    "                \n",
    "                # train the model on the given validation split\n",
    "                model.fit(X_train_split, y_train_split)\n",
    "                cv_fold_rmsd, validation_predictions = self.evaluate(model, X_val_split, y_val_split)\n",
    "                \n",
    "                # and save the result of that split\n",
    "                cv_fold_results.append(cv_fold_rmsd)\n",
    "                \n",
    "                # after all five folds, append the final predictions\n",
    "                if fold_num == 6:\n",
    "                    y_val_predicted.append(validation_predictions)\n",
    "\n",
    "            runs_results.append(np.mean(cv_fold_results))\n",
    "            y_vals_predicted.append(y_val_predicted)\n",
    "\n",
    "        # calculate the standard deviation of predictions\n",
    "        y_vals_predicted = np.array(y_vals_predicted)\n",
    "        std = np.std(y_vals_predicted, axis=0)[0]\n",
    "        \n",
    "        average_predictions = np.average(y_vals_predicted, axis=0)[0]\n",
    "        average_result = np.mean(runs_results)\n",
    "        \n",
    "        # write the result and hyperparameters of a run to csv file\n",
    "        optuna_trial_logging(self.log_csv_path, trial_number, parameters, average_result, average_predictions, std)\n",
    "\n",
    "        return average_result\n",
    "\n",
    "    def objective(self, trial=None):\n",
    "        parameters, model = self.sample_params(trial, self.model_identifier)\n",
    "        return self.train_val_return(parameters, model, trial.number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980ef532-4907-4090-8706-d0bd3b5b2696",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
