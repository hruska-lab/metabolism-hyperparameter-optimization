{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08a691e4-003d-4c69-b3b4-5648996796e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Module with only necessary functions/classes for running the code on IT4Innovations,\\nwithout any packages which are not needed'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Module with only necessary functions/classes for running the code on IT4Innovations,\n",
    "without any packages which are not needed\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d193cd9-761c-447d-bf1f-659c18c6eb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import numpy as np\n",
    "import optuna\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c10a604-4e43-4ea0-b198-9a9a6009ba64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "266a8858-c9bb-433a-a8d2-9c6c3e88c8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fp_from_smiles(list_smiles):\n",
    "    list_fingerprint = []\n",
    "    for smi in list_smiles:\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        fingerprint = AllChem.GetMorganFingerprintAsBitVect(mol, useChirality=True, radius=2, nBits=124)\n",
    "        vector = np.array(fingerprint)\n",
    "        list_fingerprint.append(vector)\n",
    "    # takes a list of smiles strings,output is a corresponding Morgan fingerprint as a list\n",
    "    return list_fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ffaefc6-2100-4f88-af52-be4d44e9ad30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_jazzy_df(df):\n",
    "    cols = df.columns\n",
    "    data = {}  # all data from csv file (i.e. mol indexes, smiles, half-lives and features)\n",
    "    for col in cols:\n",
    "        data[col] = list(df[col])\n",
    "    nan_idxs = np.argwhere(np.isnan(data[\"dgtot\"]))\n",
    "    nan_idxs = [int(idx) for idx in nan_idxs]\n",
    "    data_clumped = []  # same as data, but in the form [[idx1, smi1, thalf1, fts1], [idx2, smi2, thalf2, fts2],...]]\n",
    "    for col in cols:\n",
    "        for i, foo in zip(range(len(data[col])), data[col]):\n",
    "            if len(data_clumped) < i+1:\n",
    "                data_clumped.append([])\n",
    "            data_clumped[i].append(foo)\n",
    "\n",
    "    # remove all mols for which Jazzy features generation wasn't successful\n",
    "    num_pops = 0\n",
    "    for nan_idx in nan_idxs:\n",
    "        data_clumped.pop(nan_idx - num_pops)\n",
    "        num_pops += 1\n",
    "        print(f\"     removed index {nan_idx} corresponding to NaN\")\n",
    "    print(f\"     {len(data_clumped)}, {data_clumped[0]}\")\n",
    "\n",
    "    # filter out only the features\n",
    "    mol_features = np.array([feature[3:11] for feature in data_clumped])\n",
    "    halflives = np.array([feature[2] for feature in data_clumped])\n",
    "    smiles = np.array([feature[1] for feature in data_clumped])\n",
    "    contains_nan = np.any(np.isnan(mol_features))\n",
    "\n",
    "    return smiles, mol_features, halflives, contains_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adc7fd85-4581-4070-a1c7-8252ac946a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperparamTuner():\n",
    "    def __init__(self, model_identifier, X_train, y_train, X_test, y_test):\n",
    "        self.model_identifier = model_identifier\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "\n",
    "    def sample_params(self, trial: optuna.Trial, model_identifier):\n",
    "        if model_identifier == 'linear':\n",
    "            fit_intercept = trial.set_user_attr(\"fit_intercept\", True)\n",
    "            alpha = trial.suggest_float('alpha', 1e-5, 1e-1)\n",
    "            l1_ratio = trial.suggest_float('l1_ratio', 0, 1)\n",
    "            return {\n",
    "                \"fit_intercept\": fit_intercept,\n",
    "                \"alpha\": alpha,\n",
    "                \"l1_ratio\": l1_ratio\n",
    "            }, ElasticNet()\n",
    "\n",
    "        if model_identifier == 'KRR':\n",
    "            alpha = trial.suggest_float(\"alpha\", 1e-4, 1)\n",
    "            gamma = trial.suggest_float(\"gamma\", 0, 1e-14)\n",
    "            kernel = trial.suggest_categorical(\"kernel\", [\"linear\", \"laplacian\", \"rbf\"])\n",
    "            return {\n",
    "                \"alpha\": alpha,\n",
    "                \"gamma\": gamma,\n",
    "                \"kernel\": kernel\n",
    "            }, KernelRidge()\n",
    "\n",
    "        if model_identifier == 'GB':\n",
    "            n_estimators = trial.suggest_categorical(\"n_estimators\", [10, 20, 50, 200, 500])\n",
    "            learning_rate = trial.suggest_float(\"learning_rate\", 0.005, 1)\n",
    "            max_depth = trial.suggest_categorical(\"max_depth\", [1, 2, 3, 4, 5])\n",
    "            return {\n",
    "                \"n_estimators\": n_estimators,\n",
    "                \"learning_rate\": learning_rate,\n",
    "                \"max_depth\": max_depth\n",
    "            }, GradientBoostingRegressor()\n",
    "\n",
    "        if model_identifier == 'RF':\n",
    "            n_estimators = trial.suggest_categorical(\"n_estimators\", [10, 20, 50, 200, 500])\n",
    "            max_features = trial.suggest_categorical(\"max_features\", [\"auto\", \"sqrt\", \"log2\"])\n",
    "            max_depth = trial.suggest_categorical(\"max_depth\", [None, 2, 3, 4, 5, 10])\n",
    "            return {\n",
    "                \"n_estimators\": n_estimators,\n",
    "                \"max_features\": max_features,\n",
    "                \"max_depth\": max_depth\n",
    "            }, RandomForestRegressor()\n",
    "\n",
    "        if model_identifier == 'ANN':\n",
    "            learning_rate_init = trial.suggest_float(\"learning_rate_init\", 0.001, 0.1)\n",
    "            hidden_layer_sizes = trial.suggest_categorical(\"hidden_layer_sizes\",\n",
    "                                                           [[5], [10], [20], [50], [5]*2, [10]*2, [20]*2, [50]*2, [5]*3, [10]*3, [50]*3])\n",
    "            return {\n",
    "            \"learning_rate_init\": learning_rate_init,\n",
    "            \"hidden_layer_sizes\": hidden_layer_sizes\n",
    "            }, MLPRegressor()\n",
    "\n",
    "    def cross_validation_splits(self, X_train, X_test, y_train, y_test, cv_splits=5):\n",
    "        \"\"\"\n",
    "        Splits the data into cv_splits different combinations for cross-validation.\n",
    "\n",
    "        Parameters:\n",
    "        - X_train: Training data features\n",
    "        - X_test: Testing data features\n",
    "        - y_train: Training data labels\n",
    "        - y_test: Testing data labels\n",
    "        - cv_splits: Number of cross-validation splits\n",
    "\n",
    "        Returns:\n",
    "        - List of tuples, where each tuple contains (X_train_fold, X_test_fold, y_train_fold, y_test_fold)\n",
    "        \"\"\"\n",
    "        # Initialize StratifiedKFold with the desired number of splits\n",
    "        kf = KFold(n_splits=cv_splits, shuffle=True)  # random_state=42)\n",
    "\n",
    "        # Initialize an empty list to store the data splits\n",
    "        data_splits = []\n",
    "\n",
    "        # Loop through the cross-validation splits\n",
    "        for train_index, test_index in kf.split(X_train, y_train):\n",
    "            X_train_fold, X_val_fold = X_train[train_index], X_train[test_index]\n",
    "            y_train_fold, y_val_fold = y_train[train_index], y_train[test_index]\n",
    "\n",
    "            # Append the current split to the list\n",
    "            data_splits.append((X_train_fold, X_val_fold, y_train_fold, y_val_fold))\n",
    "\n",
    "        # Append the original test data to the list\n",
    "        data_splits.append((X_train, X_test, y_train, y_test))\n",
    "\n",
    "        return data_splits\n",
    "\n",
    "    def evaluate(self, model, X_test, y_test, return_predictions=False):\n",
    "        predictions = model.predict(X_test)\n",
    "        rmsd = mean_squared_error(y_test, predictions, squared=False)\n",
    "        if return_predictions:\n",
    "            return rmsd, predictions\n",
    "        else:\n",
    "            return rmsd\n",
    "\n",
    "    def train_test_return(self, parameters, model, return_predictions=False):\n",
    "        runs = 3\n",
    "        runs_results = []\n",
    "        y_tests_predicted = []\n",
    "\n",
    "        for run in range(runs):\n",
    "            validation_splits = self.cross_validation_splits(self.X_train, self.X_test, self.y_train, self.y_test)\n",
    "            cv_fold_results = []\n",
    "            y_test_predicted = []\n",
    "            fold_num = 0\n",
    "\n",
    "            for (X_train_val, X_test_val, y_train_val, y_test_val) in validation_splits:\n",
    "                fold_num += 1\n",
    "                model.fit(X_train_val, y_train_val)\n",
    "\n",
    "                if return_predictions and fold_num == 6:\n",
    "                    cv_fold_rmsd, validation_predictions = self.evaluate(model, X_test_val, y_test_val, return_predictions=return_predictions)\n",
    "                    y_test_predicted.append(validation_predictions)\n",
    "                else:\n",
    "                    cv_fold_rmsd = self.evaluate(model, X_test_val, y_test_val, return_predictions=False)\n",
    "\n",
    "                cv_fold_results.append(cv_fold_rmsd)\n",
    "\n",
    "            runs_results.append(np.mean(cv_fold_results))\n",
    "            y_tests_predicted.append(y_test_predicted)\n",
    "\n",
    "        # Calculate the standard deviation of predictions\n",
    "        y_tests_predicted = np.array(y_tests_predicted)\n",
    "        std = np.std(y_tests_predicted, axis=0)\n",
    "\n",
    "        if return_predictions:\n",
    "            # Return the mean RMSD, average predictions, and standard deviations\n",
    "            return np.mean(runs_results), np.average(y_tests_predicted, axis=0)[0], std[0]\n",
    "        else:\n",
    "            # Return the mean objective/s of these runs\n",
    "            return np.mean(runs_results)\n",
    "\n",
    "    def objective(self, trial=None):\n",
    "        parameters, model = self.sample_params(trial, self.model_identifier)\n",
    "        return self.train_test_return(parameters, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8577a2af-a9c0-4c5e-902f-bb4536bca838",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
